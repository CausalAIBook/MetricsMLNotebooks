{"metadata":{"kernelspec":{"name":"ir","display_name":"R","language":"R"},"language_info":{"name":"R","codemirror_mode":"r","pygments_lexer":"r","mimetype":"text/x-r-source","file_extension":".r","version":"3.6.3"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"This notebook contains an example for teaching.\n","metadata":{}},{"cell_type":"markdown","source":"# Automatic Machine Learning with H2O AutoML ","metadata":{}},{"cell_type":"markdown","source":"We illustrate how to predict an outcome variable Y in a high-dimensional setting, using the AutoML package *H2O* that covers the complete pipeline from the raw dataset to the deployable machine learning model. In last few years, AutoML or automated machine learning has become widely popular among data science community. Again, re-analyse the wage prediction problem using data from the U.S. March Supplement of the Current Population Survey (CPS) in 2015.","metadata":{}},{"cell_type":"markdown","source":"We can use AutoML as a benchmark and compare it to the methods that we used in the previous notebook where we applied one machine learning method after the other.","metadata":{}},{"cell_type":"code","source":"# load the H2O package\nlibrary(h2o)","metadata":{"execution":{"iopub.status.busy":"2021-07-22T20:53:14.87348Z","iopub.execute_input":"2021-07-22T20:53:14.875298Z","iopub.status.idle":"2021-07-22T20:53:15.245871Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# load the data set\nload(\"../input/wage2015-inference/wage2015_subsample_inference.Rdata\")\n\n# split the data\nset.seed(1234)\ntraining <- sample(nrow(data), nrow(data)*(3/4), replace=FALSE)\n\ntrain <- data[training,]\ntest <- data[-training,]","metadata":{"execution":{"iopub.status.busy":"2021-07-22T20:53:15.247826Z","iopub.execute_input":"2021-07-22T20:53:15.282838Z","iopub.status.idle":"2021-07-22T20:53:15.362809Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# start h2o cluster\nh2o.init()","metadata":{"execution":{"iopub.status.busy":"2021-07-22T20:53:15.364927Z","iopub.execute_input":"2021-07-22T20:53:15.366217Z","iopub.status.idle":"2021-07-22T20:53:19.613156Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# convert data as h2o type\ntrain_h = as.h2o(train)\ntest_h = as.h2o(test)\n\n# have a look at the data\nh2o.describe(train_h)","metadata":{"execution":{"iopub.status.busy":"2021-07-22T20:53:19.616822Z","iopub.execute_input":"2021-07-22T20:53:19.618787Z","iopub.status.idle":"2021-07-22T20:53:25.616425Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# define the variables\ny = 'lwage'\nx = setdiff(names(data), c('wage','occ2', 'ind2'))\n            \n# run AutoML for 10 base models and a maximal runtime of 100 seconds\naml = h2o.automl(x=x,y = y,\n                  training_frame = train_h,\n                  leaderboard_frame = test_h,\n                  max_models = 10,\n                  seed = 1,\n                  max_runtime_secs = 100\n                 )\n# AutoML Leaderboard\nlb = aml@leaderboard\nprint(lb, n = nrow(lb))","metadata":{"execution":{"iopub.status.busy":"2021-07-22T20:53:25.618628Z","iopub.execute_input":"2021-07-22T20:53:25.619926Z","iopub.status.idle":"2021-07-22T20:54:05.530123Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We see that two Stacked Ensembles are at the top of the leaderboard. Stacked Ensembles often outperform a single model. The out-of-sample (test) MSE of the leading model is given by","metadata":{}},{"cell_type":"code","source":"aml@leaderboard$mse[1]","metadata":{"execution":{"iopub.status.busy":"2021-07-22T20:54:05.53219Z","iopub.execute_input":"2021-07-22T20:54:05.533418Z","iopub.status.idle":"2021-07-22T20:54:05.734654Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The in-sample performance can be evaluated by","metadata":{}},{"cell_type":"code","source":"aml@leader","metadata":{"execution":{"iopub.status.busy":"2021-07-22T20:54:05.736858Z","iopub.execute_input":"2021-07-22T20:54:05.7382Z","iopub.status.idle":"2021-07-22T20:54:05.759186Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This is in line with our previous results. To understand how the ensemble works, let's take a peek inside the Stacked Ensemble \"All Models\" model.  The \"All Models\" ensemble is an ensemble of all of the individual models in the AutoML run.  This is often the top performing model on the leaderboard.","metadata":{}},{"cell_type":"code","source":"model_ids <- as.data.frame(aml@leaderboard$model_id)[,1]\n# Get the \"All Models\" Stacked Ensemble model\nse <- h2o.getModel(grep(\"StackedEnsemble_AllModels\", model_ids, value = TRUE)[1])\n# Get the Stacked Ensemble metalearner model\nmetalearner <- se@model$metalearner_model\nh2o.varimp(metalearner)","metadata":{"execution":{"iopub.status.busy":"2021-07-22T20:54:05.761268Z","iopub.execute_input":"2021-07-22T20:54:05.762558Z","iopub.status.idle":"2021-07-22T20:54:05.945209Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The table above gives us the variable importance of the metalearner in the ensemble. The AutoML Stacked Ensembles use the default metalearner algorithm (GLM with non-negative weights), so the variable importance of the metalearner is actually the standardized coefficient magnitudes of the GLM. \n","metadata":{}},{"cell_type":"code","source":"h2o.varimp_plot(metalearner)","metadata":{"execution":{"iopub.status.busy":"2021-07-22T20:54:05.947744Z","iopub.execute_input":"2021-07-22T20:54:05.949345Z","iopub.status.idle":"2021-07-22T20:54:06.274933Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Generating Predictions Using Leader Model\n\nWe can also generate predictions on a test sample using the leader model object.","metadata":{}},{"cell_type":"code","source":"pred <- as.matrix(h2o.predict(aml@leader,test_h)) # make prediction using x data from the test sample\nhead(pred)","metadata":{"execution":{"iopub.status.busy":"2021-07-22T20:54:06.277153Z","iopub.execute_input":"2021-07-22T20:54:06.278551Z","iopub.status.idle":"2021-07-22T20:54:07.406734Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This allows us to estimate the out-of-sample (test) MSE and the standard error as well.","metadata":{}},{"cell_type":"code","source":"y_test <- as.matrix(test_h$lwage)\nsummary(lm((y_test-pred)^2~1))$coef[1:2]","metadata":{"execution":{"iopub.status.busy":"2021-07-22T20:54:07.410016Z","iopub.execute_input":"2021-07-22T20:54:07.411631Z","iopub.status.idle":"2021-07-22T20:54:07.498558Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We observe both a lower MSE and a lower standard error compared to our previous results (see [here](https://www.kaggle.com/janniskueck/pm3-notebook-newdata)).","metadata":{}},{"cell_type":"code","source":"h2o.shutdown(prompt = F)","metadata":{"execution":{"iopub.status.busy":"2021-07-22T20:54:07.507396Z","iopub.execute_input":"2021-07-22T20:54:07.524738Z","iopub.status.idle":"2021-07-22T20:54:07.55563Z"},"trusted":true},"execution_count":null,"outputs":[]}]}
