{"metadata":{"kernelspec":{"name":"ir","display_name":"R","language":"R"},"language_info":{"name":"R","codemirror_mode":"r","pygments_lexer":"r","mimetype":"text/x-r-source","file_extension":".r","version":"3.6.3"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"This notebook contains an example for teaching.","metadata":{}},{"cell_type":"markdown","source":"# A Case Study: The Effect of Gun Ownership on Gun-Homicide Rates","metadata":{}},{"cell_type":"markdown","source":"We consider the problem of estimating the effect of gun\nownership on the homicide rate. For this purpose, we estimate the following partially\nlinear model\n\n$$\n Y_{j,t} = \\beta D_{j,(t-1)} + g(Z_{j,t}) + \\epsilon_{j,t}.\n$$","metadata":{}},{"cell_type":"markdown","source":"## Data","metadata":{}},{"cell_type":"markdown","source":"$Y_{j,t}$ is the log homicide rate in county $j$ at time $t$, $D_{j, t-1}$ is the log fraction of suicides committed with a firearm in county $j$ at time $t-1$, which we use as a proxy for gun ownership,  and  $Z_{j,t}$ is a set of demographic and economic characteristics of county $j$ at time $t$. The parameter $\\beta$ is the effect of gun ownership on homicide rates, controlling for county-level demographic and economic characteristics. \n\nThe sample covers 195 large United States counties between the years 1980 through 1999, giving us 3900 observations.","metadata":{}},{"cell_type":"code","source":"data <- read.csv(\"../input/gun-example/gun_clean.csv\") \ndim(data)[1]","metadata":{"execution":{"iopub.status.busy":"2021-07-23T16:06:10.05878Z","iopub.execute_input":"2021-07-23T16:06:10.06103Z","iopub.status.idle":"2021-07-23T16:06:11.171267Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Preprocessing","metadata":{}},{"cell_type":"markdown","source":"To account for heterogeneity across counties and time trends in  all variables, we remove from them county-specific and time-specific effects in the following preprocessing.","metadata":{}},{"cell_type":"code","source":"##################### Find Variable Names from Dataset ######################\n\nvarlist <- function (df=NULL,type=c(\"numeric\",\"factor\",\"character\"), pattern=\"\", exclude=NULL) {\n  vars <- character(0)\n  if (any(type %in% \"numeric\")) {\n    vars <- c(vars,names(df)[sapply(df,is.numeric)])\n  }\n  if (any(type %in% \"factor\")) {\n    vars <- c(vars,names(df)[sapply(df,is.factor)])\n  }  \n  if (any(type %in% \"character\")) {\n    vars <- c(vars,names(df)[sapply(df,is.character)])\n  }  \n  vars[(!vars %in% exclude) & grepl(vars,pattern=pattern)]\n}\n\n############################# Create Variables ##############################\n\n# dummy variables for year and county fixed effects\nfixed  <- grep(\"X_Jfips\", names(data), value=TRUE, fixed=TRUE)\nyear   <- varlist(data, pattern=\"X_Tyear\")\n\n# census control variables\ncensus     <- NULL\ncensus_var <- c(\"^AGE\", \"^BN\", \"^BP\", \"^BZ\", \"^ED\", \"^EL\",\"^HI\", \"^HS\", \"^INC\", \"^LF\", \"^LN\", \"^PI\", \"^PO\", \"^PP\", \"^PV\", \"^SPR\", \"^VS\")\n\nfor(i in 1:length(census_var)){\n    census  <- append(census, varlist(data, pattern=census_var[i])) \n}\n\n################################ Variables ##################################\n# treatment variable\nd     <- \"logfssl\"\n\n# outcome variable\ny     <- \"logghomr\"\n\n# other control variables\nX1    <- c(\"logrobr\", \"logburg\", \"burg_missing\", \"robrate_missing\")\nX2    <- c(\"newblack\", \"newfhh\", \"newmove\", \"newdens\", \"newmal\")\n\n######################## Partial out Fixed Effects ##########################\n\n# new dataset for partialled-out variables\nrdata    <- as.data.frame(data$CountyCode) \ncolnames(rdata) <- \"CountyCode\"\n\n# variables to partial out\nvarlist <- c(y, d,X1, X2, census)\n\n# partial out year and county fixed effect from variables in varlist\nfor(i in 1:length(varlist)){\n  form <- as.formula(paste(varlist[i], \"~\", paste(paste(year,collapse=\"+\"),  paste(fixed,collapse=\"+\"), sep=\"+\")))\n  rdata[, varlist[i]] <- lm(form, data)$residuals\n}","metadata":{"execution":{"iopub.status.busy":"2021-07-23T16:06:11.173295Z","iopub.execute_input":"2021-07-23T16:06:11.221715Z","iopub.status.idle":"2021-07-23T16:06:39.014448Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now, we can construct the treatment variable, the outcome variable and the matrix $Z$ that includes the control variables.","metadata":{}},{"cell_type":"code","source":"# treatment variable\nD     <- rdata[which(colnames(rdata) == d)]\n\n# outcome variable\nY     <- rdata[which(colnames(rdata) == y)]\n\n# construct matrix Z\nZ <- rdata[which(colnames(rdata) %in% c(X1,X2,census))]\ndim(Z)","metadata":{"execution":{"iopub.status.busy":"2021-07-23T16:06:39.017037Z","iopub.execute_input":"2021-07-23T16:06:39.018855Z","iopub.status.idle":"2021-07-23T16:06:39.039099Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We have 195 control variables in total. The control variables $Z_{j,t}$ are from the U.S. Census Bureau and  contain demographic and economic characteristics of the counties such as  the age distribution, the income distribution, crime rates, federal spending, home ownership rates, house prices, educational attainment, voting paterns, employment statistics, and migration rates. ","metadata":{}},{"cell_type":"code","source":"clu <- rdata[which(colnames(rdata) == \"CountyCode\")] # for clustering the standard errors\ndata <- data.frame(cbind(Y, D, Z,as.matrix(clu)))","metadata":{"execution":{"iopub.status.busy":"2021-07-23T16:06:39.041238Z","iopub.execute_input":"2021-07-23T16:06:39.042048Z","iopub.status.idle":"2021-07-23T16:06:39.055952Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"library(lfe) # linear group fixed effects package","metadata":{"execution":{"iopub.status.busy":"2021-07-23T16:06:39.057953Z","iopub.execute_input":"2021-07-23T16:06:39.059051Z","iopub.status.idle":"2021-07-23T16:06:40.215303Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## The effect of gun ownership","metadata":{}},{"cell_type":"markdown","source":"### OLS","metadata":{}},{"cell_type":"markdown","source":"After preprocessing the data, as a baseline model, we first look at simple regression of $Y_{j,t}$ on $D_{j,t-1}$ without controls.","metadata":{}},{"cell_type":"code","source":"# baseline_formula <- as.formula(paste(y, \"~\", d ))\n# baseline.ols <- lm(baseline_formula,data=rdata)\n\nbaseline.ols <- felm(logghomr ~ logfssl |0|0| CountyCode,data=data) # ols with clustered standard errors\nest_baseline <- summary(baseline.ols)$coef[2,]\nconfint(baseline.ols)[2,]\nest_baseline","metadata":{"execution":{"iopub.status.busy":"2021-07-23T16:06:40.217853Z","iopub.execute_input":"2021-07-23T16:06:40.219629Z","iopub.status.idle":"2021-07-23T16:06:40.300901Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The point estimate is $0.282$ with the confidence interval ranging from 0.155 to 0.41. This\nsuggests that increases in gun ownership rates are related to gun homicide rates - if gun ownership increases by 1% relative\nto a trend then the predicted gun homicide rate goes up by 0.28%, without controlling for counties' characteristics.\n\nSince our goal is to estimate the effect of gun ownership after controlling for a rich set county characteristics, we next include the controls. First, we estimate the model by ols and then by an array of the modern regression methods using the double machine learning approach.","metadata":{}},{"cell_type":"code","source":"control_formula <- as.formula(paste(\"logghomr\", \"~\", paste(\"logfssl\",paste(colnames(Z),collapse=\"+\"),\n                                                           sep=\"+\"),\"|0|0| CountyCode\"))\ncontrol.ols <- felm(control_formula,data=data) # fixed effects lm function\nest_ols <- summary(control.ols)$coef[2,]\nconfint(control.ols)[2,]\nest_ols","metadata":{"execution":{"iopub.status.busy":"2021-07-23T16:06:40.303193Z","iopub.execute_input":"2021-07-23T16:06:40.304695Z","iopub.status.idle":"2021-07-23T16:06:40.524104Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"After controlling for a rich set of characteristics, the point estimate of gun ownership reduces to $0.19$.","metadata":{}},{"cell_type":"markdown","source":"# DML algorithm\n\nHere we perform inference on the predictive coefficient $\\beta$ in our partially linear statistical model, \n\n$$\nY = D\\beta + g(Z) + \\epsilon, \\quad E (\\epsilon | D, Z) = 0,\n$$\n\nusing the **double machine learning** approach. \n\nFor $\\tilde Y = Y- E(Y|Z)$ and $\\tilde D= D- E(D|Z)$, we can write\n$$\n\\tilde Y = \\alpha \\tilde D + \\epsilon, \\quad E (\\epsilon |\\tilde D) =0.\n$$\n\nUsing cross-fitting, we employ modern regression methods\nto build estimators $\\hat \\ell(Z)$ and $\\hat m(Z)$ of $\\ell(Z):=E(Y|Z)$ and $m(Z):=E(D|Z)$ to obtain the estimates of the residualized quantities:\n\n$$\n\\tilde Y_i = Y_i  - \\hat \\ell (Z_i),   \\quad \\tilde D_i = D_i - \\hat m(Z_i), \\quad \\text{ for each } i = 1,\\dots,n.\n$$\n\nFinally, using ordinary least squares of $\\tilde Y_i$ on $\\tilde D_i$, we obtain the \nestimate of $\\beta$.","metadata":{}},{"cell_type":"markdown","source":"The following algorithm comsumes $Y, D, Z$, and a machine learning method for learning the residuals $\\tilde Y$ and $\\tilde D$, where the residuals are obtained by cross-validation (cross-fitting). Then, it prints the estimated coefficient $\\beta$ and the corresponding standard error from the final OLS regression.","metadata":{}},{"cell_type":"code","source":"DML2.for.PLM <- function(z, d, y, dreg, yreg, nfold=2, clu) {\n  nobs <- nrow(z) # number of observations\n  foldid <- rep.int(1:nfold,times = ceiling(nobs/nfold))[sample.int(nobs)] # define folds indices\n  I <- split(1:nobs, foldid)  # split observation indices into folds  \n  ytil <- dtil <- rep(NA, nobs)\n  cat(\"fold: \")\n  for(b in 1:length(I)){\n    dfit <- dreg(z[-I[[b]],], d[-I[[b]]]) # take a fold out\n    yfit <- yreg(z[-I[[b]],], y[-I[[b]]]) # take a fold out\n    dhat <- predict(dfit, z[I[[b]],], type=\"response\") # predict the left-out fold \n    yhat <- predict(yfit, z[I[[b]],], type=\"response\") # predict the left-out fold  \n    dtil[I[[b]]] <- (d[I[[b]]] - dhat) # record residual for the left-out fold\n    ytil[I[[b]]] <- (y[I[[b]]] - yhat) # record residial for the left-out fold\n    cat(b,\" \")\n        }\n  #rfit <- lm(ytil ~ dtil) # estimate the main parameter by regressing one residual on the other\n  data <- data.frame(cbind(ytil, dtil, as.matrix(clu)))\n  rfit <- felm(ytil ~ dtil|0|0|CountyCode,data=data) \n  coef.est <- coef(rfit)[2] # extract coefficient\n  #HC <- vcovHC(rfit)\n  se    <- summary(rfit,robust=T)$coefficients[2,2] # record robust standard error by county\n  cat(sprintf(\"\\ncoef (se) = %g (%g)\\n\", coef.est , se)) # print output\n  return( list(coef.est =coef.est , se=se, dtil=dtil, ytil=ytil, rfit=rfit) ) # save output and residuals \n}","metadata":{"execution":{"iopub.status.busy":"2021-07-23T16:06:40.528435Z","iopub.execute_input":"2021-07-23T16:06:40.531303Z","iopub.status.idle":"2021-07-23T16:06:40.554659Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now, we apply the Double Machine Learning (DML) approach with different machine learning methods. First, we load the relevant libraries.","metadata":{}},{"cell_type":"code","source":"library(hdm)\nlibrary(glmnet)\nlibrary(sandwich)\nlibrary(randomForest)","metadata":{"execution":{"iopub.status.busy":"2021-07-23T16:06:40.558595Z","iopub.execute_input":"2021-07-23T16:06:40.596311Z","iopub.status.idle":"2021-07-23T16:06:40.825721Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let us, construct the input matrices.","metadata":{}},{"cell_type":"code","source":"y <- as.matrix(Y)\nd <- as.matrix(D)\nz <- as.matrix(Z)\nclu <- rdata[which(colnames(rdata) == \"CountyCode\")]\nhead(data.frame(cbind(y,d,as.matrix(clu))))","metadata":{"execution":{"iopub.status.busy":"2021-07-23T16:06:40.827904Z","iopub.execute_input":"2021-07-23T16:06:40.829371Z","iopub.status.idle":"2021-07-23T16:06:40.874432Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In the following, we apply the DML approach with the different versions of lasso.\n","metadata":{}},{"cell_type":"markdown","source":"## Lasso","metadata":{}},{"cell_type":"code","source":"# DML with Lasso:\nset.seed(123)\ndreg <- function(z,d){ rlasso(z,d, post=FALSE) } # ML method= lasso from hdm \nyreg <- function(z,y){ rlasso(z,y, post=FALSE) } # ML method = lasso from hdm\nDML2.lasso = DML2.for.PLM(z, d, y, dreg, yreg, nfold=10,clu)","metadata":{"execution":{"iopub.status.busy":"2021-07-23T16:06:40.876724Z","iopub.execute_input":"2021-07-23T16:06:40.878309Z","iopub.status.idle":"2021-07-23T16:07:10.698522Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# DML with Post-Lasso:\ndreg <- function(z,d){ rlasso(z,d, post=T) } # ML method= lasso from hdm \nyreg <- function(z,y){ rlasso(z,y, post=T) } # ML method = lasso from hdm\nDML2.post = DML2.for.PLM(z, d, y, dreg, yreg, nfold=10, clu)","metadata":{"execution":{"iopub.status.busy":"2021-07-23T16:07:10.702074Z","iopub.execute_input":"2021-07-23T16:07:10.70433Z","iopub.status.idle":"2021-07-23T16:07:39.637987Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# DML with cross-validated Lasso:\ndreg <- function(z,d){ cv.glmnet(z,d,family=\"gaussian\", alpha=1) } # ML method = lasso from glmnet \nyreg <- function(z,y){ cv.glmnet(z,y,family=\"gaussian\", alpha=1) } # ML method = lasso from glmnet \nDML2.lasso.cv = DML2.for.PLM(z, d, y, dreg, yreg, nfold=10, clu)\n\ndreg <- function(z,d){ cv.glmnet(z,d,family=\"gaussian\", alpha=0.5) } # ML method = elastic net from glmnet \nyreg <- function(z,y){ cv.glmnet(z,y,family=\"gaussian\", alpha=0.5) } # ML method = elastic net from glmnet \nDML2.elnet = DML2.for.PLM(z, d, y, dreg, yreg, nfold=10, clu)\n\ndreg <- function(z,d){ cv.glmnet(z,d,family=\"gaussian\", alpha=0) } # ML method = ridge from glmnet \nyreg <- function(z,y){ cv.glmnet(z,y,family=\"gaussian\", alpha=0) } # ML method = ridge from glmnet \nDML2.ridge = DML2.for.PLM(z, d, y, dreg, yreg, nfold=10, clu)","metadata":{"execution":{"iopub.status.busy":"2021-07-23T16:07:39.649717Z","iopub.execute_input":"2021-07-23T16:07:39.658717Z","iopub.status.idle":"2021-07-23T16:11:10.853798Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Here we also compute DML with OLS used as the ML method","metadata":{}},{"cell_type":"code","source":"dreg <- function(z,d){  glmnet(z,d,family=\"gaussian\", lambda=0) } # ML method = ols from glmnet \nyreg <- function(z,y){  glmnet(z,y,family=\"gaussian\", lambda=0) }  # ML method = ols from glmnet \nDML2.ols = DML2.for.PLM(z, d, y, dreg, yreg, nfold=10, clu)","metadata":{"execution":{"iopub.status.busy":"2021-07-23T16:11:10.856611Z","iopub.execute_input":"2021-07-23T16:11:10.858754Z","iopub.status.idle":"2021-07-23T16:11:16.976973Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Next, we also apply Random Forest for comparison purposes.","metadata":{}},{"cell_type":"markdown","source":"### Random Forest\n","metadata":{"trusted":true}},{"cell_type":"code","source":"# DML with Random Forest:\ndreg <- function(z,d){ randomForest(z, d) } # ML method = random forest \nyreg <- function(z,y){ randomForest(z, y) } # ML method = random forest\nset.seed(1)\nDML2.RF = DML2.for.PLM(z, d, y, dreg, yreg, nfold=2, clu) # set folds to 2 to limit computation time","metadata":{"execution":{"iopub.status.busy":"2021-07-23T16:11:16.97928Z","iopub.execute_input":"2021-07-23T16:11:16.981077Z","iopub.status.idle":"2021-07-23T16:16:50.770909Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We conclude that the gun ownership rates are related to gun homicide rates - if gun ownership increases by 1% relative\nto a trend then the predicted gun homicide rate goes up by about 0.20% controlling for counties' characteristics.","metadata":{}},{"cell_type":"markdown","source":"Finally, let's see which method is best. We compute RMSE for predicting D and Y, and see which\nof the methods works better.\n","metadata":{}},{"cell_type":"code","source":"mods<- list(DML2.ols, DML2.lasso, DML2.post, DML2.lasso.cv, DML2.ridge, DML2.elnet, DML2.RF)\n\nRMSE.mdl<- function(mdl) {\nRMSEY <- sqrt(mean(mdl$ytil)^2) \nRMSED <- sqrt(mean(mdl$dtil)^2) \nreturn( list(RMSEY=RMSEY, RMSED=RMSED))\n}\n\n#RMSE.mdl(DML2.lasso)\n#DML2.lasso$ytil\n\nRes<- lapply(mods, RMSE.mdl)\n\nprRes.Y<- c( Res[[1]]$RMSEY,Res[[2]]$RMSEY, Res[[3]]$RMSEY, Res[[4]]$RMSEY, Res[[5]]$RMSEY,  Res[[6]]$RMSEY, Res[[7]]$RMSEY)\nprRes.D<- c( Res[[1]]$RMSED,Res[[2]]$RMSED, Res[[3]]$RMSED, Res[[4]]$RMSED, Res[[5]]$RMSED, Res[[6]]$RMSED, Res[[7]]$RMSED)\n\nprRes<- rbind(prRes.Y, prRes.D); \nrownames(prRes)<- c(\"RMSE D\", \"RMSE Y\");\ncolnames(prRes)<- c(\"OLS\", \"Lasso\", \"Post-Lasso\", \"CV Lasso\", \"CV Ridge\", \"CV Elnet\", \"RF\")\nprint(prRes,digit=6)","metadata":{"execution":{"iopub.status.busy":"2021-07-23T16:16:50.773735Z","iopub.execute_input":"2021-07-23T16:16:50.775532Z","iopub.status.idle":"2021-07-23T16:16:50.808986Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It looks like the best method for predicting D is Lasso, and the best method for predicting Y is CV Ridge.\n","metadata":{}},{"cell_type":"code","source":"dreg <- function(z,d){ rlasso(z,d, post=T) } # ML method = lasso from hdm \nyreg <- function(z,y){ cv.glmnet(z,y,family=\"gaussian\", alpha=0) }  # ML method = ridge from glmnet \nDML2.best= DML2.for.PLM(z, d, y, dreg, yreg, nfold=10, clu)","metadata":{"execution":{"iopub.status.busy":"2021-07-23T16:16:50.810932Z","iopub.execute_input":"2021-07-23T16:16:50.812148Z","iopub.status.idle":"2021-07-23T16:17:22.942306Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's organize the results in a table.","metadata":{}},{"cell_type":"code","source":"library(xtable)\n\ntable <- matrix(0,9,2)\ntable[1,1] <- as.numeric(est_baseline[1])\ntable[2,1] <- as.numeric(est_ols[1])\ntable[3,1]   <- as.numeric(DML2.lasso$coef.est)\ntable[4,1]   <- as.numeric(DML2.post$coef.est)\ntable[5,1]  <-as.numeric(DML2.lasso.cv$coef.est)\ntable[6,1] <-as.numeric(DML2.elnet$coef.est)\ntable[7,1] <-as.numeric(DML2.ridge$coef.est)\ntable[8,1] <-as.numeric(DML2.RF$coef.est)\ntable[9,1] <-as.numeric(DML2.best$coef.est)\ntable[1,2] <- as.numeric(est_baseline[2])\ntable[2,2] <- as.numeric(est_ols[2])\ntable[3,2]   <- as.numeric(DML2.lasso$se)\ntable[4,2]   <- as.numeric(DML2.post$se)\ntable[5,2]  <-as.numeric(DML2.lasso.cv$se)\ntable[6,2] <-as.numeric(DML2.elnet$se)\ntable[7,2] <-as.numeric(DML2.ridge$se)\ntable[8,2] <-as.numeric(DML2.RF$se)\ntable[9,2] <-as.numeric(DML2.best$se)\n\n# print results\ncolnames(table) <- c(\"Estimate\",\"Standard Error\")\nrownames(table) <- c(\"Baseline OLS\", \"Least Squares with controls\", \"Lasso\", \"Post-Lasso\", \"CV Lasso\",\"CV Elnet\", \"CV Ridge\", \"Random Forest\", \n                     \"Best\")\ntable","metadata":{"execution":{"iopub.status.busy":"2021-07-23T16:17:22.946075Z","iopub.execute_input":"2021-07-23T16:17:22.948656Z","iopub.status.idle":"2021-07-23T16:17:23.043516Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(table, digit=3)","metadata":{"execution":{"iopub.status.busy":"2021-07-23T16:17:23.045939Z","iopub.execute_input":"2021-07-23T16:17:23.047078Z","iopub.status.idle":"2021-07-23T16:17:23.058984Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tab<- xtable(table, digits=3)\nprint(tab, type=\"latex\")","metadata":{"execution":{"iopub.status.busy":"2021-07-23T16:17:23.060878Z","iopub.execute_input":"2021-07-23T16:17:23.061853Z","iopub.status.idle":"2021-07-23T16:17:23.083206Z"},"trusted":true},"execution_count":null,"outputs":[]}]}