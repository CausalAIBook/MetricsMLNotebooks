{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Regression Discontinuity\n",
        "This notebook illustrates the use of Regression Discontinuity in an empirical study. We analyze the effect of the antipoverty program *Progresa/Oportunidades* on the consumption behavior of families in Mexico in the early 2000s.\n",
        "\n",
        "The program was intended for families in extreme poverty and included financial incentives for participation in measures that improved the family's health, nutrition and children's education. The effect of this program is a widely studied problem in social and economic sciences and, according to the WHO, was a very successful measure in terms of reducing extreme poverty in Mexico.\n",
        "\n",
        "Eligibility for the program was determined based on a pre-intervention household poverty-index. Individuals above a certain threshold received the treatment (participation in the program) while individuals below the threshold were excluded and recorded as a control group. All observations above the threshold participated in the program, which makes the analysis fall into the standard (sharp) regression discontinuity design.\n",
        "\n",
        "First, we need to install and load some packages."
      ],
      "metadata": {
        "id": "-VRZnOBNA6o7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install rdd rdrobust\n",
        "import pandas as pd\n",
        "from sklearn.linear_model import LinearRegression, LassoCV\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from lightgbm import LGBMRegressor\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import patsy\n",
        "import math\n",
        "from rdd.rdd import optimal_bandwidth, rdd, truncated_data\n",
        "from rdrobust import rdrobust"
      ],
      "metadata": {
        "id": "1Yr5aL2yAgYN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We use a dataset assembled by [Calonico et al. (2014)](https://rdpackages.github.io/references/Calonico-Cattaneo-Titiunik_2014_ECMA--Supplemental.pdf) and follow the analysis in [Noack et al. (2023)](https://arxiv.org/pdf/2107.07942.pdf).\n",
        "\n",
        "First, we open the data and remove any observations that have NaN values."
      ],
      "metadata": {
        "id": "GH0wFmHSxnen"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"https://raw.githubusercontent.com/CausalAIBook/MetricsMLNotebooks/main/data/progresa.csv\", index_col=0)\n",
        "df = df.dropna()\n",
        "df.rename(columns={\"index\":\"pov_index\"}, inplace=True)\n",
        "print(\"Shape of Data:\")\n",
        "print(df.shape)\n",
        "print(\"Variable Names:\")\n",
        "print(df.columns)\n",
        "df.head()"
      ],
      "metadata": {
        "id": "Rzbv0XXCxxJt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The data set contains 1,944 observations for which full covariate information of 27 variables is available.\n",
        "\n",
        "We want to measure the local average treatment effect of program participation on four outcome variables. The outcome variables are food and non-food consumption of the recorded families at two points in time, one year and two years after the implementation of the program.\n",
        "\n",
        "The baseline covariates, recorded prior to program implementation, include the household's size; household head's age, sex, years of education and employment status; spouse's age and years of education; number of children not older than five years and their sex, and physical characteristics of the house: whether the house has cement floors, water connection, water connection inside the house, a bathroom, electricity, number of rooms, pre-intervention consumption, and an identifier of the urban locality in which the house is located.\n",
        "\n",
        "The data fits to the pattern of a sharp RD design, namely, all individuals that were below the cut-off index received no intervention, and all individuals above the cut-off were eligible to join the *progresa* program and thus participated."
      ],
      "metadata": {
        "id": "vGbvqQmpmoqV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Estimation without Covariates"
      ],
      "metadata": {
        "id": "9yvX75wy98g9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, we will perform a very simple RD estimation with a weighted linear regression. We use a triangular kernel, which assigns weights to observations based on their distance from the cutoff point. The weights decrease linearly as the distance from the cutoff point increases."
      ],
      "metadata": {
        "id": "bCueRzpuqNXn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def triangular_kernel(index, h):\n",
        "   weights = 1 - np.abs(index)/h\n",
        "   weights[weights < 0] = 0\n",
        "   return weights"
      ],
      "metadata": {
        "id": "1LAMZP540pLn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The parameter `h` is the bandwidth that controls the range of observations that receive non-zero weights. We use the `IKbandwidth` function from the `rdd` package that implements the *Imbens-Kalyanaraman* method. Another standard approach would be to use the standard deviation of `index`."
      ],
      "metadata": {
        "id": "N-I-EBps0ubO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "h = optimal_bandwidth(X=df.pov_index, Y=df.conspcfood_t1, cut = 0)"
      ],
      "metadata": {
        "id": "bFuzAouP04lO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We use the triangular kernel function to calculate weights for each observation. After that, we can fit two seperate linear regressions for both treatment and control groups."
      ],
      "metadata": {
        "id": "J9kU7tQ207A3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "weights = triangular_kernel(df.pov_index, h)\n",
        "model_treated, model_control = LinearRegression(), LinearRegression()\n",
        "model_treated.fit(y = df.loc[df.pov_index>0, \"conspcfood_t1\"].values.reshape(-1, 1),\n",
        "                  X = df.loc[df.pov_index>0, \"pov_index\"].values.reshape(-1, 1),\n",
        "                  sample_weight = weights[df.pov_index > 0])\n",
        "model_control.fit(y = df.loc[df.pov_index<0, \"conspcfood_t1\"].values.reshape(-1, 1),\n",
        "                  X = df.loc[df.pov_index<0, \"pov_index\"].values.reshape(-1, 1),\n",
        "                  sample_weight = weights[df.pov_index < 0]);"
      ],
      "metadata": {
        "id": "cjc7f7F6qM36"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The treatment effect at the cutoff point is estimated as the difference between the predictions of the two models at the cutoff point."
      ],
      "metadata": {
        "id": "MC5vPB-I1jeH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cutoff = 0\n",
        "treatment_effect = model_treated.predict(np.array([cutoff]).reshape(-1, 1)) -\\\n",
        "                   model_control.predict(np.array([cutoff]).reshape(-1, 1))\n",
        "treatment_effect[0,0]"
      ],
      "metadata": {
        "id": "279my1C8o9a3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We estimate that the participation in the program reduced food consumption by $22.1$ units in the year following the intervention.The following plot visualizes the two weighted regressions at the cut-off for the last outcome variable (for food consumption in `t1`). We can clearly see the \"jump\" at the cut-off, which is our LATE."
      ],
      "metadata": {
        "id": "uW6PYdz-BESB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.scatter(df.pov_index, df.conspcfood_t1, s=10, color='black')  # s controls the size\n",
        "neg_xval = np.linspace(-0.1,0,100)\n",
        "neg_line = model_control.predict(neg_xval.reshape(-1,1))\n",
        "pos_xval = np.linspace(0,0.1,100)\n",
        "pos_line = model_treated.predict(pos_xval.reshape(-1,1))\n",
        "plt.plot(neg_xval, neg_line, linewidth=3, label=\"Control Regression\")\n",
        "plt.plot(pos_xval, pos_line, linewidth=3, label=\"Treated Regression\")\n",
        "plt.axvline(x=0, color='red', linestyle='--', label=\"Cut-Off\")\n",
        "plt.legend()\n",
        "plt.xlim(-0.1,0.1)\n",
        "plt.ylim(250,350)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "uvONPb44A56z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can repeat the estimation using the `rdd` package, which yields us an estimate as well as a confidence band calculated according to the formulas presented in the book. We look at all four targets."
      ],
      "metadata": {
        "id": "W6MN7zBmAxn9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.pov_index.duplicated().sum()"
      ],
      "metadata": {
        "id": "Xkqz2Bk_56v4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result = []\n",
        "for outcome in [\"conspcfood_t1\", \"conspcnonfood_t1\", \"conspcfood_t2\", \"conspcnonfood_t2\"]:\n",
        "  rdd_result = rdrobust(x=df.pov_index, y=df[outcome], rho=1, masspoints=\"off\")\n",
        "  result.append([rdd_result.coef.iloc[0].values[0], rdd_result.se.iloc[2].values[0]])\n",
        "res_dataframe = pd.DataFrame(result, columns=[\"LATE\", \"s.e.\"],\n",
        "                             index = [\"Food T_1\", \"Non-Food T_1\", \"Food T_2\", \"Non-Food T_2\"])\n",
        "res_dataframe"
      ],
      "metadata": {
        "id": "6rLo9c_YGWIq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "While the effects in the first year after the intervention are negative, we observe significant positive effects in the second year after an individual or household was accepted in the *Progresa* program. This is in accordance to the previous analysis of this dataset. One possible explanation for this is that the program households have more money and can thus afford more. This was the desired effect of the program to combat hunger and extreme poverty."
      ],
      "metadata": {
        "id": "BzzCc3oWZycJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Estimation with Covariates\n",
        "\n",
        "For the identification and estimation of the average treatment effect at the cutoff value no covariate information is required except the running variable, but nevertheless in many applications additional covariates are collected which might be exploited for the analysis to improve the efficiency of the estimates.\n",
        "\n",
        "The standard approach is simply to take up the regressors in the weighted least squares regression."
      ],
      "metadata": {
        "id": "hDEf53bE-Aki"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_treated, model_control = LinearRegression(), LinearRegression()\n",
        "model_treated.fit(y = df.loc[df.pov_index>0, \"conspcfood_t1\"].values.reshape(-1, 1),\n",
        "                  X = df.loc[df.pov_index>0, [\"pov_index\", \"hhownhouse\", \"headage\", \"heademp\", \"headeduc\"]],\n",
        "                  sample_weight = weights[df.pov_index > 0])\n",
        "model_control.fit(y = df.loc[df.pov_index<0, \"conspcfood_t1\"].values.reshape(-1, 1),\n",
        "                  X = df.loc[df.pov_index<0, [\"pov_index\", \"hhownhouse\", \"headage\", \"heademp\", \"headeduc\"]],\n",
        "                  sample_weight = weights[df.pov_index < 0]);\n",
        "pred_t = model_treated.predict(pd.DataFrame({\"pov_index\" : cutoff,\n",
        "                                             \"hhownhouse\" : np.average(df.loc[df.pov_index > 0,\"hhownhouse\"], weights = weights[df.pov_index > 0]),\n",
        "                                             \"headage\" : np.average(df.loc[df.pov_index > 0,\"headage\"], weights = weights[df.pov_index > 0]),\n",
        "                                             \"heademp\" : np.average(df.loc[df.pov_index > 0,\"heademp\"], weights = weights[df.pov_index > 0]),\n",
        "                                             \"headeduc\" : np.average(df.loc[df.pov_index > 0,\"headeduc\"], weights = weights[df.pov_index > 0])},\n",
        "                                            index=[0]))\n",
        "pred_c = model_control.predict(pd.DataFrame({\"pov_index\" : cutoff,\n",
        "                                             \"hhownhouse\" : np.average(df.loc[df.pov_index < 0,\"hhownhouse\"], weights = weights[df.pov_index < 0]),\n",
        "                                             \"headage\" : np.average(df.loc[df.pov_index < 0,\"headage\"], weights = weights[df.pov_index < 0]),\n",
        "                                             \"heademp\" : np.average(df.loc[df.pov_index < 0,\"heademp\"], weights = weights[df.pov_index < 0]),\n",
        "                                             \"headeduc\" : np.average(df.loc[df.pov_index < 0,\"headeduc\"], weights = weights[df.pov_index < 0])},\n",
        "                                            index=[0]))\n",
        "treatment_effect = pred_t - pred_c\n",
        "treatment_effect[0][0]"
      ],
      "metadata": {
        "id": "JRdUQ8gcsGCg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Including these selected covariates does not have a significant impact on the LATE estimation. Again, we can also use `rdd` to repeat the estimation with all other outcomes.\n"
      ],
      "metadata": {
        "id": "Mlc6ZqDuMfl8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "res_dataframe"
      ],
      "metadata": {
        "id": "GZdzDHEdOt-j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result = []\n",
        "for outcome in [\"conspcfood_t1\", \"conspcnonfood_t1\", \"conspcfood_t2\", \"conspcnonfood_t2\"]:\n",
        "  rdd_result = rdrobust(x=df.pov_index, y=df[outcome], rho=1, masspoints=\"off\",\n",
        "                        covs=df.iloc[:,[0,1,2,3,4,5,6,7,10,11,12,13,14,15,16,18,21]])\n",
        "  result.append([rdd_result.coef.iloc[0].values[0], rdd_result.se.iloc[2].values[0]])\n",
        "res_dataframe_adj = pd.DataFrame(result, columns=[\"LATE\", \"s.e.\"],\n",
        "                                 index = [\"Food T_1\", \"Non-Food T_1\", \"Food T_2\", \"Non-Food T_2\"])\n",
        "res_dataframe_adj[\"% reduction\"] = (res_dataframe_adj[\"s.e.\"] - res_dataframe[\"s.e.\"]) * 100 / res_dataframe[\"s.e.\"]\n",
        "res_dataframe_adj"
      ],
      "metadata": {
        "id": "OYQuZcvjyYx6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Overall, the adjustment by only a few covariates has not changed the estimated coefficient much from the result without covariates. However, including covariates does reduce the standard deviation of the estimation."
      ],
      "metadata": {
        "id": "5q8S0wNhabWy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Estimation using ML"
      ],
      "metadata": {
        "id": "9U8UkHmv-D-0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As discussed in the book, including many covariates in RDD estimation can be beneficial for multiple reasons:\n",
        "1. **Efficiency and power improvements**: As in randomized control trials, using covariates can increase efficiency and improve power.\n",
        "2. **Auxiliary information**: In RDD the score determines the treatment assignment and measurement errors in the running variable can distort the results. Additional covariates can be exploited to overcome these issues or to deal with missing data problems.\n",
        "3. **Treatment effect heterogeneity**: Covariates can be used to define subgroups in which the treatment effects differ.\n",
        "4. **Other parameters of interest and extrapolation**: As the identified treatment effect in RDD is local at the cutoff, additional covariates might help for extrapolation of the treatment effects or identify other causal parameters.\n",
        "\n",
        "However, including a high number of covariates also comes with additional challenges, such as variables selection, non-linearities or interactions between covariates. The best way to overcome these is the use of modern ML methods.\n",
        "\n",
        "There are multiple ways to implement the estimators presented in the book, we will closely follow the analysis of [Noack et al. (2023)](https://arxiv.org/pdf/2107.07942.pdf). We set up running variable and outcome as above. The baseline covariates will be all the other variables in the data."
      ],
      "metadata": {
        "id": "NiYSglH9E0Er"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Running Variable and Outcome\n",
        "investigated_outcome = \"conspcfood_t1\"\n",
        "df_ml = df.rename(columns={\"pov_index\": \"X\", investigated_outcome: \"Y\"})\n",
        "\n",
        "# Baseline covariates including consumption\n",
        "b_covs = df_ml.columns[[0,1,2,3,4,5,6,7,10,11,12,13,14,15,16,18,21]]\n",
        "\n",
        "# Fixed effects for localities\n",
        "i_fe = pd.get_dummies(df_ml['clus'], drop_first=True)\n",
        "\n",
        "# Flexible covariates including localities indicators\n",
        "f_covs = patsy.dmatrix('~ (' + ' + '.join(b_covs) + ')**2', data=df_ml, return_type='dataframe')\n",
        "\n",
        "# Dropping the intercept column that is automatically added by patsy\n",
        "f_covs = f_covs.iloc[:, 1:]\n",
        "\n",
        "Z_lasso = pd.concat([i_fe, f_covs], axis=1)"
      ],
      "metadata": {
        "id": "n2uoMwzkCq4P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will use the package `rdrobust` for the RD estimation. Before starting the DML procedure, we have to estimate a bandwidth to restrict the samples in the first stage estimation."
      ],
      "metadata": {
        "id": "2n8yvua4Ns_A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "h_fs = rdrobust(y = df_ml.Y, x=df_ml.X, masspoints=\"off\").bws.values[1,0]"
      ],
      "metadata": {
        "id": "VIO-PQEtOKob"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The next chunk sets up the crossfitting and estimates the function $\\eta(Z)$, which we will use to adjust $Y$ for the second stage. We use Random Forest, a Boosting implementation, Linear Regression and Lasso with both a baseline and flexible covariate structure."
      ],
      "metadata": {
        "id": "nm8BC6JTQnV7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def first_stage():\n",
        "  # Set up the cross-fitting\n",
        "  n = df_ml.shape[0]\n",
        "  Kf = 5 # Number of folds\n",
        "  sampleframe = np.repeat(range(0, Kf), math.ceil(n/Kf))\n",
        "  cfgroup = np.random.choice(sampleframe, size=n, replace=False)\n",
        "\n",
        "  # Matrix to store eta predictions\n",
        "  eta_fit = np.empty((n, 5))\n",
        "\n",
        "  # Create vector of observations to be considered in the first stage model\n",
        "  weights = np.abs(df_ml.X)<h_fs\n",
        "\n",
        "  for k in range(Kf):\n",
        "    fold = (cfgroup == k)\n",
        "\n",
        "    data_treated = df_ml.loc[(df_ml.X>0) & ~fold & (weights>0),]\n",
        "    data_control = df_ml.loc[(df_ml.X<0) & ~fold & (weights>0),]\n",
        "\n",
        "    data_fold = df_ml.loc[fold,]\n",
        "\n",
        "    rf1 = RandomForestRegressor(max_features = 4, n_estimators = 1000)\n",
        "    rf1.fit(y=data_treated.Y, X=data_treated[b_covs])\n",
        "    rf0 = RandomForestRegressor(max_features = 4, n_estimators = 1000)\n",
        "    rf0.fit(y=data_control.Y, X=data_control[b_covs])\n",
        "    eta_fit[fold,0] = (rf1.predict(data_fold[b_covs]) + rf0.predict(data_fold[b_covs]))/2\n",
        "\n",
        "    lgbm1 = LGBMRegressor(verbosity=-1)\n",
        "    lgbm1.fit(y=data_treated.Y, X=data_treated[b_covs])\n",
        "    lgbm0 = LGBMRegressor(verbosity=-1)\n",
        "    lgbm0.fit(y=data_control.Y, X=data_control[b_covs])\n",
        "    eta_fit[fold,1] = (lgbm1.predict(data_fold[b_covs]) + lgbm0.predict(data_fold[b_covs]))/2\n",
        "\n",
        "    lm1 = LinearRegression()\n",
        "    lm1.fit(y=data_treated.Y, X=data_treated[b_covs])\n",
        "    lm0 = LinearRegression()\n",
        "    lm0.fit(y=data_control.Y, X=data_control[b_covs])\n",
        "    eta_fit[fold,2] = (lm1.predict(data_fold[b_covs]) + lm0.predict(data_fold[b_covs]))/2\n",
        "\n",
        "    las_base1 = LassoCV()\n",
        "    las_base1.fit(y=data_treated.Y, X=data_treated[b_covs])\n",
        "    las_base0 = LassoCV()\n",
        "    las_base0.fit(y=data_control.Y, X=data_control[b_covs])\n",
        "    eta_fit[fold,3] = (las_base1.predict(data_fold[b_covs]) + las_base0.predict(data_fold[b_covs]))/2\n",
        "\n",
        "    X_flex_treated = pd.concat([Z_lasso.loc[data_treated.index],data_treated[b_covs]], axis=1)\n",
        "    X_flex_control = pd.concat([Z_lasso.loc[data_control.index],data_control[b_covs]], axis=1)\n",
        "    X_flex_fold = pd.concat([Z_lasso.loc[data_fold.index],data_fold[b_covs]], axis=1)\n",
        "    X_flex_treated.columns = X_flex_treated.columns.astype(str)\n",
        "    X_flex_control.columns = X_flex_control.columns.astype(str)\n",
        "    X_flex_fold.columns = X_flex_fold.columns.astype(str)\n",
        "    las_flex1 = LassoCV()\n",
        "    las_flex1.fit(y=data_treated.Y, X=X_flex_treated)\n",
        "    las_flex0 = LassoCV()\n",
        "    las_flex0.fit(y=data_control.Y, X=X_flex_control)\n",
        "    eta_fit[fold,4] = (las_flex1.predict(X_flex_fold) + \\\n",
        "                      las_flex0.predict(X_flex_fold)) / 2\n",
        "\n",
        "  return eta_fit\n",
        "\n",
        "eta_fit = first_stage()"
      ],
      "metadata": {
        "id": "y-tGMe5iQhVd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "With the estimated $\\hat{\\eta}(Z)$ we can correct for confounding in $Y$ and now run the RD estimation as second stage again."
      ],
      "metadata": {
        "id": "ybTRUohWi_xE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "methods = [\"Random Forest\", \"Gradient Boosting\", \"Linear Regression\",\n",
        "           \"Lasso Baseline\", \"Lasso Flexible\"]\n",
        "\n",
        "def second_stage(eta_fit):\n",
        "  adj_results = []\n",
        "  for i in range(len(methods)):\n",
        "    M_Y = df_ml.Y - eta_fit[:,i]\n",
        "    rd_call = rdrobust(y=M_Y, x=df_ml.X, masspoints = \"off\")\n",
        "    adj_results.append([rd_call.coef.iloc[0].values[0],\n",
        "                        rd_call.se.iloc[2].values[0]])\n",
        "  return adj_results\n",
        "\n",
        "adj_frame = pd.DataFrame(second_stage(eta_fit), columns=[\"LATE\", \"s.e.\"],\n",
        "                         index=methods)\n",
        "adj_frame"
      ],
      "metadata": {
        "id": "WdJkfePmx4iN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, we create a small simulation study with only $R=20$ repetitions to show the variance reducing effect of the inclusion of ML-based estimators for the covariates. The next block runs up to ten minutes."
      ],
      "metadata": {
        "id": "dWRrjcieIKwk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "estimates = [adj_frame[\"LATE\"].values]\n",
        "std_err = [adj_frame[\"s.e.\"].values]\n",
        "R = 19\n",
        "\n",
        "for i in range(R):\n",
        "  eta_fit = first_stage()\n",
        "  adj_results = np.array(second_stage(eta_fit))\n",
        "  estimates.append(adj_results[:,0])\n",
        "  std_err.append(adj_results[:,1])"
      ],
      "metadata": {
        "id": "3DxmPT9LIJ3E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We aggregate the median of the estimates, the mean of the standard errors and also calculate the mean reduction of standard error compared to the \"no covariates\" estimation. We see, that including covariates can reduce the standard error of estimation around 15-20%."
      ],
      "metadata": {
        "id": "W_ybpwiMS1MH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "med_est = np.median(np.array(estimates),axis=0)\n",
        "mean_se = np.array(std_err).mean(axis=0)\n",
        "adj_frame = pd.DataFrame(np.c_[med_est,mean_se], index= methods, columns=[\"LATE\", \"s.e.\"])\n",
        "adj_frame[\"% reduction\"] = (adj_frame[\"s.e.\"] - res_dataframe.loc[\"Food T_1\", \"s.e.\"]) * 100 / res_dataframe.loc[\"Food T_1\", \"s.e.\"]\n",
        "adj_frame.loc[\"Linear Adjusted (no cross-fit)\"] = res_dataframe_adj.loc[\"Food T_1\"]\n",
        "adj_frame"
      ],
      "metadata": {
        "id": "bzp90cR4I6RL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## We now repeat the exercise for the other outcomes (excluding the simulation)."
      ],
      "metadata": {
        "id": "0ioABDq3TK_o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Non-Food Consumption (Year 1)"
      ],
      "metadata": {
        "id": "g-dLrCgrTm3n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Running Variable and Outcome\n",
        "investigated_outcome = \"conspcnonfood_t1\"\n",
        "df_ml = df.rename(columns={\"pov_index\": \"X\", investigated_outcome: \"Y\"})\n",
        "\n",
        "# Baseline covariates including consumption\n",
        "b_covs = df_ml.columns[[0,1,2,3,4,5,6,7,10,11,12,13,14,15,16,18,21]]\n",
        "\n",
        "# Fixed effects for localities\n",
        "i_fe = pd.get_dummies(df_ml['clus'], drop_first=True)\n",
        "\n",
        "# Flexible covariates including localities indicators\n",
        "f_covs = patsy.dmatrix('~ (' + ' + '.join(b_covs) + ')**2', data=df_ml, return_type='dataframe')\n",
        "\n",
        "# Dropping the intercept column that is automatically added by patsy\n",
        "f_covs = f_covs.iloc[:, 1:]\n",
        "\n",
        "Z_lasso = pd.concat([i_fe, f_covs], axis=1)\n",
        "\n",
        "h_fs = rdrobust(y = df_ml.Y, x=df_ml.X, masspoints=\"off\").bws.values[1,0]\n",
        "\n",
        "def first_stage():\n",
        "  # Set up the cross-fitting\n",
        "  n = df_ml.shape[0]\n",
        "  Kf = 5 # Number of folds\n",
        "  sampleframe = np.repeat(range(0, Kf), math.ceil(n/Kf))\n",
        "  cfgroup = np.random.choice(sampleframe, size=n, replace=False)\n",
        "\n",
        "  # Matrix to store eta predictions\n",
        "  eta_fit = np.empty((n, 5))\n",
        "\n",
        "  # Create vector of observations to be considered in the first stage model\n",
        "  weights = np.abs(df_ml.X)<h_fs\n",
        "\n",
        "  for k in range(Kf):\n",
        "    fold = (cfgroup == k)\n",
        "\n",
        "    data_treated = df_ml.loc[(df_ml.X>0) & ~fold & (weights>0),]\n",
        "    data_control = df_ml.loc[(df_ml.X<0) & ~fold & (weights>0),]\n",
        "\n",
        "    data_fold = df_ml.loc[fold,]\n",
        "\n",
        "    rf1 = RandomForestRegressor(max_features = 4, n_estimators = 1000)\n",
        "    rf1.fit(y=data_treated.Y, X=data_treated[b_covs])\n",
        "    rf0 = RandomForestRegressor(max_features = 4, n_estimators = 1000)\n",
        "    rf0.fit(y=data_control.Y, X=data_control[b_covs])\n",
        "    eta_fit[fold,0] = (rf1.predict(data_fold[b_covs]) + rf0.predict(data_fold[b_covs]))/2\n",
        "\n",
        "    lgbm1 = LGBMRegressor(verbosity=-1)\n",
        "    lgbm1.fit(y=data_treated.Y, X=data_treated[b_covs])\n",
        "    lgbm0 = LGBMRegressor(verbosity=-1)\n",
        "    lgbm0.fit(y=data_control.Y, X=data_control[b_covs])\n",
        "    eta_fit[fold,1] = (lgbm1.predict(data_fold[b_covs]) + lgbm0.predict(data_fold[b_covs]))/2\n",
        "\n",
        "    lm1 = LinearRegression()\n",
        "    lm1.fit(y=data_treated.Y, X=data_treated[b_covs])\n",
        "    lm0 = LinearRegression()\n",
        "    lm0.fit(y=data_control.Y, X=data_control[b_covs])\n",
        "    eta_fit[fold,2] = (lm1.predict(data_fold[b_covs]) + lm0.predict(data_fold[b_covs]))/2\n",
        "\n",
        "    las_base1 = LassoCV()\n",
        "    las_base1.fit(y=data_treated.Y, X=data_treated[b_covs])\n",
        "    las_base0 = LassoCV()\n",
        "    las_base0.fit(y=data_control.Y, X=data_control[b_covs])\n",
        "    eta_fit[fold,3] = (las_base1.predict(data_fold[b_covs]) + las_base0.predict(data_fold[b_covs]))/2\n",
        "\n",
        "    X_flex_treated = pd.concat([Z_lasso.loc[data_treated.index],data_treated[b_covs]], axis=1)\n",
        "    X_flex_control = pd.concat([Z_lasso.loc[data_control.index],data_control[b_covs]], axis=1)\n",
        "    X_flex_fold = pd.concat([Z_lasso.loc[data_fold.index],data_fold[b_covs]], axis=1)\n",
        "    X_flex_treated.columns = X_flex_treated.columns.astype(str)\n",
        "    X_flex_control.columns = X_flex_control.columns.astype(str)\n",
        "    X_flex_fold.columns = X_flex_fold.columns.astype(str)\n",
        "    las_flex1 = LassoCV()\n",
        "    las_flex1.fit(y=data_treated.Y, X=X_flex_treated)\n",
        "    las_flex0 = LassoCV()\n",
        "    las_flex0.fit(y=data_control.Y, X=X_flex_control)\n",
        "    eta_fit[fold,4] = (las_flex1.predict(X_flex_fold) + \\\n",
        "                      las_flex0.predict(X_flex_fold)) / 2\n",
        "\n",
        "  return eta_fit\n",
        "\n",
        "eta_fit = first_stage()\n",
        "\n",
        "methods = [\"Random Forest\", \"Gradient Boosting\", \"Linear Regression\",\n",
        "           \"Lasso Baseline\", \"Lasso Flexible\"]\n",
        "\n",
        "def second_stage(eta_fit):\n",
        "  adj_results = []\n",
        "  for i in range(len(methods)):\n",
        "    M_Y = df_ml.Y - eta_fit[:,i]\n",
        "    rd_call = rdrobust(y=M_Y, x=df_ml.X, masspoints = \"off\")\n",
        "    adj_results.append([rd_call.coef.iloc[0].values[0],\n",
        "                        rd_call.se.iloc[2].values[0]])\n",
        "  return adj_results\n",
        "\n",
        "adj_frame = pd.DataFrame(second_stage(eta_fit), columns=[\"LATE\", \"s.e.\"],\n",
        "                         index=methods)\n",
        "adj_frame"
      ],
      "metadata": {
        "id": "zcKoNbJeTpSg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Food Consumption (Year 2)"
      ],
      "metadata": {
        "id": "4AR3BRv7Tp2D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Running Variable and Outcome\n",
        "investigated_outcome = \"conspcfood_t2\"\n",
        "df_ml = df.rename(columns={\"pov_index\": \"X\", investigated_outcome: \"Y\"})\n",
        "\n",
        "# Baseline covariates including consumption\n",
        "b_covs = df_ml.columns[[0,1,2,3,4,5,6,7,10,11,12,13,14,15,16,18,21]]\n",
        "\n",
        "# Fixed effects for localities\n",
        "i_fe = pd.get_dummies(df_ml['clus'], drop_first=True)\n",
        "\n",
        "# Flexible covariates including localities indicators\n",
        "f_covs = patsy.dmatrix('~ (' + ' + '.join(b_covs) + ')**2', data=df_ml, return_type='dataframe')\n",
        "\n",
        "# Dropping the intercept column that is automatically added by patsy\n",
        "f_covs = f_covs.iloc[:, 1:]\n",
        "\n",
        "Z_lasso = pd.concat([i_fe, f_covs], axis=1)\n",
        "\n",
        "h_fs = rdrobust(y = df_ml.Y, x=df_ml.X, masspoints=\"off\").bws.values[1,0]\n",
        "\n",
        "def first_stage():\n",
        "  # Set up the cross-fitting\n",
        "  n = df_ml.shape[0]\n",
        "  Kf = 5 # Number of folds\n",
        "  sampleframe = np.repeat(range(0, Kf), math.ceil(n/Kf))\n",
        "  cfgroup = np.random.choice(sampleframe, size=n, replace=False)\n",
        "\n",
        "  # Matrix to store eta predictions\n",
        "  eta_fit = np.empty((n, 5))\n",
        "\n",
        "  # Create vector of observations to be considered in the first stage model\n",
        "  weights = np.abs(df_ml.X)<h_fs\n",
        "\n",
        "  for k in range(Kf):\n",
        "    fold = (cfgroup == k)\n",
        "\n",
        "    data_treated = df_ml.loc[(df_ml.X>0) & ~fold & (weights>0),]\n",
        "    data_control = df_ml.loc[(df_ml.X<0) & ~fold & (weights>0),]\n",
        "\n",
        "    data_fold = df_ml.loc[fold,]\n",
        "\n",
        "    rf1 = RandomForestRegressor(max_features = 4, n_estimators = 1000)\n",
        "    rf1.fit(y=data_treated.Y, X=data_treated[b_covs])\n",
        "    rf0 = RandomForestRegressor(max_features = 4, n_estimators = 1000)\n",
        "    rf0.fit(y=data_control.Y, X=data_control[b_covs])\n",
        "    eta_fit[fold,0] = (rf1.predict(data_fold[b_covs]) + rf0.predict(data_fold[b_covs]))/2\n",
        "\n",
        "    lgbm1 = LGBMRegressor(verbosity=-1)\n",
        "    lgbm1.fit(y=data_treated.Y, X=data_treated[b_covs])\n",
        "    lgbm0 = LGBMRegressor(verbosity=-1)\n",
        "    lgbm0.fit(y=data_control.Y, X=data_control[b_covs])\n",
        "    eta_fit[fold,1] = (lgbm1.predict(data_fold[b_covs]) + lgbm0.predict(data_fold[b_covs]))/2\n",
        "\n",
        "    lm1 = LinearRegression()\n",
        "    lm1.fit(y=data_treated.Y, X=data_treated[b_covs])\n",
        "    lm0 = LinearRegression()\n",
        "    lm0.fit(y=data_control.Y, X=data_control[b_covs])\n",
        "    eta_fit[fold,2] = (lm1.predict(data_fold[b_covs]) + lm0.predict(data_fold[b_covs]))/2\n",
        "\n",
        "    las_base1 = LassoCV()\n",
        "    las_base1.fit(y=data_treated.Y, X=data_treated[b_covs])\n",
        "    las_base0 = LassoCV()\n",
        "    las_base0.fit(y=data_control.Y, X=data_control[b_covs])\n",
        "    eta_fit[fold,3] = (las_base1.predict(data_fold[b_covs]) + las_base0.predict(data_fold[b_covs]))/2\n",
        "\n",
        "    X_flex_treated = pd.concat([Z_lasso.loc[data_treated.index],data_treated[b_covs]], axis=1)\n",
        "    X_flex_control = pd.concat([Z_lasso.loc[data_control.index],data_control[b_covs]], axis=1)\n",
        "    X_flex_fold = pd.concat([Z_lasso.loc[data_fold.index],data_fold[b_covs]], axis=1)\n",
        "    X_flex_treated.columns = X_flex_treated.columns.astype(str)\n",
        "    X_flex_control.columns = X_flex_control.columns.astype(str)\n",
        "    X_flex_fold.columns = X_flex_fold.columns.astype(str)\n",
        "    las_flex1 = LassoCV()\n",
        "    las_flex1.fit(y=data_treated.Y, X=X_flex_treated)\n",
        "    las_flex0 = LassoCV()\n",
        "    las_flex0.fit(y=data_control.Y, X=X_flex_control)\n",
        "    eta_fit[fold,4] = (las_flex1.predict(X_flex_fold) + \\\n",
        "                      las_flex0.predict(X_flex_fold)) / 2\n",
        "\n",
        "  return eta_fit\n",
        "\n",
        "eta_fit = first_stage()\n",
        "\n",
        "methods = [\"Random Forest\", \"Gradient Boosting\", \"Linear Regression\",\n",
        "           \"Lasso Baseline\", \"Lasso Flexible\"]\n",
        "\n",
        "def second_stage(eta_fit):\n",
        "  adj_results = []\n",
        "  for i in range(len(methods)):\n",
        "    M_Y = df_ml.Y - eta_fit[:,i]\n",
        "    rd_call = rdrobust(y=M_Y, x=df_ml.X, masspoints = \"off\")\n",
        "    adj_results.append([rd_call.coef.iloc[0].values[0],\n",
        "                        rd_call.se.iloc[2].values[0]])\n",
        "  return adj_results\n",
        "\n",
        "adj_frame = pd.DataFrame(second_stage(eta_fit), columns=[\"LATE\", \"s.e.\"],\n",
        "                         index=methods)\n",
        "adj_frame"
      ],
      "metadata": {
        "id": "aoFEC4l-TsIV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Non-Food Consumption (Year 2)"
      ],
      "metadata": {
        "id": "jGG6Nb_gTsdu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Running Variable and Outcome\n",
        "investigated_outcome = \"conspcnonfood_t2\"\n",
        "df_ml = df.rename(columns={\"pov_index\": \"X\", investigated_outcome: \"Y\"})\n",
        "\n",
        "# Baseline covariates including consumption\n",
        "b_covs = df_ml.columns[[0,1,2,3,4,5,6,7,10,11,12,13,14,15,16,18,21]]\n",
        "\n",
        "# Fixed effects for localities\n",
        "i_fe = pd.get_dummies(df_ml['clus'], drop_first=True)\n",
        "\n",
        "# Flexible covariates including localities indicators\n",
        "f_covs = patsy.dmatrix('~ (' + ' + '.join(b_covs) + ')**2', data=df_ml, return_type='dataframe')\n",
        "\n",
        "# Dropping the intercept column that is automatically added by patsy\n",
        "f_covs = f_covs.iloc[:, 1:]\n",
        "\n",
        "Z_lasso = pd.concat([i_fe, f_covs], axis=1)\n",
        "\n",
        "h_fs = rdrobust(y = df_ml.Y, x=df_ml.X, masspoints=\"off\").bws.values[1,0]\n",
        "\n",
        "def first_stage():\n",
        "  # Set up the cross-fitting\n",
        "  n = df_ml.shape[0]\n",
        "  Kf = 5 # Number of folds\n",
        "  sampleframe = np.repeat(range(0, Kf), math.ceil(n/Kf))\n",
        "  cfgroup = np.random.choice(sampleframe, size=n, replace=False)\n",
        "\n",
        "  # Matrix to store eta predictions\n",
        "  eta_fit = np.empty((n, 5))\n",
        "\n",
        "  # Create vector of observations to be considered in the first stage model\n",
        "  weights = np.abs(df_ml.X)<h_fs\n",
        "\n",
        "  for k in range(Kf):\n",
        "    fold = (cfgroup == k)\n",
        "\n",
        "    data_treated = df_ml.loc[(df_ml.X>0) & ~fold & (weights>0),]\n",
        "    data_control = df_ml.loc[(df_ml.X<0) & ~fold & (weights>0),]\n",
        "\n",
        "    data_fold = df_ml.loc[fold,]\n",
        "\n",
        "    rf1 = RandomForestRegressor(max_features = 4, n_estimators = 1000)\n",
        "    rf1.fit(y=data_treated.Y, X=data_treated[b_covs])\n",
        "    rf0 = RandomForestRegressor(max_features = 4, n_estimators = 1000)\n",
        "    rf0.fit(y=data_control.Y, X=data_control[b_covs])\n",
        "    eta_fit[fold,0] = (rf1.predict(data_fold[b_covs]) + rf0.predict(data_fold[b_covs]))/2\n",
        "\n",
        "    lgbm1 = LGBMRegressor(verbosity=-1)\n",
        "    lgbm1.fit(y=data_treated.Y, X=data_treated[b_covs])\n",
        "    lgbm0 = LGBMRegressor(verbosity=-1)\n",
        "    lgbm0.fit(y=data_control.Y, X=data_control[b_covs])\n",
        "    eta_fit[fold,1] = (lgbm1.predict(data_fold[b_covs]) + lgbm0.predict(data_fold[b_covs]))/2\n",
        "\n",
        "    lm1 = LinearRegression()\n",
        "    lm1.fit(y=data_treated.Y, X=data_treated[b_covs])\n",
        "    lm0 = LinearRegression()\n",
        "    lm0.fit(y=data_control.Y, X=data_control[b_covs])\n",
        "    eta_fit[fold,2] = (lm1.predict(data_fold[b_covs]) + lm0.predict(data_fold[b_covs]))/2\n",
        "\n",
        "    las_base1 = LassoCV()\n",
        "    las_base1.fit(y=data_treated.Y, X=data_treated[b_covs])\n",
        "    las_base0 = LassoCV()\n",
        "    las_base0.fit(y=data_control.Y, X=data_control[b_covs])\n",
        "    eta_fit[fold,3] = (las_base1.predict(data_fold[b_covs]) + las_base0.predict(data_fold[b_covs]))/2\n",
        "\n",
        "    X_flex_treated = pd.concat([Z_lasso.loc[data_treated.index],data_treated[b_covs]], axis=1)\n",
        "    X_flex_control = pd.concat([Z_lasso.loc[data_control.index],data_control[b_covs]], axis=1)\n",
        "    X_flex_fold = pd.concat([Z_lasso.loc[data_fold.index],data_fold[b_covs]], axis=1)\n",
        "    X_flex_treated.columns = X_flex_treated.columns.astype(str)\n",
        "    X_flex_control.columns = X_flex_control.columns.astype(str)\n",
        "    X_flex_fold.columns = X_flex_fold.columns.astype(str)\n",
        "    las_flex1 = LassoCV()\n",
        "    las_flex1.fit(y=data_treated.Y, X=X_flex_treated)\n",
        "    las_flex0 = LassoCV()\n",
        "    las_flex0.fit(y=data_control.Y, X=X_flex_control)\n",
        "    eta_fit[fold,4] = (las_flex1.predict(X_flex_fold) + \\\n",
        "                      las_flex0.predict(X_flex_fold)) / 2\n",
        "\n",
        "  return eta_fit\n",
        "\n",
        "eta_fit = first_stage()\n",
        "\n",
        "methods = [\"Random Forest\", \"Gradient Boosting\", \"Linear Regression\",\n",
        "           \"Lasso Baseline\", \"Lasso Flexible\"]\n",
        "\n",
        "def second_stage(eta_fit):\n",
        "  adj_results = []\n",
        "  for i in range(len(methods)):\n",
        "    M_Y = df_ml.Y - eta_fit[:,i]\n",
        "    rd_call = rdrobust(y=M_Y, x=df_ml.X, masspoints = \"off\")\n",
        "    adj_results.append([rd_call.coef.iloc[0].values[0],\n",
        "                        rd_call.se.iloc[2].values[0]])\n",
        "  return adj_results\n",
        "\n",
        "adj_frame = pd.DataFrame(second_stage(eta_fit), columns=[\"LATE\", \"s.e.\"],\n",
        "                         index=methods)\n",
        "adj_frame"
      ],
      "metadata": {
        "id": "B9lCmaP1TvVz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}