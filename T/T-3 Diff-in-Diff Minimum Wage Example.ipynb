{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rsOnp1Y-TJy_"
      },
      "source": [
        "# Minimum Wage Example Notebook with DiD"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "trvqH1pjTJpR"
      },
      "source": [
        "This notebook implements Difference-in-Differences in an application on\n",
        "the effect of minimum wage changes on teen employment. We use data from\n",
        "[Callaway\n",
        "(2022)](https://bcallaway11.github.io/files/Callaway-Chapter-2022/main.pdf). The data are annual county level data from the United States covering 2001 to 2007. The outcome variable is log county-level teen employment, and the treatment variable is an indicator for whether the county has a minimum wage above the federal minimum wage. Note that this definition of the treatment variable makes the analysis straightforward but ignores the nuances of the exact value of the minimum wage in each county and how far those values are from the federal minimum. The data also include county population and county average annual pay.\n",
        "See [Callaway and Santâ€™Anna\n",
        "(2021)](https://www.sciencedirect.com/science/article/abs/pii/S0304407620303948)\n",
        "for additional details on the data.\n",
        "\n",
        "First, we will load some libraries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 201
        },
        "id": "FFlG2QhXTJav",
        "outputId": "254f92c0-bae4-41e2-fb92-7d32f17eb751"
      },
      "outputs": [],
      "source": [
        "!pip install doubleml\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import doubleml as dml\n",
        "from sklearn.dummy import DummyRegressor\n",
        "from sklearn.linear_model import LinearRegression, LogisticRegression, LassoCV, RidgeCV, LogisticRegressionCV\n",
        "from sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "import patsy\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "np.random.seed(772023)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u6jWjkrzU8I6"
      },
      "source": [
        "## Loading the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "znh8YcAXSp3E"
      },
      "outputs": [],
      "source": [
        "data = pd.read_csv(\"https://raw.githubusercontent.com/CausalAIBook/MetricsMLNotebooks/main/data/minwage_data.csv\", index_col=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        },
        "id": "PQdsT6BnWKeq",
        "outputId": "d71da67c-541c-4c5f-f65a-7e6dd70230cd"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>countyreal</th>\n",
              "      <th>state_name</th>\n",
              "      <th>year</th>\n",
              "      <th>FIPS</th>\n",
              "      <th>emp0A01_BS</th>\n",
              "      <th>quarter</th>\n",
              "      <th>censusdiv</th>\n",
              "      <th>pop</th>\n",
              "      <th>annual_avg_pay</th>\n",
              "      <th>state_mw</th>\n",
              "      <th>fed_mw</th>\n",
              "      <th>treated</th>\n",
              "      <th>G</th>\n",
              "      <th>lemp</th>\n",
              "      <th>lpop</th>\n",
              "      <th>lavg_pay</th>\n",
              "      <th>region</th>\n",
              "      <th>ever_treated</th>\n",
              "      <th>id</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2013</td>\n",
              "      <td>Alaska</td>\n",
              "      <td>2001</td>\n",
              "      <td>2013</td>\n",
              "      <td>15</td>\n",
              "      <td>1</td>\n",
              "      <td>9</td>\n",
              "      <td>2459</td>\n",
              "      <td>22155</td>\n",
              "      <td>5.65</td>\n",
              "      <td>5.15</td>\n",
              "      <td>1</td>\n",
              "      <td>2001</td>\n",
              "      <td>2.708050</td>\n",
              "      <td>7.807510</td>\n",
              "      <td>10.005818</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>2013</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2013</td>\n",
              "      <td>Alaska</td>\n",
              "      <td>2002</td>\n",
              "      <td>2013</td>\n",
              "      <td>17</td>\n",
              "      <td>1</td>\n",
              "      <td>9</td>\n",
              "      <td>2664</td>\n",
              "      <td>28447</td>\n",
              "      <td>5.65</td>\n",
              "      <td>5.15</td>\n",
              "      <td>1</td>\n",
              "      <td>2001</td>\n",
              "      <td>2.833213</td>\n",
              "      <td>7.887584</td>\n",
              "      <td>10.255798</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>2013</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2013</td>\n",
              "      <td>Alaska</td>\n",
              "      <td>2003</td>\n",
              "      <td>2013</td>\n",
              "      <td>12</td>\n",
              "      <td>1</td>\n",
              "      <td>9</td>\n",
              "      <td>2715</td>\n",
              "      <td>30184</td>\n",
              "      <td>7.15</td>\n",
              "      <td>5.15</td>\n",
              "      <td>1</td>\n",
              "      <td>2001</td>\n",
              "      <td>2.484907</td>\n",
              "      <td>7.906547</td>\n",
              "      <td>10.315067</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>2013</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2013</td>\n",
              "      <td>Alaska</td>\n",
              "      <td>2004</td>\n",
              "      <td>2013</td>\n",
              "      <td>13</td>\n",
              "      <td>1</td>\n",
              "      <td>9</td>\n",
              "      <td>2677</td>\n",
              "      <td>27557</td>\n",
              "      <td>7.15</td>\n",
              "      <td>5.15</td>\n",
              "      <td>1</td>\n",
              "      <td>2001</td>\n",
              "      <td>2.564949</td>\n",
              "      <td>7.892452</td>\n",
              "      <td>10.224012</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>2013</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>2013</td>\n",
              "      <td>Alaska</td>\n",
              "      <td>2005</td>\n",
              "      <td>2013</td>\n",
              "      <td>11</td>\n",
              "      <td>1</td>\n",
              "      <td>9</td>\n",
              "      <td>2646</td>\n",
              "      <td>30396</td>\n",
              "      <td>7.15</td>\n",
              "      <td>5.15</td>\n",
              "      <td>1</td>\n",
              "      <td>2001</td>\n",
              "      <td>2.397895</td>\n",
              "      <td>7.880804</td>\n",
              "      <td>10.322066</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>2013</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   countyreal state_name  year  FIPS  emp0A01_BS  quarter  censusdiv   pop  \\\n",
              "1        2013     Alaska  2001  2013          15        1          9  2459   \n",
              "2        2013     Alaska  2002  2013          17        1          9  2664   \n",
              "3        2013     Alaska  2003  2013          12        1          9  2715   \n",
              "4        2013     Alaska  2004  2013          13        1          9  2677   \n",
              "5        2013     Alaska  2005  2013          11        1          9  2646   \n",
              "\n",
              "   annual_avg_pay  state_mw  fed_mw  treated     G      lemp      lpop  \\\n",
              "1           22155      5.65    5.15        1  2001  2.708050  7.807510   \n",
              "2           28447      5.65    5.15        1  2001  2.833213  7.887584   \n",
              "3           30184      7.15    5.15        1  2001  2.484907  7.906547   \n",
              "4           27557      7.15    5.15        1  2001  2.564949  7.892452   \n",
              "5           30396      7.15    5.15        1  2001  2.397895  7.880804   \n",
              "\n",
              "    lavg_pay  region  ever_treated    id  \n",
              "1  10.005818       4             1  2013  \n",
              "2  10.255798       4             1  2013  \n",
              "3  10.315067       4             1  2013  \n",
              "4  10.224012       4             1  2013  \n",
              "5  10.322066       4             1  2013  "
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v37g7zlwW5pH"
      },
      "source": [
        "### Data Preparation\n",
        "\n",
        "We remove observations that are already treated in the first observed period (2001). We drop all variables that we won't use in our analysis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "W6ob7pptW49G"
      },
      "outputs": [],
      "source": [
        "data = data.loc[(data.G==0) | (data.G>2001)]\n",
        "data.drop(columns=[\"countyreal\",\"state_name\",\"FIPS\",\"emp0A01_BS\",\n",
        "                   \"quarter\", \"censusdiv\",\"pop\",\"annual_avg_pay\",\n",
        "                   \"state_mw\",\"fed_mw\", \"ever_treated\"], inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ri12EDNJaAfF"
      },
      "source": [
        "Next, we create the treatment groups. We focus our analysis exclusively on the set of counties that had wage increases away from the federal minimum wage in 2004. That is, we treat 2003 and earlier as the pre-treatment period."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "huj7huQ1aQSq"
      },
      "outputs": [],
      "source": [
        "years = [2001,2002,2003,2004,2005,2006,2007]\n",
        "treat, cont = [], []\n",
        "for year in years:\n",
        "    treat.append(data.loc[(data.G == 2004) & (data.year == year)].copy())\n",
        "    cont.append(data.loc[((data.G == 0) | (data.G > year)) & (data.year == year)].copy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6HC1PX_Uc5bQ"
      },
      "source": [
        "We assume that the basic assumptions, particularly parallel trends, hold after conditioning on pre-treatment variables: 2001 population, 2001 average pay and 2001 teen employment, as well as the region in which the county is located. (The region is characterized by four\n",
        "categories.)\n",
        "\n",
        "Consequently, we want to extract the control variables for both treatment and control group in 2001."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "KvkwAdL6evsU"
      },
      "outputs": [],
      "source": [
        "treat[0].drop(columns=[\"year\",\"G\",\"region\",\"treated\"], inplace=True)\n",
        "cont[0].drop(columns=[\"year\",\"G\",\"region\",\"treated\"], inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zU7rM_5Ne3Xr"
      },
      "source": [
        "2003 serves as the pre-treatmeny period for both counties that do receive the treatment in 2004 and those that do not."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "3cd3dBDqeyqa"
      },
      "outputs": [],
      "source": [
        "treatB = pd.merge(treat[2], treat[0], on = \"id\", suffixes = [\"_pre\",\"_0\"])\n",
        "treatB.drop(columns = [\"treated\",\"lpop_pre\",\"lavg_pay_pre\",\"year\",\"G\"], inplace= True)\n",
        "\n",
        "contB = pd.merge(cont[2], cont[0], on = \"id\", suffixes = [\"_pre\",\"_0\"])\n",
        "contB.drop(columns = [\"treated\",\"lpop_pre\",\"lavg_pay_pre\",\"year\",\"G\"], inplace= True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xL1fSfb5e82d"
      },
      "source": [
        "We estimate the ATET in 2004-2007, which corresponds to the effect in the year of treatment as well as in the three years after the treatment. The control observations are the observations that still have the federal minimum wage in each year. (The control group is shrinking in each year as additional units receive treatment)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "zvN6Nmy0gPy4"
      },
      "outputs": [],
      "source": [
        "tdid, cdid = [], []\n",
        "for year in [3,4,5,6]:\n",
        "    treat[year].drop(columns=[\"lpop\",\"lavg_pay\",\"year\",\"G\",\"region\"], inplace=True)\n",
        "    cont[year].drop(columns=[\"lpop\",\"lavg_pay\",\"year\",\"G\",\"region\"], inplace=True)\n",
        "\n",
        "    tdid.append(pd.merge(treat[year], treatB, on = \"id\"))\n",
        "    tdid[year-3][\"dy\"] = tdid[year-3][\"lemp\"] - tdid[year-3][\"lemp_pre\"]\n",
        "    tdid[year-3].drop(columns=[\"id\",\"lemp\",\"lemp_pre\"], inplace=True)\n",
        "\n",
        "    cdid.append(pd.merge(cont[year], contB, on = \"id\"))\n",
        "    cdid[year-3][\"dy\"] = cdid[year-3][\"lemp\"] - cdid[year-3][\"lemp_pre\"]\n",
        "    cdid[year-3].drop(columns=[\"id\",\"lemp\",\"lemp_pre\"], inplace=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EqHmiHaZgPZz"
      },
      "source": [
        "### Estimation of the ATET with DML\n",
        "\n",
        "We estimate the ATET of the county level minimum wage being larger than the federal minimum with the DML algorithm presented in Section 16.3 in the book. This requires estimation of the nuisance functions $E[Y|D=0,X]$, $E[D|X]$ as well as $P(D = 1)$. For the conditional expectation functions, we will consider different modern ML regression methods, namely: Constant (= no controls); a linear combination of the controls; an expansion of the raw control variables including all third order interactions; Lasso (CV); Ridge (CV); Random Forest; Shallow Tree; Deep Tree; and CV Tree.\n",
        "The methods indicated with CV have their tuning parameter selected by cross-validation.\n",
        "\n",
        "We implement a helper for fitting the constant value model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "class DummyClassifier(object):\n",
        "    def __init__(self, strategy=None):\n",
        "        self._estimator_type = \"classifier\"\n",
        "        pass\n",
        "    def get_params(self, deep=True):\n",
        "        return dict()\n",
        "    def set_params(self):\n",
        "        pass\n",
        "    def fit(self, X, y):\n",
        "        self.classes_ = np.unique(y)\n",
        "        self.prediction = np.mean(y)\n",
        "    def predict_proba(self, X):\n",
        "        return np.ones((X.shape[0],2)) * self.prediction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The following code block implements the DML cross-fitting procedure. Please note, that it will run for a while (around 15min). We do not implement the \"Best\" learner in python, because of the structre of the DML implementation in `DoubleML`. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Estimating ATET for year 2004. Please wait.\n",
            "Estimating ATET for year 2005. Please wait.\n",
            "Estimating ATET for year 2006. Please wait.\n"
          ]
        }
      ],
      "source": [
        "att = np.zeros((4,9))\n",
        "se_att = np.zeros((4,9))\n",
        "RMSE_d = np.zeros((4,9))\n",
        "RMSE_y = np.zeros((4,9))\n",
        "for year in range(3): # These are the years 2004, 2005, 2006, 2007\n",
        "        print(f\"Estimating ATET for year {2004+year}. Please wait.\")\n",
        "        did_data = pd.concat((tdid[year], cdid[year]))\n",
        "        dummy_data = pd.get_dummies(did_data.region)\n",
        "        dummy_data = dummy_data.rename(columns=lambda x: 'region_' + str(x))\n",
        "        did_data = pd.concat((did_data, dummy_data.drop(columns=[\"region_4\"])), axis=1)\n",
        "\n",
        "        dml_data = dml.DoubleMLData(data = did_data, x_cols=[\"lemp_0\",\"lpop_0\",\"lavg_pay_0\", \n",
        "                                                        \"region_1\", \"region_2\", \"region_3\"],\n",
        "                                                y_col=\"dy\",\n",
        "                                                d_cols=\"treated\")\n",
        "\n",
        "        learners = [{\"ml_g\": DummyRegressor(strategy=\"mean\"), \"ml_m\": DummyClassifier(strategy=\"mean\")},\n",
        "                {\"ml_g\": LinearRegression(), \"ml_m\": LogisticRegression()},\n",
        "                {\"ml_g\": LinearRegression(), \"ml_m\": LogisticRegression()},\n",
        "                {\"ml_g\": LassoCV(n_jobs=-1), \"ml_m\": LogisticRegressionCV(penalty=\"l1\", solver=\"liblinear\", n_jobs=-1)},\n",
        "                {\"ml_g\": RidgeCV(), \"ml_m\": LogisticRegressionCV(n_jobs=-1)},\n",
        "                {\"ml_g\": RandomForestRegressor(n_estimators=1000, max_features=4, n_jobs=-1), \n",
        "                \"ml_m\": RandomForestClassifier(n_estimators=1000, max_features=4, n_jobs=-1)},\n",
        "                {\"ml_g\": DecisionTreeRegressor(max_depth=15, ccp_alpha=0, min_samples_split=10), \n",
        "                \"ml_m\": DecisionTreeClassifier(max_depth=15, ccp_alpha=0, min_samples_split=10)},\n",
        "                {\"ml_g\": DecisionTreeRegressor(max_depth=3, ccp_alpha=0, min_samples_split=10),\n",
        "                \"ml_m\": DecisionTreeClassifier(max_depth=3, ccp_alpha=0, min_samples_split=10)},\n",
        "                {\"ml_g\": DecisionTreeRegressor(),\n",
        "                \"ml_m\": DecisionTreeClassifier()}]\n",
        "\n",
        "        for i in [0,1,5,6,7]: # Constant, Baseline, Random Forest, Deep Tree and Shallowtree\n",
        "                dml_obj = dml.DoubleMLDID(dml_data, ml_g=learners[i][\"ml_g\"], \n",
        "                                        ml_m=learners[i][\"ml_m\"], trimming_threshold=0.05)\n",
        "                dml_obj.fit()\n",
        "                att[year,i] = dml_obj._coef\n",
        "                se_att[year,i] = dml_obj._se\n",
        "                RMSE_d[year,i] = dml_obj.rmses[\"ml_m\"]\n",
        "                RMSE_y[year,i] = np.mean(dml_obj.rmses[\"ml_g0\"] + dml_obj.rmses[\"ml_g1\"])\n",
        "\n",
        "        # make interaction data for Region Specific index\n",
        "        i = 2\n",
        "        formula = \" ~ region_1 * (lemp_0 + lpop_0 + lavg_pay_0) + region_2 * (lemp_0 + lpop_0 + lavg_pay_0) + region_3 * (lemp_0 + lpop_0 + lavg_pay_0)\"\n",
        "        design_matrix = patsy.dmatrix(formula, data=did_data)\n",
        "\n",
        "        dml_data_reg = dml.DoubleMLData.from_arrays(x=design_matrix,\n",
        "                                                y=did_data.dy.values,\n",
        "                                                d=did_data.treated.values)\n",
        "\n",
        "        dml_obj = dml.DoubleMLDID(dml_data_reg, ml_g=learners[i][\"ml_g\"], \n",
        "                                ml_m=learners[i][\"ml_m\"], trimming_threshold=0.05)\n",
        "        dml_obj.fit()\n",
        "        att[year,i] = dml_obj._coef\n",
        "        se_att[year,i] = dml_obj._se\n",
        "        RMSE_d[year,i] = dml_obj.rmses[\"ml_m\"]\n",
        "        RMSE_y[year,i] = np.mean(dml_obj.rmses[\"ml_g0\"] + dml_obj.rmses[\"ml_g1\"])\n",
        "\n",
        "        # make interaction data for polynomial features\n",
        "        pf = PolynomialFeatures(degree=3)\n",
        "        poly_X = pf.fit_transform(did_data[[\"lemp_0\",\"lpop_0\",\"lavg_pay_0\",\"region_1\", \"region_2\", \"region_3\"]])\n",
        "\n",
        "        dml_data_poly = dml.DoubleMLData.from_arrays(x=poly_X,\n",
        "                                                y=did_data.dy.values,\n",
        "                                                d=did_data.treated.values)\n",
        "\n",
        "        for i in [3,4]:\n",
        "                dml_obj = dml.DoubleMLDID(dml_data_poly, ml_g=learners[i][\"ml_g\"], \n",
        "                                ml_m=learners[i][\"ml_m\"], trimming_threshold=0.05)\n",
        "                dml_obj.fit()\n",
        "                att[year,i] = dml_obj._coef\n",
        "                se_att[year,i] = dml_obj._se\n",
        "                RMSE_d[year,i] = dml_obj.rmses[\"ml_m\"]\n",
        "                RMSE_y[year,i] = np.mean(dml_obj.rmses[\"ml_g0\"] + dml_obj.rmses[\"ml_g1\"])\n",
        "\n",
        "        # run cross-validated tree\n",
        "        i = 8\n",
        "        grid = {\"ml_m\": {\"max_depth\": [15], \"min_samples_split\": [10], \"ccp_alpha\" : np.linspace(0,0.1,10)},\n",
        "                \"ml_g\": {\"max_depth\": [15], \"min_samples_split\": [10], \"ccp_alpha\" : np.linspace(0,0.1,10)}}\n",
        "\n",
        "        dml_obj = dml.DoubleMLDID(dml_data, ml_g=learners[i][\"ml_g\"], \n",
        "                        ml_m=learners[i][\"ml_m\"], trimming_threshold=0.05)\n",
        "        dml_obj.tune(grid, n_jobs_cv=-1)\n",
        "        dml_obj.fit()\n",
        "        att[year,i] = dml_obj._coef\n",
        "        se_att[year,i] = dml_obj._se\n",
        "        RMSE_d[year,i] = dml_obj.rmses[\"ml_m\"]\n",
        "        RMSE_y[year,i] = np.mean(dml_obj.rmses[\"ml_g0\"] + dml_obj.rmses[\"ml_g1\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The year `2007` has no observations of untreated individuals in region `1`. Thus, we need to do some adjustments for that year."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Estimating ATET for year 2007. Please wait.\n"
          ]
        }
      ],
      "source": [
        "for year in [3]: # These are the years 2007\n",
        "        print(f\"Estimating ATET for year {2004+year}. Please wait.\")\n",
        "        did_data = pd.concat((tdid[year], cdid[year]))\n",
        "        dummy_data = pd.get_dummies(did_data.region)\n",
        "        dummy_data = dummy_data.rename(columns=lambda x: 'region_' + str(x))\n",
        "        did_data = pd.concat((did_data, dummy_data.drop(columns=[\"region_4\"])), axis=1)\n",
        "\n",
        "        dml_data = dml.DoubleMLData(data = did_data, x_cols=[\"lemp_0\",\"lpop_0\",\"lavg_pay_0\", \n",
        "                                                        \"region_3\", \"region_2\"],\n",
        "                                                y_col=\"dy\",\n",
        "                                                d_cols=\"treated\")\n",
        "\n",
        "        learners = [{\"ml_g\": DummyRegressor(strategy=\"mean\"), \"ml_m\": DummyClassifier(strategy=\"mean\")},\n",
        "                {\"ml_g\": LinearRegression(), \"ml_m\": LogisticRegression()},\n",
        "                {\"ml_g\": LinearRegression(), \"ml_m\": LogisticRegression()},\n",
        "                {\"ml_g\": LassoCV(n_jobs=-1), \"ml_m\": LogisticRegressionCV(penalty=\"l1\", solver=\"liblinear\", n_jobs=-1)},\n",
        "                {\"ml_g\": RidgeCV(), \"ml_m\": LogisticRegressionCV(n_jobs=-1)},\n",
        "                {\"ml_g\": RandomForestRegressor(n_estimators=1000, max_features=4, n_jobs=-1), \n",
        "                \"ml_m\": RandomForestClassifier(n_estimators=1000, max_features=4, n_jobs=-1)},\n",
        "                {\"ml_g\": DecisionTreeRegressor(max_depth=15, ccp_alpha=0, min_samples_split=10), \n",
        "                \"ml_m\": DecisionTreeClassifier(max_depth=15, ccp_alpha=0, min_samples_split=10)},\n",
        "                {\"ml_g\": DecisionTreeRegressor(max_depth=3, ccp_alpha=0, min_samples_split=10),\n",
        "                \"ml_m\": DecisionTreeClassifier(max_depth=3, ccp_alpha=0, min_samples_split=10)},\n",
        "                {\"ml_g\": DecisionTreeRegressor(),\n",
        "                \"ml_m\": DecisionTreeClassifier()}]\n",
        "\n",
        "        for i in [0,1,5,6,7]: # Constant, Baseline, Random Forest, Deep Tree and Shallowtree\n",
        "                dml_obj = dml.DoubleMLDID(dml_data, ml_g=learners[i][\"ml_g\"], \n",
        "                                        ml_m=learners[i][\"ml_m\"], trimming_threshold=0.05)\n",
        "                dml_obj.fit()\n",
        "                att[year,i] = dml_obj._coef\n",
        "                se_att[year,i] = dml_obj._se\n",
        "                RMSE_d[year,i] = dml_obj.rmses[\"ml_m\"]\n",
        "                RMSE_y[year,i] = np.mean(dml_obj.rmses[\"ml_g0\"] + dml_obj.rmses[\"ml_g1\"])\n",
        "\n",
        "        # make interaction data for Region Specific index\n",
        "        i = 2\n",
        "        formula = \" ~ region_2 * (lemp_0 + lpop_0 + lavg_pay_0) + region_3 * (lemp_0 + lpop_0 + lavg_pay_0)\"\n",
        "        design_matrix = patsy.dmatrix(formula, data=did_data)\n",
        "\n",
        "        dml_data_reg = dml.DoubleMLData.from_arrays(x=design_matrix,\n",
        "                                                y=did_data.dy.values,\n",
        "                                                d=did_data.treated.values)\n",
        "\n",
        "        dml_obj = dml.DoubleMLDID(dml_data_reg, ml_g=learners[i][\"ml_g\"], \n",
        "                                ml_m=learners[i][\"ml_m\"], trimming_threshold=0.05)\n",
        "        dml_obj.fit()\n",
        "        att[year,i] = dml_obj._coef\n",
        "        se_att[year,i] = dml_obj._se\n",
        "        RMSE_d[year,i] = dml_obj.rmses[\"ml_m\"]\n",
        "        RMSE_y[year,i] = np.mean(dml_obj.rmses[\"ml_g0\"] + dml_obj.rmses[\"ml_g1\"])\n",
        "\n",
        "        # make interaction data for polynomial features\n",
        "        pf = PolynomialFeatures(degree=3)\n",
        "        poly_X = pf.fit_transform(did_data[[\"lemp_0\",\"lpop_0\",\"lavg_pay_0\", \"region_2\", \"region_3\"]])\n",
        "\n",
        "        dml_data_poly = dml.DoubleMLData.from_arrays(x=poly_X,\n",
        "                                                y=did_data.dy.values,\n",
        "                                                d=did_data.treated.values)\n",
        "\n",
        "        for i in [3,4]:\n",
        "                dml_obj = dml.DoubleMLDID(dml_data_poly, ml_g=learners[i][\"ml_g\"], \n",
        "                                ml_m=learners[i][\"ml_m\"], trimming_threshold=0.05)\n",
        "                dml_obj.fit()\n",
        "                att[year,i] = dml_obj._coef\n",
        "                se_att[year,i] = dml_obj._se\n",
        "                RMSE_d[year,i] = dml_obj.rmses[\"ml_m\"]\n",
        "                RMSE_y[year,i] = np.mean(dml_obj.rmses[\"ml_g0\"] + dml_obj.rmses[\"ml_g1\"])\n",
        "\n",
        "        # run cross-validated tree\n",
        "        i = 8\n",
        "        grid = {\"ml_m\": {\"max_depth\": [15], \"min_samples_split\": [10], \"ccp_alpha\" : np.linspace(0,0.1,10)},\n",
        "                \"ml_g\": {\"max_depth\": [15], \"min_samples_split\": [10], \"ccp_alpha\" : np.linspace(0,0.1,10)}}\n",
        "\n",
        "        dml_obj = dml.DoubleMLDID(dml_data, ml_g=learners[i][\"ml_g\"], \n",
        "                        ml_m=learners[i][\"ml_m\"], trimming_threshold=0.05)\n",
        "        dml_obj.tune(grid, n_jobs_cv=-1)\n",
        "        dml_obj.fit()\n",
        "        att[year,i] = dml_obj._coef\n",
        "        se_att[year,i] = dml_obj._se\n",
        "        RMSE_d[year,i] = dml_obj.rmses[\"ml_m\"]\n",
        "        RMSE_y[year,i] = np.mean(dml_obj.rmses[\"ml_g0\"] + dml_obj.rmses[\"ml_g1\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o-r2aJIv2_Yh"
      },
      "source": [
        "We start by reporting the RMSE obtained during cross-fitting for each learner in each period."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 349
        },
        "id": "xRazffP5kaq8",
        "outputId": "eb34dd8d-fb75-4d48-cfe7-63baf9906645"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>2004</th>\n",
              "      <th>2005</th>\n",
              "      <th>2006</th>\n",
              "      <th>2007</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>No Controls</th>\n",
              "      <td>0.355682</td>\n",
              "      <td>0.387948</td>\n",
              "      <td>0.418108</td>\n",
              "      <td>0.453009</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Basic</th>\n",
              "      <td>0.366668</td>\n",
              "      <td>0.370151</td>\n",
              "      <td>0.404329</td>\n",
              "      <td>0.434847</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Expansion</th>\n",
              "      <td>0.352975</td>\n",
              "      <td>0.384314</td>\n",
              "      <td>0.401772</td>\n",
              "      <td>0.445056</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Lasso (CV)</th>\n",
              "      <td>0.349643</td>\n",
              "      <td>0.368332</td>\n",
              "      <td>0.405830</td>\n",
              "      <td>0.433699</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Ridge (CV)</th>\n",
              "      <td>0.335870</td>\n",
              "      <td>0.379488</td>\n",
              "      <td>0.407717</td>\n",
              "      <td>0.445700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Random Forest</th>\n",
              "      <td>0.371786</td>\n",
              "      <td>0.379539</td>\n",
              "      <td>0.441016</td>\n",
              "      <td>0.486183</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Deep Tree</th>\n",
              "      <td>0.426649</td>\n",
              "      <td>0.415303</td>\n",
              "      <td>0.486346</td>\n",
              "      <td>0.574548</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Shallow Tree</th>\n",
              "      <td>0.393449</td>\n",
              "      <td>0.378987</td>\n",
              "      <td>0.426423</td>\n",
              "      <td>0.469549</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Tree (CV)</th>\n",
              "      <td>0.353772</td>\n",
              "      <td>0.384895</td>\n",
              "      <td>0.417904</td>\n",
              "      <td>0.452951</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                   2004      2005      2006      2007\n",
              "No Controls    0.355682  0.387948  0.418108  0.453009\n",
              "Basic          0.366668  0.370151  0.404329  0.434847\n",
              "Expansion      0.352975  0.384314  0.401772  0.445056\n",
              "Lasso (CV)     0.349643  0.368332  0.405830  0.433699\n",
              "Ridge (CV)     0.335870  0.379488  0.407717  0.445700\n",
              "Random Forest  0.371786  0.379539  0.441016  0.486183\n",
              "Deep Tree      0.426649  0.415303  0.486346  0.574548\n",
              "Shallow Tree   0.393449  0.378987  0.426423  0.469549\n",
              "Tree (CV)      0.353772  0.384895  0.417904  0.452951"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "table1y = pd.DataFrame(RMSE_y[:,0:9].T, columns = [\"2004\",\"2005\",\"2006\",\"2007\"], \n",
        "                       index = [\"No Controls\", \"Basic\", \"Expansion\", \"Lasso (CV)\", \"Ridge (CV)\",\n",
        "                                \"Random Forest\",\"Deep Tree\", \"Shallow Tree\", \"Tree (CV)\"])\n",
        "\n",
        "table1y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 349
        },
        "id": "tlrNObn1kvpF",
        "outputId": "80322a03-006e-4696-ae2e-a02c1c97422a"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>2004</th>\n",
              "      <th>2005</th>\n",
              "      <th>2006</th>\n",
              "      <th>2007</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>No Controls</th>\n",
              "      <td>0.198375</td>\n",
              "      <td>0.200732</td>\n",
              "      <td>0.211120</td>\n",
              "      <td>0.250505</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Basic</th>\n",
              "      <td>0.194086</td>\n",
              "      <td>0.196242</td>\n",
              "      <td>0.204615</td>\n",
              "      <td>0.223249</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Expansion</th>\n",
              "      <td>0.193859</td>\n",
              "      <td>0.197086</td>\n",
              "      <td>0.205414</td>\n",
              "      <td>0.223516</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Lasso (CV)</th>\n",
              "      <td>0.198695</td>\n",
              "      <td>0.201153</td>\n",
              "      <td>0.211804</td>\n",
              "      <td>0.251897</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Ridge (CV)</th>\n",
              "      <td>0.195181</td>\n",
              "      <td>0.197515</td>\n",
              "      <td>0.206271</td>\n",
              "      <td>0.222062</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Random Forest</th>\n",
              "      <td>0.202125</td>\n",
              "      <td>0.200904</td>\n",
              "      <td>0.213112</td>\n",
              "      <td>0.238080</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Deep Tree</th>\n",
              "      <td>0.226236</td>\n",
              "      <td>0.242409</td>\n",
              "      <td>0.234684</td>\n",
              "      <td>0.261518</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Shallow Tree</th>\n",
              "      <td>0.196959</td>\n",
              "      <td>0.197994</td>\n",
              "      <td>0.206247</td>\n",
              "      <td>0.228207</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Tree (CV)</th>\n",
              "      <td>0.198375</td>\n",
              "      <td>0.200732</td>\n",
              "      <td>0.211120</td>\n",
              "      <td>0.239808</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                   2004      2005      2006      2007\n",
              "No Controls    0.198375  0.200732  0.211120  0.250505\n",
              "Basic          0.194086  0.196242  0.204615  0.223249\n",
              "Expansion      0.193859  0.197086  0.205414  0.223516\n",
              "Lasso (CV)     0.198695  0.201153  0.211804  0.251897\n",
              "Ridge (CV)     0.195181  0.197515  0.206271  0.222062\n",
              "Random Forest  0.202125  0.200904  0.213112  0.238080\n",
              "Deep Tree      0.226236  0.242409  0.234684  0.261518\n",
              "Shallow Tree   0.196959  0.197994  0.206247  0.228207\n",
              "Tree (CV)      0.198375  0.200732  0.211120  0.239808"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "table1d = pd.DataFrame(RMSE_d[:,0:9].T, columns = [\"2004\",\"2005\",\"2006\",\"2007\"], \n",
        "                       index = [\"No Controls\", \"Basic\", \"Expansion\", \"Lasso (CV)\", \"Ridge (CV)\",\n",
        "                                \"Random Forest\",\"Deep Tree\", \"Shallow Tree\", \"Tree (CV)\"])\n",
        "\n",
        "table1d"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nxlhGchj4n-9"
      },
      "source": [
        "Here we see that the Deep Tree systematically performs worse in terms of cross-fit predictions than the other learners for both tasks and that Expansion performs similarly poorly for the outcome prediction. It also appears there is some signal in the regressors, especially for the propensity score, as all methods outside of Deep Tree and Expansion produce smaller RMSEs than the No Controls baseline. The other methods all produce similar RMSEs, with a small edge going to Ridge and Lasso. While it would be hard to reliably conclude which of the relatively good performing methods is statistically best here, one could exclude Expansion and Deep Tree from further consideration on the basis of out-of-sample performance suggesting\n",
        "they are doing a poor job approximating the nuisance functions. Best (or a different ensemble) provides a good baseline that is principled in the sense that one could pre-commit to using the best learners without having first looked at the subsequent estimation results."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "85jrGLgG4-Gw"
      },
      "source": [
        "We report estimates of the ATET in each period in the following table."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 693
        },
        "id": "-tfALIgnkvao",
        "outputId": "cf1c07a7-60ef-41a3-f8b3-51a01819c4c9"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>2004</th>\n",
              "      <th>2005</th>\n",
              "      <th>2006</th>\n",
              "      <th>2007</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>No Controls</th>\n",
              "      <td>-0.040141</td>\n",
              "      <td>-0.076152</td>\n",
              "      <td>-0.116830</td>\n",
              "      <td>-0.130854</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>s.e.</th>\n",
              "      <td>0.018996</td>\n",
              "      <td>0.020104</td>\n",
              "      <td>0.019786</td>\n",
              "      <td>0.022568</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Basic</th>\n",
              "      <td>-0.025488</td>\n",
              "      <td>-0.049940</td>\n",
              "      <td>-0.054026</td>\n",
              "      <td>-0.069152</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>s.e.</th>\n",
              "      <td>0.019273</td>\n",
              "      <td>0.020061</td>\n",
              "      <td>0.019319</td>\n",
              "      <td>0.022559</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Expansion</th>\n",
              "      <td>-0.023411</td>\n",
              "      <td>-0.048877</td>\n",
              "      <td>-0.050754</td>\n",
              "      <td>-0.062235</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>s.e.</th>\n",
              "      <td>0.019714</td>\n",
              "      <td>0.021169</td>\n",
              "      <td>0.019834</td>\n",
              "      <td>0.025347</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Lasso (CV)</th>\n",
              "      <td>-0.035736</td>\n",
              "      <td>-0.048229</td>\n",
              "      <td>-0.060006</td>\n",
              "      <td>-0.072672</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>s.e.</th>\n",
              "      <td>0.019053</td>\n",
              "      <td>0.019824</td>\n",
              "      <td>0.019619</td>\n",
              "      <td>0.022104</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Ridge (CV)</th>\n",
              "      <td>-0.025642</td>\n",
              "      <td>-0.043243</td>\n",
              "      <td>-0.049233</td>\n",
              "      <td>-0.054087</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>s.e.</th>\n",
              "      <td>0.019200</td>\n",
              "      <td>0.020162</td>\n",
              "      <td>0.019443</td>\n",
              "      <td>0.023782</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Random Forest</th>\n",
              "      <td>-0.004526</td>\n",
              "      <td>-0.042635</td>\n",
              "      <td>-0.046630</td>\n",
              "      <td>-0.060026</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>s.e.</th>\n",
              "      <td>0.018027</td>\n",
              "      <td>0.019627</td>\n",
              "      <td>0.020088</td>\n",
              "      <td>0.025083</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Deep Tree</th>\n",
              "      <td>-0.038519</td>\n",
              "      <td>-0.054290</td>\n",
              "      <td>-0.031885</td>\n",
              "      <td>-0.043109</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>s.e.</th>\n",
              "      <td>0.036047</td>\n",
              "      <td>0.032234</td>\n",
              "      <td>0.030163</td>\n",
              "      <td>0.038079</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Shallow Tree</th>\n",
              "      <td>-0.032665</td>\n",
              "      <td>-0.036959</td>\n",
              "      <td>-0.053187</td>\n",
              "      <td>-0.037007</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>s.e.</th>\n",
              "      <td>0.019122</td>\n",
              "      <td>0.019462</td>\n",
              "      <td>0.019885</td>\n",
              "      <td>0.024390</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Tree (CV)</th>\n",
              "      <td>-0.040077</td>\n",
              "      <td>-0.076306</td>\n",
              "      <td>-0.116553</td>\n",
              "      <td>-0.093843</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>s.e.</th>\n",
              "      <td>0.019017</td>\n",
              "      <td>0.020126</td>\n",
              "      <td>0.019753</td>\n",
              "      <td>0.023181</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                   2004      2005      2006      2007\n",
              "No Controls   -0.040141 -0.076152 -0.116830 -0.130854\n",
              "s.e.           0.018996  0.020104  0.019786  0.022568\n",
              "Basic         -0.025488 -0.049940 -0.054026 -0.069152\n",
              "s.e.           0.019273  0.020061  0.019319  0.022559\n",
              "Expansion     -0.023411 -0.048877 -0.050754 -0.062235\n",
              "s.e.           0.019714  0.021169  0.019834  0.025347\n",
              "Lasso (CV)    -0.035736 -0.048229 -0.060006 -0.072672\n",
              "s.e.           0.019053  0.019824  0.019619  0.022104\n",
              "Ridge (CV)    -0.025642 -0.043243 -0.049233 -0.054087\n",
              "s.e.           0.019200  0.020162  0.019443  0.023782\n",
              "Random Forest -0.004526 -0.042635 -0.046630 -0.060026\n",
              "s.e.           0.018027  0.019627  0.020088  0.025083\n",
              "Deep Tree     -0.038519 -0.054290 -0.031885 -0.043109\n",
              "s.e.           0.036047  0.032234  0.030163  0.038079\n",
              "Shallow Tree  -0.032665 -0.036959 -0.053187 -0.037007\n",
              "s.e.           0.019122  0.019462  0.019885  0.024390\n",
              "Tree (CV)     -0.040077 -0.076306 -0.116553 -0.093843\n",
              "s.e.           0.019017  0.020126  0.019753  0.023181"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "table2 =np.zeros((18, 4))\n",
        "table2[np.arange(0,18,2),] = att.T\n",
        "table2[np.arange(1,18,2),] = se_att.T\n",
        "table2 = pd.DataFrame(table2, columns=[\"2004\",\"2005\",\"2006\",\"2007\"],\n",
        "                      index = [\"No Controls\",\"s.e.\",\"Basic\",\"s.e.\",\n",
        "                               \"Expansion\",\"s.e.\",\"Lasso (CV)\",\"s.e.\",\n",
        "                               \"Ridge (CV)\",\"s.e.\",\"Random Forest\",\"s.e.\",\n",
        "                               \"Deep Tree\",\"s.e.\",\"Shallow Tree\",\"s.e.\",\n",
        "                               \"Tree (CV)\",\"s.e.\"])\n",
        "\n",
        "table2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "euBQPSYs47iZ"
      },
      "source": [
        "Here, we see that all methods provide point estimates that suggest the effect of the minimum wage increase leads to decreases in youth employment with small effects in the initial period that become larger in the years following the treatment. This pattern seems economically plausible as it may take time for firms to adjust employment and other input choices in response to a minimum wage change. In the estimates that are reported in the book we have values that are not consistent with this pattern, however, they systematically underperform in terms of having poor cross-fit prediction performance. In terms of point estimates, the other pattern that emerges is that all estimates that use the covariates produce ATET estimates that are systematically smaller in magnitude than the No Controls baseline, suggesting that failing to include the controls may lead to overstatement of treatment effects in this example.\n",
        "\n",
        "Turning to inference, we would reject the hypothesis of no minimum wage effect two or more years after the change at the 5% level, even after multiple testing correction, if we were to focus on many of the estimators."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MhFmwCr3kulb"
      },
      "source": [
        "### Assess pre-trends\n",
        "\n",
        "Because we have data for the period 2001-2007, we can perform a so-called pre-trends test to provide some evidence about the plausibility of the conditional parallel trends assumption. Specifically, we can continue to use 2003 as the reference period but now consider 2002 to be the treatment period. Sensible economic mechanisms underlying the assumption would then typically suggest that the ATET in 2002 - before the 2004 minimum wage change we are considering - should be zero. Finding evidence that the ATET in 2002 is non-zero then calls into question the validity of the assumption.\n",
        "\n",
        "We change the treatment status of those observations, which received treatment in 2004 in the 2002 data and create a placebo treatment as well as control group."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "dogBL7W4o3P4"
      },
      "outputs": [],
      "source": [
        "treat[1].drop(columns=[\"lpop\",\"lavg_pay\",\"year\",\"G\",\"region\"], inplace=True)\n",
        "treat[1].treated = 1  # Code these observations as treated\n",
        "\n",
        "tdid02 = pd.merge(treat[1], treatB, on = \"id\")\n",
        "tdid02[\"dy\"] = tdid02[\"lemp\"] - tdid02[\"lemp_pre\"]\n",
        "tdid02.drop(columns=[\"id\",\"lemp\",\"lemp_pre\"], inplace=True)\n",
        "\n",
        "cont[1].drop(columns=[\"lpop\",\"lavg_pay\",\"year\",\"G\",\"region\"], inplace=True)\n",
        "\n",
        "cdid02 = pd.merge(cont[1], contB, on = \"id\")\n",
        "cdid02[\"dy\"] = cdid02[\"lemp\"] - cdid02[\"lemp_pre\"]\n",
        "cdid02.drop(columns=[\"id\",\"lemp\",\"lemp_pre\"], inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JhjbleYX599z"
      },
      "source": [
        "We repeat the exercise for obtaining our ATET estimates and standard error for 2004-2007. Particularly, we also use all the learners as mentioned above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Df8A2qj_odSh",
        "outputId": "7b81df57-3ad3-4f15-9651-1313dd53189f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Estimating ATET for year 2002. Please wait.\n"
          ]
        }
      ],
      "source": [
        "att_pre = np.zeros((1,9))\n",
        "se_att_pre = np.zeros((1,9))\n",
        "RMSE_d_pre = np.zeros((1,9))\n",
        "RMSE_y_pre = np.zeros((1,9))\n",
        "for year in range(1): # Only year 2002\n",
        "        print(f\"Estimating ATET for year {2002+year}. Please wait.\")\n",
        "        did_data = pd.concat((tdid02, cdid02))\n",
        "        dummy_data = pd.get_dummies(did_data.region)\n",
        "        dummy_data = dummy_data.rename(columns=lambda x: 'region_' + str(x))\n",
        "        did_data = pd.concat((did_data, dummy_data.drop(columns=[\"region_4\"])), axis=1)\n",
        "\n",
        "        dml_data = dml.DoubleMLData(data = did_data, x_cols=[\"lemp_0\",\"lpop_0\",\"lavg_pay_0\", \n",
        "                                                        \"region_1\", \"region_2\", \"region_3\"],\n",
        "                                                y_col=\"dy\",\n",
        "                                                d_cols=\"treated\")\n",
        "\n",
        "        learners = [{\"ml_g\": DummyRegressor(strategy=\"mean\"), \"ml_m\": DummyClassifier(strategy=\"mean\")},\n",
        "                {\"ml_g\": LinearRegression(), \"ml_m\": LogisticRegression()},\n",
        "                {\"ml_g\": LinearRegression(), \"ml_m\": LogisticRegression()},\n",
        "                {\"ml_g\": LassoCV(n_jobs=-1), \"ml_m\": LogisticRegressionCV(penalty=\"l1\", solver=\"liblinear\", n_jobs=-1)},\n",
        "                {\"ml_g\": RidgeCV(), \"ml_m\": LogisticRegressionCV(n_jobs=-1)},\n",
        "                {\"ml_g\": RandomForestRegressor(n_estimators=1000, max_features=4, n_jobs=-1), \n",
        "                \"ml_m\": RandomForestClassifier(n_estimators=1000, max_features=4, n_jobs=-1)},\n",
        "                {\"ml_g\": DecisionTreeRegressor(max_depth=15, ccp_alpha=0, min_samples_split=10), \n",
        "                \"ml_m\": DecisionTreeClassifier(max_depth=15, ccp_alpha=0, min_samples_split=10)},\n",
        "                {\"ml_g\": DecisionTreeRegressor(max_depth=3, ccp_alpha=0, min_samples_split=10),\n",
        "                \"ml_m\": DecisionTreeClassifier(max_depth=3, ccp_alpha=0, min_samples_split=10)},\n",
        "                {\"ml_g\": DecisionTreeRegressor(),\n",
        "                \"ml_m\": DecisionTreeClassifier()}]\n",
        "\n",
        "        for i in [0,1,5,6,7]: # Constant, Baseline, Random Forest, Deep Tree and Shallowtree\n",
        "                dml_obj = dml.DoubleMLDID(dml_data, ml_g=learners[i][\"ml_g\"], \n",
        "                                        ml_m=learners[i][\"ml_m\"], trimming_threshold=0.05)\n",
        "                dml_obj.fit()\n",
        "                att_pre[year,i] = dml_obj._coef\n",
        "                se_att_pre[year,i] = dml_obj._se\n",
        "                RMSE_d_pre[year,i] = dml_obj.rmses[\"ml_m\"]\n",
        "                RMSE_y_pre[year,i] = np.mean(dml_obj.rmses[\"ml_g0\"] + dml_obj.rmses[\"ml_g1\"])\n",
        "\n",
        "        # make interaction data for Region Specific index\n",
        "        i = 2\n",
        "        formula = \" ~ region_1 * (lemp_0 + lpop_0 + lavg_pay_0) + region_2 * (lemp_0 + lpop_0 + lavg_pay_0) + region_3 * (lemp_0 + lpop_0 + lavg_pay_0)\"\n",
        "        design_matrix = patsy.dmatrix(formula, data=did_data)\n",
        "\n",
        "        dml_data_reg = dml.DoubleMLData.from_arrays(x=design_matrix,\n",
        "                                                y=did_data.dy.values,\n",
        "                                                d=did_data.treated.values)\n",
        "\n",
        "        dml_obj = dml.DoubleMLDID(dml_data_reg, ml_g=learners[i][\"ml_g\"], \n",
        "                                ml_m=learners[i][\"ml_m\"], trimming_threshold=0.05)\n",
        "        dml_obj.fit()\n",
        "        att_pre[year,i] = dml_obj._coef\n",
        "        se_att_pre[year,i] = dml_obj._se\n",
        "        RMSE_d_pre[year,i] = dml_obj.rmses[\"ml_m\"]\n",
        "        RMSE_y_pre[year,i] = np.mean(dml_obj.rmses[\"ml_g0\"] + dml_obj.rmses[\"ml_g1\"])\n",
        "\n",
        "        # make interaction data for polynomial features\n",
        "        pf = PolynomialFeatures(degree=3)\n",
        "        poly_X = pf.fit_transform(did_data[[\"lemp_0\",\"lpop_0\",\"lavg_pay_0\",\"region_1\", \"region_2\", \"region_3\"]])\n",
        "\n",
        "        dml_data_poly = dml.DoubleMLData.from_arrays(x=poly_X,\n",
        "                                                y=did_data.dy.values,\n",
        "                                                d=did_data.treated.values)\n",
        "\n",
        "        for i in [3,4]:\n",
        "                dml_obj = dml.DoubleMLDID(dml_data_poly, ml_g=learners[i][\"ml_g\"], \n",
        "                                ml_m=learners[i][\"ml_m\"], trimming_threshold=0.05)\n",
        "                dml_obj.fit()\n",
        "                att_pre[year,i] = dml_obj._coef\n",
        "                se_att_pre[year,i] = dml_obj._se\n",
        "                RMSE_d_pre[year,i] = dml_obj.rmses[\"ml_m\"]\n",
        "                RMSE_y_pre[year,i] = np.mean(dml_obj.rmses[\"ml_g0\"] + dml_obj.rmses[\"ml_g1\"])\n",
        "\n",
        "        # run cross-validated tree\n",
        "        i = 8\n",
        "        grid = {\"ml_m\": {\"max_depth\": [15], \"min_samples_split\": [10], \"ccp_alpha\" : np.linspace(0,0.1,10)},\n",
        "                \"ml_g\": {\"max_depth\": [15], \"min_samples_split\": [10], \"ccp_alpha\" : np.linspace(0,0.1,10)}}\n",
        "\n",
        "        dml_obj = dml.DoubleMLDID(dml_data, ml_g=learners[i][\"ml_g\"], \n",
        "                        ml_m=learners[i][\"ml_m\"], trimming_threshold=0.05)\n",
        "        dml_obj.tune(grid, n_jobs_cv=-1)\n",
        "        dml_obj.fit()\n",
        "        att_pre[year,i] = dml_obj._coef\n",
        "        se_att_pre[year,i] = dml_obj._se\n",
        "        RMSE_d_pre[year,i] = dml_obj.rmses[\"ml_m\"]\n",
        "        RMSE_y_pre[year,i] = np.mean(dml_obj.rmses[\"ml_g0\"] + dml_obj.rmses[\"ml_g1\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZpL1SPbe6HRE"
      },
      "source": [
        "We report the results in the following table."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 380
        },
        "id": "2y3YugM4omz4",
        "outputId": "44e812c4-bc35-4309-e567-f6a37bca1d0f"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>RMSE Y</th>\n",
              "      <th>RMSE D</th>\n",
              "      <th>ATET</th>\n",
              "      <th>s.e.</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>No Controls</th>\n",
              "      <td>0.287126</td>\n",
              "      <td>0.194687</td>\n",
              "      <td>-0.004932</td>\n",
              "      <td>0.013258</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Basic</th>\n",
              "      <td>0.295007</td>\n",
              "      <td>0.192069</td>\n",
              "      <td>0.003385</td>\n",
              "      <td>0.013469</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Expansion</th>\n",
              "      <td>0.301267</td>\n",
              "      <td>0.191703</td>\n",
              "      <td>0.005378</td>\n",
              "      <td>0.013259</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Lasso (CV)</th>\n",
              "      <td>0.287478</td>\n",
              "      <td>0.194943</td>\n",
              "      <td>-0.003734</td>\n",
              "      <td>0.013216</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Ridge (CV)</th>\n",
              "      <td>0.317353</td>\n",
              "      <td>0.192812</td>\n",
              "      <td>0.000460</td>\n",
              "      <td>0.012827</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Random Forest</th>\n",
              "      <td>0.308639</td>\n",
              "      <td>0.234943</td>\n",
              "      <td>-0.002581</td>\n",
              "      <td>0.010669</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Deep Tree</th>\n",
              "      <td>0.345325</td>\n",
              "      <td>0.255873</td>\n",
              "      <td>0.022914</td>\n",
              "      <td>0.018740</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Shallow Tree</th>\n",
              "      <td>0.341059</td>\n",
              "      <td>0.195106</td>\n",
              "      <td>-0.007538</td>\n",
              "      <td>0.013786</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Tree (CV)</th>\n",
              "      <td>0.285115</td>\n",
              "      <td>0.194687</td>\n",
              "      <td>-0.004861</td>\n",
              "      <td>0.013275</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                 RMSE Y    RMSE D      ATET      s.e.\n",
              "No Controls    0.287126  0.194687 -0.004932  0.013258\n",
              "Basic          0.295007  0.192069  0.003385  0.013469\n",
              "Expansion      0.301267  0.191703  0.005378  0.013259\n",
              "Lasso (CV)     0.287478  0.194943 -0.003734  0.013216\n",
              "Ridge (CV)     0.317353  0.192812  0.000460  0.012827\n",
              "Random Forest  0.308639  0.234943 -0.002581  0.010669\n",
              "Deep Tree      0.345325  0.255873  0.022914  0.018740\n",
              "Shallow Tree   0.341059  0.195106 -0.007538  0.013786\n",
              "Tree (CV)      0.285115  0.194687 -0.004861  0.013275"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tableP = np.zeros((4, 9))\n",
        "tableP[0,:] = RMSE_y_pre\n",
        "tableP[1,:] = RMSE_d_pre\n",
        "tableP[2,:] = att_pre\n",
        "tableP[3,:] = se_att_pre\n",
        "tableP = pd.DataFrame(tableP.T, columns = [\"RMSE Y\",\"RMSE D\",\"ATET\",\"s.e.\"],\n",
        "                      index = [\"No Controls\", \"Basic\", \"Expansion\",\n",
        "                               \"Lasso (CV)\", \"Ridge (CV)\", \"Random Forest\",\"Deep Tree\",\n",
        "                               \"Shallow Tree\", \"Tree (CV)\"])\n",
        "\n",
        "tableP"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k0jGwOFO6NZD"
      },
      "source": [
        "Here we see broad agreement across all methods in the sense of returning point estimates that are small in magnitude and small relative to standard errors. In no case would we reject the hypothesis that the pre-event effect in 2002 is different from zero at usual levels of significance. We note that failing to reject the hypothesis of no pre-event effects certainly does not imply that the conditional DiD assumption is in fact satisfied. For example, confidence intervals include values that would be consistent with relatively large pre-event effects. However, it is reassuring to see that there is not strong evidence of a violation of the underlying identifying assumption."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "didnotebook",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
