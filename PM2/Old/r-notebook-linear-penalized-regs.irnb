{"cells":[{"metadata":{},"cell_type":"markdown","source":"\n\nThis notebook contains an example for teaching.\n"},{"metadata":{"_uuid":"051d70d956493feee0c6d64651c6a088724dca2a","_execution_state":"idle","trusted":false},"cell_type":"markdown","source":"# Penalized Linear Regressions: A Simulation Experiment"},{"metadata":{},"cell_type":"markdown","source":"## Data Generating Process: Approximately Sparse"},{"metadata":{"trusted":true},"cell_type":"code","source":"set.seed(1)\n\nn = 100;\np = 400;\n\nZ= runif(n)-1/2;\nW = matrix(runif(n*p)-1/2, n, p);\n\n\n\nbeta = 1/seq(1:p)^2;   # approximately sparse beta\n#beta = rnorm(p)*.2    # dense beta\ngX = exp(4*Z)+ W%*%beta;  # leading term nonlinear\nX = cbind(Z, Z^2, Z^3, W );  # polynomials in Zs will be approximating exp(4*Z)\n\n\nY = gX + rnorm(n);    #generate Y\n\n\nplot(gX,Y, xlab=\"g(X)\", ylab=\"Y\")    #plot V vs g(X)\n\nprint( c(\"theoretical R2:\", var(gX)/var(Y)))\n\nvar(gX)/var(Y); #theoretical R-square in the simulation example\n\n\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We use package Glmnet to carry out predictions using cross-validated lasso, ridge, and elastic net"},{"metadata":{"trusted":true},"cell_type":"code","source":"\nlibrary(glmnet)\nfit.lasso.cv   <- cv.glmnet(X, Y, family=\"gaussian\", alpha=1)  # family gaussian means that we'll be using square loss\nfit.ridge   <- cv.glmnet(X, Y, family=\"gaussian\", alpha=0)     # family gaussian means that we'll be using square loss\nfit.elnet   <- cv.glmnet(X, Y, family=\"gaussian\", alpha=.5)    # family gaussian means that we'll be using square loss\n\nyhat.lasso.cv    <- predict(fit.lasso.cv, newx = X)            # predictions\nyhat.ridge   <- predict(fit.ridge, newx = X)\nyhat.elnet   <- predict(fit.elnet, newx = X)\n\nMSE.lasso.cv <- summary(lm((gX-yhat.lasso.cv)^2~1))$coef[1:2]  # report MSE and standard error for MSE for approximating g(X)\nMSE.ridge <- summary(lm((gX-yhat.ridge)^2~1))$coef[1:2]        # report MSE and standard error for MSE for approximating g(X)\nMSE.elnet <- summary(lm((gX-yhat.elnet)^2~1))$coef[1:2]        # report MSE and standard error for MSE for approximating g(X)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here we compute the lasso and ols post lasso using plug-in choices for penalty levels, using package hdm"},{"metadata":{"trusted":true},"cell_type":"code","source":"library(hdm) \nfit.rlasso  <- rlasso(Y~X,  post=FALSE)      # lasso with plug-in penalty level\nfit.rlasso.post <- rlasso(Y~X,  post=TRUE)    # post-lasso with plug-in penalty level\n\nyhat.rlasso   <- predict(fit.rlasso)            #predict g(X) for values of X\nyhat.rlasso.post   <- predict(fit.rlasso.post)  #predict g(X) for values of X\n\nMSE.lasso <- summary(lm((gX-yhat.rlasso)^2~1))$coef[1:2]       # report MSE and standard error for MSE for approximating g(X)\nMSE.lasso.post <- summary(lm((gX-yhat.rlasso.post)^2~1))$coef[1:2]  # report MSE and standard error for MSE for approximating g(X)\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Next we code up lava, which alternates the fitting of lasso and ridge"},{"metadata":{"trusted":true},"cell_type":"code","source":"library(glmnet)\n\nlava.predict<- function(X,Y, iter=5){\n    \ng1 = predict(rlasso(X, Y, post=F))  #lasso step fits \"sparse part\"\nm1 =  predict(glmnet(X, as.vector(Y-g1), family=\"gaussian\", alpha=0, lambda =20),newx=X ) #ridge step fits the \"dense\" part\n\n    \ni=1\nwhile(i<= iter) {\ng1 = predict(rlasso(X, Y, post=F))   #lasso step fits \"sparse part\"\nm1 = predict(glmnet(X, as.vector(Y-g1), family=\"gaussian\",  alpha=0, lambda =20),newx=X );  #ridge step fits the \"dense\" part\ni = i+1 }\n\nreturn(g1+m1);\n    }\n\n\nyhat.lava = lava.predict(X,Y)\nMSE.lava <- summary(lm((gX-yhat.lava)^2~1))$coef[1:2]       # report MSE and standard error for MSE for approximating g(X)\n\n    \nMSE.lava","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"library(xtable)\ntable<- matrix(0, 6, 2)\ntable[1,1:2]   <- MSE.lasso.cv\ntable[2,1:2]   <- MSE.ridge\ntable[3,1:2]   <- MSE.elnet\ntable[4,1:2]   <- MSE.lasso\ntable[5,1:2]   <- MSE.lasso.post\ntable[6,1:2]   <- MSE.lava\n\ncolnames(table)<- c(\"MSA\", \"S.E. for MSA\")\nrownames(table)<- c(\"Cross-Validated Lasso\", \"Cross-Validated ridge\",\"Cross-Validated elnet\",\n                    \"Lasso\",\"Post-Lasso\",\"Lava\")\ntab <- xtable(table, digits =3)\nprint(tab,type=\"latex\") # set type=\"latex\" for printing table in LaTeX\ntab\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nplot(gX, gX, pch=19, cex=1, ylab=\"predicted value\", xlab=\"true g(X)\")\n\npoints(gX, yhat.rlasso, col=2, pch=18, cex = 1.5 )\npoints(gX,  yhat.rlasso.post, col=3, pch=17,  cex = 1.2  )\npoints( gX, yhat.lasso.cv,col=4, pch=19,  cex = 1.2 )\n\n\nlegend(\"bottomright\", \n  legend = c(\"rLasso\", \"Post-rLasso\", \"CV Lasso\"), \n  col = c(2,3,4), \n  pch = c(18,17, 19), \n  bty = \"n\", \n  pt.cex = 1.3, \n  cex = 1.2, \n  text.col = \"black\", \n  horiz = F , \n  inset = c(0.1, 0.1))\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data Generating Process: Approximately Sparse + Small Dense Part"},{"metadata":{"trusted":true},"cell_type":"code","source":"set.seed(1)\n\nn = 100;\np = 400;\n\nZ= runif(n)-1/2;\nW = matrix(runif(n*p)-1/2, n, p);\n\n\nbeta = rnorm(p)*.2    # dense beta\ngX = exp(4*Z)+ W%*%beta;  # leading term nonlinear\nX = cbind(Z, Z^2, Z^3, W );  # polynomials in Zs will be approximating exp(4*Z)\n\n\nY = gX + rnorm(n);    #generate Y\n\n\nplot(gX,Y, xlab=\"g(X)\", ylab=\"Y\")    #plot V vs g(X)\n\nprint( c(\"theoretical R2:\", var(gX)/var(Y)))\n\nvar(gX)/var(Y); #theoretical R-square in the simulation example\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nlibrary(glmnet)\nfit.lasso.cv   <- cv.glmnet(X, Y, family=\"gaussian\", alpha=1)  # family gaussian means that we'll be using square loss\nfit.ridge   <- cv.glmnet(X, Y, family=\"gaussian\", alpha=0)     # family gaussian means that we'll be using square loss\nfit.elnet   <- cv.glmnet(X, Y, family=\"gaussian\", alpha=.5)    # family gaussian means that we'll be using square loss\n\nyhat.lasso.cv    <- predict(fit.lasso.cv, newx = X)            # predictions\nyhat.ridge   <- predict(fit.ridge, newx = X)\nyhat.elnet   <- predict(fit.elnet, newx = X)\n\nMSE.lasso.cv <- summary(lm((gX-yhat.lasso.cv)^2~1))$coef[1:2]  # report MSE and standard error for MSE for approximating g(X)\nMSE.ridge <- summary(lm((gX-yhat.ridge)^2~1))$coef[1:2]        # report MSE and standard error for MSE for approximating g(X)\nMSE.elnet <- summary(lm((gX-yhat.elnet)^2~1))$coef[1:2]        # report MSE and standard error for MSE for approximating g(X)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"library(hdm) \nfit.rlasso  <- rlasso(Y~X,  post=FALSE)      # lasso with plug-in penalty level\nfit.rlasso.post <- rlasso(Y~X,  post=TRUE)    # post-lasso with plug-in penalty level\n\nyhat.rlasso   <- predict(fit.rlasso)            #predict g(X) for values of X\nyhat.rlasso.post   <- predict(fit.rlasso.post)  #predict g(X) for values of X\n\nMSE.lasso <- summary(lm((gX-yhat.rlasso)^2~1))$coef[1:2]       # report MSE and standard error for MSE for approximating g(X)\nMSE.lasso.post <- summary(lm((gX-yhat.rlasso.post)^2~1))$coef[1:2]  # report MSE and standard error for MSE for approximating g(X)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"library(glmnet)\n\nlava.predict<- function(X,Y, iter=5){\n    \ng1 = predict(rlasso(X, Y, post=F))  #lasso step fits \"sparse part\"\nm1 =  predict(glmnet(X, as.vector(Y-g1), family=\"gaussian\", alpha=0, lambda =20),newx=X ) #ridge step fits the \"dense\" part\n\n    \ni=1\nwhile(i<= iter) {\ng1 = predict(rlasso(X, Y, post=F))   #lasso step fits \"sparse part\"\nm1 = predict(glmnet(X, as.vector(Y-g1), family=\"gaussian\",  alpha=0, lambda =20),newx=X );  #ridge step fits the \"dense\" part\ni = i+1 }\n\nreturn(g1+m1);\n    }\n\n\nyhat.lava = lava.predict(X,Y)\nMSE.lava <- summary(lm((gX-yhat.lava)^2~1))$coef[1:2]       # report MSE and standard error for MSE for approximating g(X)\n\n    \nMSE.lava","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"library(xtable)\ntable<- matrix(0, 6, 2)\ntable[1,1:2]   <- MSE.lasso.cv\ntable[2,1:2]   <- MSE.ridge\ntable[3,1:2]   <- MSE.elnet\ntable[4,1:2]   <- MSE.lasso\ntable[5,1:2]   <- MSE.lasso.post\ntable[6,1:2]   <- MSE.lava\n\ncolnames(table)<- c(\"MSA\", \"S.E. for MSA\")\nrownames(table)<- c(\"Cross-Validated Lasso\", \"Cross-Validated ridge\",\"Cross-Validated elnet\",\n                    \"Lasso\",\"Post-Lasso\",\"Lava\")\ntab <- xtable(table, digits =3)\nprint(tab,type=\"latex\") # set type=\"latex\" for printing table in LaTeX\ntab\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nplot(gX, gX, pch=19, cex=1, ylab=\"predicted value\", xlab=\"true g(X)\")\n\npoints(gX, yhat.rlasso,   col=2, pch=18, cex = 1.5 )\npoints(gX, yhat.elnet,  col=3, pch=17,  cex = 1.2  )\npoints(gX, yhat.lava,  col=4, pch=19,  cex = 1.2 )\n\n\nlegend(\"bottomright\", \n  legend = c(\"rLasso\", \"Elnet\", \"Lava\"), \n  col = c(2,3,4), \n  pch = c(18,17, 19), \n  bty = \"n\", \n  pt.cex = 1.3, \n  cex = 1.2, \n  text.col = \"black\", \n  horiz = F , \n  inset = c(0.1, 0.1))\n","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"ir","display_name":"R","language":"R"},"language_info":{"name":"R","codemirror_mode":"r","pygments_lexer":"r","mimetype":"text/x-r-source","file_extension":".r","version":"3.6.3"}},"nbformat":4,"nbformat_minor":4}
