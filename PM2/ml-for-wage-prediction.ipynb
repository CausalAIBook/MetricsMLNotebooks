{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5Rsndc93i52B"
   },
   "source": [
    "Authors: Andreas Haupt, Jannis KÃ¼ck, Alexander Quispe, Anzony Quispe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fFIsBLlF7YFv"
   },
   "source": [
    "# Machine Learning Estimators for Wage Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ssGRQl-d7U9O"
   },
   "source": [
    "We illustrate how to predict an outcome variable $Y$ in a high-dimensional setting, where the number of covariates $p$ is large in relation to the sample size $n$. So far we have used linear prediction rules, e.g. Lasso regression, for estimation.\n",
    "Now, we also consider nonlinear prediction rules including tree-based methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yYmcd6mN7VCV"
   },
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1n1-LWsu53N6"
   },
   "source": [
    "Again, we consider data from the U.S. March Supplement of the Current Population Survey (CPS) in 2015.\n",
    "The preproccessed sample consists of $5150$ never-married individuals."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1or5aUNr7yTv"
   },
   "source": [
    "Set the following file_directory to a place where you downloaded https://github.com/CausalAIBook/MetricsMLNotebooks/blob/main/PM1/wage2015_subsample_inference.rdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "57TFoHNk8BIg",
    "outputId": "09a65cae-699a-41da-9605-ab8bab757fdc"
   },
   "outputs": [],
   "source": [
    "# Import relevant packages\n",
    "!pip install pyreadr\n",
    "!pip install wget\n",
    "import pyreadr\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "import wget\n",
    "np.random.seed(1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eGQky2ch_EMc",
    "outputId": "fb87669c-2ab2-4826-b6ac-d45a08dd01d2"
   },
   "outputs": [],
   "source": [
    "rdata_read = pyreadr.read_r(wget.download(\"https://github.com/CausalAIBook/MetricsMLNotebooks/blob/main/data/wage2015_subsample_inference.rdata?raw=true\"))\n",
    "data = rdata_read['data']\n",
    "type(data)\n",
    "data.shape\n",
    "data.head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fUxf95F1B2EE",
    "outputId": "c4931ab3-06d8-4f5c-d640-950932fef064"
   },
   "outputs": [],
   "source": [
    "Y = np.log(data['wage'])\n",
    "Z = data[data.columns.difference(['wage', 'lwage'])]\n",
    "Z.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v2E-oxA8DQqH"
   },
   "source": [
    "The following figure shows the weekly wage distribution from the US survey data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 313
    },
    "id": "4QJc_mnlB2KN",
    "outputId": "b9b4fe0c-c173-4f84-ec40-0de80bcbcc35"
   },
   "outputs": [],
   "source": [
    "plt.hist(data.wage , bins = np.arange(0, 350, 20) )\n",
    "plt.xlabel('hourly wage')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title( 'Empirical wage distribution from the US survey data' )\n",
    "plt.ylim((0, 3000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TvMhq0RNDL43"
   },
   "source": [
    "Wages show a high degree of skewness. Hence, wages are transformed in almost all studies by\n",
    "the logarithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xBL1FmvgDV3f"
   },
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nDbY6BAuDYVD"
   },
   "source": [
    "Due to the skewness of the data, we are considering log wages which leads to the following regression model\n",
    "\n",
    "$$\\log(\\operatorname{wage}) = g(Z) + \\epsilon.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jNACoPwVDdpK"
   },
   "source": [
    "We will estimate the two sets of prediction rules: Linear and Nonlinear Models.\n",
    "In linear models, we estimate the prediction rule of the form\n",
    "\n",
    "$$\\hat g(Z) = \\hat \\beta'X.$$\n",
    "Again, we generate $X$ in two ways:\n",
    " \n",
    "1. Basic Model:   $X$ consists of a set of raw regressors (e.g. gender, experience, education indicators, regional indicators).\n",
    "\n",
    "\n",
    "2. Flexible Model:  $X$ consists of all raw regressors from the basic model plus occupation and industry indicators, transformations (e.g., $\\operatorname{exp}^2$ and $\\operatorname{exp}^3$) and additional two-way interactions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iDM3apFhDlgf"
   },
   "source": [
    "To evaluate the out-of-sample performance, we split the data first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VV_7ptEsEEIE"
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(Z,Y, test_size = 0.25, random_state = 123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 473
    },
    "id": "Q3HDx6pFK1s2",
    "outputId": "0f73d6ba-cc06-42a8-dc77-ffc37bfc5863"
   },
   "outputs": [],
   "source": [
    "data_train = pd.concat([y_train, X_train], axis=1)\n",
    "print(data_train.shape)\n",
    "data_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PPt0pZtgOMSn"
   },
   "source": [
    "We are starting by running a simple OLS regression. We fit the basic and flexible model to our training data by running an ols regression and compute the mean squared error on the test sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A_gsLED_F-g1"
   },
   "outputs": [],
   "source": [
    "model1 = 'wage ~ sex + exp1 + shs + hsg+ scl + clg + mw + so + we + C(occ2) + C(ind2)'\n",
    "results1 = smf.ols(model1, data=data_train).fit(cov_type = \"HC3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LcdeLTRGO3xH",
    "outputId": "b1844163-c881-4159-8ec6-5e768f66ba32"
   },
   "outputs": [],
   "source": [
    "yhat_lm_basic = results1.predict(X_test)\n",
    "print( f\"The mean squared error (MSE) using the basic model is equal to , {np.mean((y_test-yhat_lm_basic)**2)} \") # MSE OLS (basic model)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B27FJNCkQxYp"
   },
   "source": [
    "We can als compute the out-of-sample MSE and the standard error in one step:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pMlAdfH7QUPK",
    "outputId": "e3c90a6d-e756-495b-e64f-9c34ae53962c"
   },
   "outputs": [],
   "source": [
    "resid_basic = (y_test-yhat_lm_basic)**2\n",
    "\n",
    "MSE_lm_basic = sm.OLS(resid_basic , np.ones(resid_basic.shape[0])).fit().summary2().tables[1].iloc[0, 0:2]\n",
    "MSE_lm_basic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ej8l96qqQ6wK"
   },
   "source": [
    "We also compute the out-of-sample $R^2$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mMFVXMDWQmIV",
    "outputId": "dca54ca0-13fa-407a-e828-7ba9b67e201e"
   },
   "outputs": [],
   "source": [
    "R2_lm_basic = 1 - ( MSE_lm_basic[0]/y_test.var() )\n",
    "print( f\"The R^2 using the basic model is equal to, {R2_lm_basic}\" ) # MSE OLS (basic model) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jX09oKqhRJgz"
   },
   "source": [
    "We repeat the same procedure for the flexible model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fXFP-zYwK0Pc"
   },
   "outputs": [],
   "source": [
    "model2 = 'wage ~ sex + shs+hsg+scl+clg+C(occ2)+C(ind2)+mw+so+we + (exp1+exp2+exp3+exp4)*(shs+hsg+scl+clg+C(occ2)+C(ind2)+mw+so+we)'\n",
    "results2 = smf.ols(model2, data = data_train).fit(cov_type = \"HC3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bBTOYODMPc-3",
    "outputId": "c39fce09-719e-4e82-d7a5-fd4a7ad7920d"
   },
   "outputs": [],
   "source": [
    "yhat_lm_flex = results2.predict(X_test)\n",
    "print( f\"The mean squared error (MSE) using the flexible model is equal to , {np.mean((y_test-yhat_lm_flex)**2)} \") # MSE OLS (flex model)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wZebtPIkRlto",
    "outputId": "31e44a50-e9fb-4bb9-9440-929c9358acce"
   },
   "outputs": [],
   "source": [
    "resid_flex = (y_test-yhat_lm_flex)**2\n",
    "\n",
    "MSE_lm_flex = sm.OLS(resid_flex , np.ones(resid_flex.shape[0])).fit().summary2().tables[1].iloc[0, 0:2]\n",
    "MSE_lm_flex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Hh2zfp0dR7GY",
    "outputId": "09a67c09-f484-4a5b-d6d0-3ccf25e9680e"
   },
   "outputs": [],
   "source": [
    "R2_lm_flex = 1 - ( MSE_lm_flex[0]/y_test.var() )\n",
    "print( f\"The R^2 using the flexible model is equal to, {R2_lm_flex}\" ) # MSE OLS (flex model) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-4QL8R2OUbT_"
   },
   "source": [
    "We observe that ols regression works better for the basic model with smaller $p/n$ ratio. We are proceeding by running lasso regressions and its versions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4gkVN6cpXLud",
    "outputId": "1d58d739-22ac-4a62-d476-b1fc01b57a74"
   },
   "outputs": [],
   "source": [
    "import sklearn.linear_model as lm\n",
    "\n",
    "# Lasso with cross-validation\n",
    "flex_model_train = smf.ols(model2, data = data_train)\n",
    "X_train_flex = flex_model_train.data.exog # create model matrix\n",
    "\n",
    "\n",
    "# train model using CV \n",
    "lassocv_reg = lm.LassoCV(cv=10, fit_intercept= False)\n",
    "lassomod = lassocv_reg.fit(X_train_flex, y_train)\n",
    "\n",
    "# predict out of sample\n",
    "data_test = pd.concat([y_test, X_test], axis=1)\n",
    "flex_model_test = smf.ols(model2, data = data_test)\n",
    "X_test_matrix = flex_model_test.data.exog\n",
    "y2 = flex_model_test.data.endog\n",
    "trainreglasso = lassomod.predict(X_test_matrix)\n",
    "\n",
    "# calculating out-of-sample MSE\n",
    "MSE_lasso = np.mean((y_test-trainreglasso)**2)\n",
    "R2_lasso = 1. - MSE_lasso/np.var(y_test)\n",
    "\n",
    "print(\"Test MSE for the flexibel model using lasso: \"+ str(MSE_lasso))\n",
    "print(\"Test R2 for the flexibel model using lasso: \"+ str(R2_lasso))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pv8cfEj1c379",
    "outputId": "91b70f38-9cf6-4744-9268-77d1c5373f8a"
   },
   "outputs": [],
   "source": [
    "# Ridge with cross-validation\n",
    "\n",
    "# train model using CV \n",
    "ridgecv_reg = lm.RidgeCV(cv=5, fit_intercept= False)\n",
    "ridgemod = ridgecv_reg.fit(X_train_flex, y_train)\n",
    "\n",
    "# predict out of sample\n",
    "trainregridge = ridgemod.predict(X_test_matrix)\n",
    "\n",
    "# calculating out-of-sample MSE\n",
    "MSE_ridge = np.mean((np.array(y_test)-trainregridge)**2)\n",
    "R2_ridge = 1. - MSE_ridge/np.var(y_test)\n",
    "\n",
    "print(\"Test MSE for the flexibel model using ridge: \"+ str(MSE_ridge))\n",
    "print(\"Test R2 for the flexibel model using ridge: \"+ str(R2_ridge))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Y2NnCvw6VC0h",
    "outputId": "43971522-6c3c-44c4-ffe0-3488f2508706"
   },
   "outputs": [],
   "source": [
    "trainregridge\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "PM2A_prediction",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
