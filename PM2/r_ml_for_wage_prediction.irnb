{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_execution_state": "idle",
    "_uuid": "051d70d956493feee0c6d64651c6a088724dca2a",
    "id": "dtt9U13qNGOn",
    "papermill": {
     "duration": 0.036479,
     "end_time": "2021-02-13T18:19:43.396666",
     "exception": false,
     "start_time": "2021-02-13T18:19:43.360187",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# A Simple Case Study using Wage Data from 2015"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LCKYnHWrNGOn",
    "papermill": {
     "duration": 0.036639,
     "end_time": "2021-02-13T18:19:43.468425",
     "exception": false,
     "start_time": "2021-02-13T18:19:43.431786",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "We illustrate how to predict an outcome variable $Y$ in a high-dimensional setting, where the number of covariates $p$ is large in relation to the sample size $n$. We use linear prediction rules for estimation, including OLS and the penalized linear methods we've studied. Later, we will also consider nonlinear prediction rules including tree-based methods and neural nets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VPwV7nNDS_nz",
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "install.packages(\"xtable\")\n",
    "install.packages(\"hdm\")\n",
    "install.packages(\"glmnet\")\n",
    "install.packages(\"MLmetrics\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "library(hdm)\n",
    "library(xtable)\n",
    "library(glmnet)\n",
    "library(MLmetrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gRYGynhrNGOo",
    "papermill": {
     "duration": 0.034705,
     "end_time": "2021-02-13T18:19:43.537814",
     "exception": false,
     "start_time": "2021-02-13T18:19:43.503109",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S_YMURKqNGOo",
    "papermill": {
     "duration": 0.036082,
     "end_time": "2021-02-13T18:19:43.609347",
     "exception": false,
     "start_time": "2021-02-13T18:19:43.573265",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Again, we consider data from the U.S. March Supplement of the Current Population Survey (CPS) in 2015.\n",
    "The preproccessed sample consists of $5150$ never-married individuals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B09_5wrUNGOo",
    "papermill": {
     "duration": 0.279387,
     "end_time": "2021-02-13T18:19:43.923823",
     "exception": false,
     "start_time": "2021-02-13T18:19:43.644436",
     "status": "completed"
    },
    "tags": [],
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "file <- \"https://raw.githubusercontent.com/CausalAIBook/MetricsMLNotebooks/main/data/wage2015_subsample_inference.csv\"\n",
    "data <- read.csv(file)\n",
    "dim(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ol9dToMQNGOq",
    "papermill": {
     "duration": 0.034902,
     "end_time": "2021-02-13T18:19:43.994834",
     "exception": false,
     "start_time": "2021-02-13T18:19:43.959932",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "The outcomes $Y_i$'s are hourly (log) wages of never-married workers living in the U.S. The raw regressors $Z_i$'s consist of a variety of characteristics, including experience, education and industry and occupation indicators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bsqnI6d0NGOq",
    "papermill": {
     "duration": 0.091723,
     "end_time": "2021-02-13T18:19:44.123394",
     "exception": false,
     "start_time": "2021-02-13T18:19:44.031671",
     "status": "completed"
    },
    "tags": [],
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "Z <- subset(data, select = -c(lwage, wage)) # regressors\n",
    "colnames(Z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VeU2XMYENGOr",
    "papermill": {
     "duration": 0.037074,
     "end_time": "2021-02-13T18:19:44.196749",
     "exception": false,
     "start_time": "2021-02-13T18:19:44.159675",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "The following figure shows the weekly wage distribution from the US survey data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d3sbTfpRNGOr",
    "papermill": {
     "duration": 0.443391,
     "end_time": "2021-02-13T18:19:44.677379",
     "exception": false,
     "start_time": "2021-02-13T18:19:44.233988",
     "status": "completed"
    },
    "tags": [],
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "hist(data$wage, xlab = \"hourly wage\", main = \"Empirical wage distribution from the US survey data\", breaks = 35)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BmGfrWFNNGOs",
    "papermill": {
     "duration": 0.036602,
     "end_time": "2021-02-13T18:19:44.752465",
     "exception": false,
     "start_time": "2021-02-13T18:19:44.715863",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Wages show a high degree of skewness. Hence, wages are transformed in almost all studies by\n",
    "the logarithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "msBSjtuKNGOs",
    "papermill": {
     "duration": 0.036009,
     "end_time": "2021-02-13T18:19:44.826260",
     "exception": false,
     "start_time": "2021-02-13T18:19:44.790251",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B-XZMCogNGOs",
    "papermill": {
     "duration": 0.036925,
     "end_time": "2021-02-13T18:19:44.899159",
     "exception": false,
     "start_time": "2021-02-13T18:19:44.862234",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Due to the skewness of the data, we are considering log wages which leads to the following regression model\n",
    "\n",
    "$$log(wage) = g(Z) + \\epsilon.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pNLS-C_7NGOt",
    "papermill": {
     "duration": 0.036183,
     "end_time": "2021-02-13T18:19:44.971528",
     "exception": false,
     "start_time": "2021-02-13T18:19:44.935345",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "In this notebook, we will evaluate *linear* prediction rules. In later notebooks, we will also utilize nonlinear prediction methods. In linear models, we estimate the prediction rule of the form\n",
    "\n",
    "$$\\hat g(Z) = \\hat \\beta'X.$$\n",
    "\n",
    "Again, we generate $X$ in three ways:\n",
    "\n",
    "1. Basic Model:   $X$ consists of a set of raw regressors (e.g. gender, experience, education indicators, regional indicators).\n",
    "\n",
    "\n",
    "2. Flexible Model:  $X$ consists of all raw regressors from the basic model plus occupation and industry indicators, transformations (e.g., $\\operatorname{exp}^2$ and $\\operatorname{exp}^3$) and additional two-way interactions.\n",
    "\n",
    "3. Extra Flexible Model: $X$ takes the flexible model and takes all pairwise interactions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-kGLyGXvNGOt",
    "papermill": {
     "duration": 0.037318,
     "end_time": "2021-02-13T18:19:45.044959",
     "exception": false,
     "start_time": "2021-02-13T18:19:45.007641",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "To evaluate the out-of-sample performance, we split the data first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oAqJIgmlNGOt",
    "papermill": {
     "duration": 0.062188,
     "end_time": "2021-02-13T18:19:45.143118",
     "exception": false,
     "start_time": "2021-02-13T18:19:45.080930",
     "status": "completed"
    },
    "tags": [],
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "set.seed(1234)\n",
    "training <- sample(nrow(data), nrow(data) * (3 / 4), replace = FALSE)\n",
    "\n",
    "data_train <- data[training, ]\n",
    "data_test <- data[-training, ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C6qC_wyjNGOu",
    "papermill": {
     "duration": 0.060969,
     "end_time": "2021-02-13T18:19:45.445389",
     "exception": false,
     "start_time": "2021-02-13T18:19:45.384420",
     "status": "completed"
    },
    "tags": [],
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "y_train <- data_train$lwage\n",
    "y_test <- data_test$lwage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5-CCjJnbNGOt",
    "papermill": {
     "duration": 0.038774,
     "end_time": "2021-02-13T18:19:45.217757",
     "exception": false,
     "start_time": "2021-02-13T18:19:45.178983",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "We are starting by running a simple OLS regression. We fit the basic and flexible model to our training data by running an ols regression and compute the R-squared on the test sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qbummAR-NGOu",
    "papermill": {
     "duration": 0.037704,
     "end_time": "2021-02-13T18:19:45.622370",
     "exception": false,
     "start_time": "2021-02-13T18:19:45.584666",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "As known from our first lab, the basic model consists of $51$ regressors and the flexible model of $246$ regressors. Let us fit our models to the training sample using the two different model specifications. We are starting by running a simple ols regression and computing the mean squared error and $R^2$ on the test sample."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4LNs__OcfmFV"
   },
   "source": [
    "### Low dimensional specification (basic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WD7tshOlNGOt",
    "papermill": {
     "duration": 0.094135,
     "end_time": "2021-02-13T18:19:45.347955",
     "exception": false,
     "start_time": "2021-02-13T18:19:45.253820",
     "status": "completed"
    },
    "tags": [],
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "x_basic <- \"sex + exp1 + shs + hsg+ scl + clg + mw + so + we + C(occ2)+ C(ind2)\"\n",
    "formula_basic <- as.formula(paste(\"lwage\", \"~\", x_basic))\n",
    "model_x_basic_train <- model.matrix(formula_basic, data_train)\n",
    "model_x_basic_test <- model.matrix(formula_basic, data_test)\n",
    "p_basic <- dim(model_x_basic_train)[2]\n",
    "p_basic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kx1xoWHFNGOv",
    "papermill": {
     "duration": 0.069537,
     "end_time": "2021-02-13T18:19:45.887169",
     "exception": false,
     "start_time": "2021-02-13T18:19:45.817632",
     "status": "completed"
    },
    "tags": [],
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# ols (basic model)\n",
    "fit_lm_basic <- lm(formula_basic, data_train)\n",
    "# Compute the Out-Of-Sample Performance\n",
    "yhat_lm_basic <- predict(fit_lm_basic, newdata = data_test)\n",
    "cat(\"Basic model MSE (OLS): \", mean((y_test - yhat_lm_basic)^2)) # MSE OLS (basic model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hDlMXF0ANGOw",
    "papermill": {
     "duration": 0.052764,
     "end_time": "2021-02-13T18:19:46.122829",
     "exception": false,
     "start_time": "2021-02-13T18:19:46.070065",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "To determine the out-of-sample $MSE$ and the standard error in one step, we can use the function *lm*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ERCs22oDNGOw",
    "papermill": {
     "duration": 0.076484,
     "end_time": "2021-02-13T18:19:46.239015",
     "exception": false,
     "start_time": "2021-02-13T18:19:46.162531",
     "status": "completed"
    },
    "tags": [],
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "mse_lm_basic <- summary(lm((y_test - yhat_lm_basic)^2 ~ 1))$coef[1:2]\n",
    "mse_lm_basic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3PQ-f_waNGOw",
    "papermill": {
     "duration": 0.039088,
     "end_time": "2021-02-13T18:19:46.317915",
     "exception": false,
     "start_time": "2021-02-13T18:19:46.278827",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "We also compute the out-of-sample $R^2$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YLgvLE2BNGOw",
    "papermill": {
     "duration": 0.057098,
     "end_time": "2021-02-13T18:19:46.413754",
     "exception": false,
     "start_time": "2021-02-13T18:19:46.356656",
     "status": "completed"
    },
    "tags": [],
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "r2_lm_basic <- 1 - mse_lm_basic[1] / var(y_test)\n",
    "cat(\"Basic model R^2 (OLS): \", r2_lm_basic) # MSE OLS (basic model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tTZyELgyf51J"
   },
   "source": [
    "### High-dimensional specification (flexible)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J8Rffx0ef3nM",
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "x_flex <- paste(\"sex + exp1 + shs + hsg + scl + clg + C(occ2) + C(ind2) + mw + so + we \",\n",
    "                \"+ (exp1 + exp2 + exp3 + exp4) * (shs + hsg + scl + clg + C(occ2) + C(ind2) + mw + so + we)\")\n",
    "formula_flex <- as.formula(paste(\"lwage\", \"~\", x_flex))\n",
    "model_x_flex_train <- model.matrix(formula_flex, data_train)\n",
    "model_x_flex_test <- model.matrix(formula_flex, data_test)\n",
    "p_flex <- dim(model_x_flex_train)[2]\n",
    "p_flex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "77G7YfbnNGOw",
    "papermill": {
     "duration": 0.039585,
     "end_time": "2021-02-13T18:19:46.492903",
     "exception": false,
     "start_time": "2021-02-13T18:19:46.453318",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "We repeat the same procedure for the flexible model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KpRtjaAlNGOw",
    "papermill": {
     "duration": 0.198636,
     "end_time": "2021-02-13T18:19:46.730717",
     "exception": false,
     "start_time": "2021-02-13T18:19:46.532081",
     "status": "completed"
    },
    "tags": [],
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# ols (flexible model)\n",
    "fit_lm_flex <- lm(formula_flex, data_train)\n",
    "# Compute the Out-Of-Sample Performance\n",
    "options(warn = -1)\n",
    "yhat_lm_flex <- predict(fit_lm_flex, newdata = data_test)\n",
    "mse_lm_flex <- summary(lm((y_test - yhat_lm_flex)^2 ~ 1))$coef[1:2]\n",
    "r2_lm_flex <- 1 - mse_lm_flex[1] / var(y_test)\n",
    "cat(\"Flexible model R^2 (OLS): \", r2_lm_flex) # MSE OLS (flexible model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sShiB-i9NGOx",
    "papermill": {
     "duration": 0.042521,
     "end_time": "2021-02-13T18:19:46.935859",
     "exception": false,
     "start_time": "2021-02-13T18:19:46.893338",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Penalized regressions (flexible model)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5wgFVRkkNGOx",
    "papermill": {
     "duration": 0.051953,
     "end_time": "2021-02-13T18:19:46.853182",
     "exception": false,
     "start_time": "2021-02-13T18:19:46.801229",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "We observe that ols regression works better for the basic model with smaller $p/n$ ratio. We proceed by running penalized regressions first for the flexible model, tuned via cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n3jvO5HQmzbf",
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "fit_lasso_cv_flex <- cv.glmnet(model_x_flex_train, y_train, family = \"gaussian\", alpha = 1)\n",
    "fit_ridge_flex <- cv.glmnet(model_x_flex_train, y_train, family = \"gaussian\", alpha = 0)\n",
    "fit_elnet_flex <- cv.glmnet(model_x_flex_train, y_train, family = \"gaussian\", alpha = .5)\n",
    "\n",
    "yhat_lasso_cv_flex <- predict(fit_lasso_cv_flex, newx = model_x_flex_test)\n",
    "yhat_ridge_flex <- predict(fit_ridge_flex, newx = model_x_flex_test)\n",
    "yhat_elnet_flex <- predict(fit_elnet_flex, newx = model_x_flex_test)\n",
    "\n",
    "mse_lasso_cv_flex <- summary(lm((y_test - yhat_lasso_cv_flex)^2 ~ 1))$coef[1:2]\n",
    "mse_ridge_flex <- summary(lm((y_test - yhat_ridge_flex)^2 ~ 1))$coef[1:2]\n",
    "mse_elnet_flex <- summary(lm((y_test - yhat_elnet_flex)^2 ~ 1))$coef[1:2]\n",
    "\n",
    "r2_lasso_cv_flex <- 1 - mse_lasso_cv_flex[1] / var(y_test)\n",
    "r2_ridge_flex <- 1 - mse_ridge_flex[1] / var(y_test)\n",
    "r2_elnet_flex <- 1 - mse_elnet_flex[1] / var(y_test)\n",
    "\n",
    "# R^2 using cross-validation (flexible model)\n",
    "cat(\"Flexible model R^2 (Lasso): \", r2_lasso_cv_flex)\n",
    "cat(\"\\nFlexible model R^2 (Ridge): \", r2_ridge_flex)\n",
    "cat(\"\\nFlexible model R^2 (Elastic Net): \", r2_elnet_flex)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZytMS-aCNGOx",
    "papermill": {
     "duration": 0.040161,
     "end_time": "2021-02-13T18:19:47.015626",
     "exception": false,
     "start_time": "2021-02-13T18:19:46.975465",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "We can also try a variant of the `l1` penalty, where the weight is chosen based on theoretical derivations. We use package *hdm* and the function *rlasso*, relying on a theoretical based choice of the penalty level $\\lambda$ in the lasso regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n-D_fRJBnkEH"
   },
   "source": [
    "Specifically, we use \"plug-in\" tuning with a theoretically valid choice of penalty $\\lambda = 2 \\cdot c \\hat{\\sigma} \\sqrt{n} \\Phi^{-1}(1-\\alpha/2p)$, where $c>1$ and $1-\\alpha$ is a confidence level, $\\Phi^{-1}$ denotes the quantile function, and $\\hat{\\sigma}$ is estimated in an iterative manner (see corresponding notes in book). Under homoskedasticity, this choice ensures that the Lasso predictor is well behaved, delivering good predictive performance under approximate sparsity. In practice, this formula will work well even in the absence of homoskedasticity, especially when the random variables $\\epsilon$ and $X$ in the regression equation decay quickly at the tails.\n",
    "\n",
    "In practice, many people choose to use cross-validation, which is perfectly fine for predictive tasks. However, when conducting inference, to make our analysis valid we will require cross-fitting in addition to cross-validation. As we have not yet discussed cross-fitting, we rely on this theoretically-driven penalty in order to allow for accurate inference in the upcoming notebooks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yBiZ3q3INGOy",
    "papermill": {
     "duration": 0.049543,
     "end_time": "2021-02-13T18:19:47.757271",
     "exception": false,
     "start_time": "2021-02-13T18:19:47.707728",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Now, we repeat the same procedure for the flexible model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PlTdJh5PNGOy",
    "papermill": {
     "duration": 3.430649,
     "end_time": "2021-02-13T18:19:51.229007",
     "exception": false,
     "start_time": "2021-02-13T18:19:47.798358",
     "status": "completed"
    },
    "tags": [],
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "fit_rlasso_flex <- hdm::rlasso(formula_flex, data_train, post = FALSE)\n",
    "fit_rlasso_post_flex <- hdm::rlasso(formula_flex, data_train, post = TRUE)\n",
    "yhat_rlasso_flex <- predict(fit_rlasso_flex, newdata = data_test)\n",
    "yhat_rlasso_post_flex <- predict(fit_rlasso_post_flex, newdata = data_test)\n",
    "\n",
    "mse_lasso_flex <- summary(lm((y_test - yhat_rlasso_flex)^2 ~ 1))$coef[1:2]\n",
    "mse_lasso_post_flex <- summary(lm((y_test - yhat_rlasso_post_flex)^2 ~ 1))$coef[1:2]\n",
    "\n",
    "r2_lasso_flex <- 1 - mse_lasso_flex[1] / var(y_test)\n",
    "r2_lasso_post_flex <- 1 - mse_lasso_post_flex[1] / var(y_test)\n",
    "# R^2 theoretically chosen penalty (flexible model)\n",
    "cat(\"Flexible model R^2 (RLasso): \", r2_lasso_flex)\n",
    "cat(\"\\nFlexible model R^2 (RLasso post): \", r2_lasso_post_flex)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aude922IfxBG"
   },
   "source": [
    "Finally, we try the combination of a sparse and a dense coefficient using the LAVA method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WgBPFQ72ftBz",
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Define function to compute lava estimator. Doing an iterative scheme with fixed\n",
    "# number of iteration. Could iterate until a convergence criterion is met.\n",
    "lava_predict <- function(X, Y, newX, lambda1, lambda2, iter = 5) {\n",
    "\n",
    "  # Need to demean internally\n",
    "  dy <- Y - mean(Y)\n",
    "  dx <- scale(X, scale = FALSE)\n",
    "\n",
    "  sp1 <- glmnet::glmnet(dx, dy, lambda = lambda1) # lasso step fits \"sparse part\"\n",
    "  de1 <- glmnet::glmnet(dx, dy - predict(sp1, newx = dx), alpha = 0, lambda = lambda2)\n",
    "\n",
    "  i <- 1\n",
    "  while (i <= iter) {\n",
    "    sp1 <- glmnet::glmnet(dx, dy - predict(de1, newx = dx, s = \"lambda.min\"), lambda = lambda1)\n",
    "    de1 <- glmnet::glmnet(dx, dy - predict(sp1, newx = dx, s = \"lambda.min\"), alpha = 0, lambda = lambda2)\n",
    "    i <- i + 1\n",
    "  }\n",
    "\n",
    "  bhat <- sp1$beta + de1$beta\n",
    "  a0 <- mean(Y) - sum(colMeans(X) * bhat)\n",
    "\n",
    "  # Need to add intercept to output\n",
    "\n",
    "  yhat <- newX %*% bhat + a0\n",
    "\n",
    "  return(yhat)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2HFE2EbdkMjj",
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# define function to get predictions and r2 scores for lava estimator\n",
    "lava_yhat_r2 <- function(xtr_mod, xte_mod, ytr, yte, num_folds = 5) {\n",
    "  # 5-fold CV. glmnet does cross-validation internally and\n",
    "  # relatively efficiently. We're going to write out all the steps to make sure\n",
    "  # we're using the same CV folds across all procedures in a transparent way and\n",
    "  # to keep the overall structure clear as well.\n",
    "\n",
    "  # Setup for brute force K-Fold CV\n",
    "  n <- length(ytr)\n",
    "  Kf <- num_folds # Number of folds\n",
    "  sampleframe <- rep(1:Kf, ceiling(n / Kf))\n",
    "  cvgroup <- sample(sampleframe, size = n, replace = FALSE) # CV groups\n",
    "\n",
    "\n",
    "  ## ------------------------------------------------------------\n",
    "  # We're going to take a shortcut and use the range of lambda values that come out\n",
    "  # of the default implementation in glmnet for everything. Could do better here - maybe\n",
    "\n",
    "  ## Fit ridge on grid of lambda values (chosen by default using glmnet) using basic model.\n",
    "  ridge_mod <- glmnet::glmnet(xtr_mod, ytr, alpha = 0) # alpha = 0 gives ridge\n",
    "  ridge_lambda <- ridge_mod$lambda # values of penalty parameter\n",
    "\n",
    "  ## Fit lasso on grid of lambda values (chosen by default using glmnet) using basic model.\n",
    "  lasso_mod <- glmnet::glmnet(xtr_mod, ytr) # default is lasso (equivalent to alpha = 1)\n",
    "  lasso_lambda <- lasso_mod$lambda # values of penalty parameter\n",
    "\n",
    "  ## ------------------------------------------------------------\n",
    "\n",
    "\n",
    "  # Lava - Using a double loop over candidate penalty parameter values.\n",
    "\n",
    "  lambda1_lava_mod <- lasso_mod$lambda[seq(5, length(lasso_lambda), 10)]\n",
    "  lambda2_lava_mod <- ridge_mod$lambda[seq(5, length(ridge_lambda), 10)]\n",
    "\n",
    "  cv_mod_lava <- matrix(0, length(lambda1_lava_mod), length(lambda2_lava_mod))\n",
    "\n",
    "  for (k in 1:Kf) {\n",
    "    indk <- cvgroup == k\n",
    "\n",
    "    k_xtr_mod <- xtr_mod[!indk, ]\n",
    "    k_ytr <- ytr[!indk]\n",
    "    k_xte_mod <- xtr_mod[indk, ]\n",
    "    k_yte <- ytr[indk]\n",
    "\n",
    "    for (ii in seq_along(lambda1_lava_mod)) {\n",
    "      for (jj in seq_along(lambda2_lava_mod)) {\n",
    "        cv_mod_lava[ii, jj] <- cv_mod_lava[ii, jj] +\n",
    "          sum((k_yte - lava_predict(k_xtr_mod, k_ytr,\n",
    "                                    newX = k_xte_mod,\n",
    "                                    lambda1 = lambda1_lava_mod[ii],\n",
    "                                    lambda2 = lambda2_lava_mod[jj]))^2)\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "\n",
    "  # Get CV min values of tuning parameters\n",
    "  cvmin_lava_mod <- which(cv_mod_lava == min(cv_mod_lava), arr.ind = TRUE)\n",
    "  cvlambda1_lava_mod <- lambda1_lava_mod[cvmin_lava_mod[1]]\n",
    "  cvlambda2_lava_mod <- lambda2_lava_mod[cvmin_lava_mod[2]]\n",
    "\n",
    "  #### Look at performance on test sample\n",
    "\n",
    "  # Calculate R^2 in training data and in validation data as measures\n",
    "  # Refit on entire training sample\n",
    "\n",
    "  #### CV-min model\n",
    "\n",
    "  # In sample fit\n",
    "  cvmin_yhat_lava_tr <- lava_predict(xtr_mod, ytr,\n",
    "    newX = xtr_mod,\n",
    "    lambda1 = cvlambda1_lava_mod,\n",
    "    lambda2 = cvlambda2_lava_mod\n",
    "  )\n",
    "  r2_lava_mod <- 1 - sum((ytr - cvmin_yhat_lava_tr)^2) / sum((ytr - mean(ytr))^2)\n",
    "\n",
    "  # Out of sample fit\n",
    "  cvmin_yhat_lava_test <- lava_predict(xtr_mod, ytr,\n",
    "    newX = xte_mod,\n",
    "    lambda1 = cvlambda1_lava_mod,\n",
    "    lambda2 = cvlambda2_lava_mod\n",
    "  )\n",
    "  r2v_lava_mod <- 1 - sum((yte - cvmin_yhat_lava_test)^2) / sum((yte - mean(ytr))^2)\n",
    "\n",
    "  #### Use average model across cv-folds and refit model using all training data\n",
    "  ###### we won't report these results.\n",
    "  ###### Averaging is theoretically more solid, but cv-min is more practical.\n",
    "  n_tr <- length(ytr)\n",
    "  n_te <- length(yte)\n",
    "  yhat_tr_lava_mod <- matrix(0, n_tr, Kf)\n",
    "  yhat_te_lava_mod <- matrix(0, n_te, Kf)\n",
    "\n",
    "\n",
    "  for (k in 1:Kf) {\n",
    "    indk <- cvgroup == k\n",
    "\n",
    "    k_xtr_mod <- xtr_mod[!indk, ]\n",
    "    k_ytr <- ytr[!indk]\n",
    "\n",
    "    # Lava\n",
    "    yhat_tr_lava_mod[, k] <- as.vector(lava_predict(k_xtr_mod, k_ytr,\n",
    "      newX = xtr_mod,\n",
    "      lambda1 = cvlambda1_lava_mod,\n",
    "      lambda2 = cvlambda2_lava_mod\n",
    "    ))\n",
    "    yhat_te_lava_mod[, k] <- as.vector(lava_predict(k_xtr_mod, k_ytr,\n",
    "      newX = xte_mod,\n",
    "      lambda1 = cvlambda1_lava_mod,\n",
    "      lambda2 = cvlambda2_lava_mod\n",
    "    ))\n",
    "  }\n",
    "\n",
    "  avg_yhat_lava_tr <- rowMeans(yhat_tr_lava_mod)\n",
    "  avg_yhat_lava_test <- rowMeans(yhat_te_lava_mod)\n",
    "\n",
    "  r2_cv_ave_lava_mod <- 1 - sum((ytr - avg_yhat_lava_tr)^2) / sum((ytr - mean(ytr))^2)\n",
    "  r2v_cv_ave_lava_mod <- 1 - sum((yte - avg_yhat_lava_test)^2) / sum((yte - mean(ytr))^2)\n",
    "\n",
    "  return(c(\n",
    "    cvlambda1_lava_mod,\n",
    "    cvlambda2_lava_mod,\n",
    "    cvmin_yhat_lava_tr, # CV_min\n",
    "    cvmin_yhat_lava_test, # CV_min\n",
    "    r2_lava_mod, # CV_min\n",
    "    r2v_lava_mod, # CV_min\n",
    "    avg_yhat_lava_tr, # Average across Folds\n",
    "    avg_yhat_lava_test, # Average across Folds\n",
    "    r2_cv_ave_lava_mod, # Average across Folds\n",
    "    r2v_cv_ave_lava_mod # Average across Folds\n",
    "  ))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Uw3LMCiskJzV",
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "fit_lava_flex <- lava_yhat_r2(model_x_flex_train, model_x_flex_test, y_train, y_test)\n",
    "cat(\"Flexible model R^2 (LAVA): \", fit_lava_flex[[6]]) # using CV_min"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f8lYhGaWfpYR"
   },
   "source": [
    "<!-- We find that for this dataset the low dimensional OLS was the best among all specifications. The high-dimensional approaches did not manage to increase predictive power. -->\n",
    "\n",
    "We find that for this dataset the low dimensional OLS is sufficient. The high-dimensional approaches did not manage to substantively increase predictive power."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bxuPZI4Zx0Vm"
   },
   "source": [
    "### Extra high-dimensional specification (extra flexible)\n",
    "\n",
    "We repeat the same procedure for the extra flexible model.\n",
    "\n",
    "<!-- Given the results above, it is not immediately clear why one would choose to use Lasso as results are fairly similar. To motivate, we consider an extra flexible model to show how OLS can overfit significantly to the in-sample train data and perform poorly on the out-of-sample testing data. -->\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JsFhSsM_rGjN",
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "x_extra <- \" sex + (exp1 + exp2 + exp3 + exp4 + shs + hsg + scl + clg + C(occ2) + C(ind2) + mw + so + we)^2\"\n",
    "formula_extra <- as.formula(paste(\"lwage\", \"~\", x_extra))\n",
    "model_x_extra_train <- model.matrix(formula_extra, data_train)\n",
    "model_x_extra_test <- model.matrix(formula_extra, data_test)\n",
    "p_extra <- dim(model_x_extra_train)[2]\n",
    "p_extra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eheA1UPBsHfL",
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# ols (extra flexible model)\n",
    "fit_lm_extra <- lm(formula_extra, data_train)\n",
    "options(warn = -1)\n",
    "yhat_lm_extra <- predict(fit_lm_extra, newdata = data_test)\n",
    "mse_lm_extra <- summary(lm((y_test - yhat_lm_extra)^2 ~ 1))$coef[1:2]\n",
    "r2_lm_extra <- 1 - mse_lm_extra[1] / var(y_test)\n",
    "cat(\"Extra flexible model R^2 (OLS): \", r2_lm_extra)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Scm5monOrJu5"
   },
   "source": [
    "#### Penalized regressions (extra flexible model)\n",
    "\n",
    "Now let's repeat our penalized regression analysis for the extra flexible model. Note this block takes a while ~ 1 hour 15 minutes. To reduce time substantially, reduce the number of folds in LAVA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tOKoNLKFovrI",
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# penalized regressions\n",
    "fit_lasso_cv_extra <- cv.glmnet(model_x_extra_train, y_train, family = \"gaussian\", alpha = 1)\n",
    "fit_ridge_extra <- cv.glmnet(model_x_extra_train, y_train, family = \"gaussian\", alpha = 0)\n",
    "fit_elnet_extra <- cv.glmnet(model_x_extra_train, y_train, family = \"gaussian\", alpha = .5)\n",
    "fit_rlasso_extra <- hdm::rlasso(formula_extra, data_train, post = FALSE)\n",
    "fit_rlasso_post_extra <- hdm::rlasso(formula_extra, data_train, post = TRUE)\n",
    "fit_lava_extra <- lava_yhat_r2(model_x_extra_train, model_x_extra_test, y_train, y_test)\n",
    "\n",
    "yhat_lasso_cv_extra <- predict(fit_lasso_cv_extra, newx = model_x_extra_test)\n",
    "yhat_ridge_extra <- predict(fit_ridge_extra, newx = model_x_extra_test)\n",
    "yhat_elnet_extra <- predict(fit_elnet_extra, newx = model_x_extra_test)\n",
    "yhat_rlasso_extra <- predict(fit_rlasso_extra, newdata = data_test)\n",
    "yhat_rlasso_post_extra <- predict(fit_rlasso_post_extra, newdata = data_test)\n",
    "yhat_lava_extra <- fit_lava_extra[[4]]\n",
    "\n",
    "mse_lasso_cv_extra <- summary(lm((y_test - yhat_lasso_cv_extra)^2 ~ 1))$coef[1:2]\n",
    "mse_ridge_extra <- summary(lm((y_test - yhat_ridge_extra)^2 ~ 1))$coef[1:2]\n",
    "mse_elnet_extra <- summary(lm((y_test - yhat_elnet_extra)^2 ~ 1))$coef[1:2]\n",
    "mse_lasso_extra <- summary(lm((y_test - yhat_rlasso_extra)^2 ~ 1))$coef[1:2]\n",
    "mse_lasso_post_extra <- summary(lm((y_test - yhat_rlasso_post_extra)^2 ~ 1))$coef[1:2]\n",
    "mse_lava_extra <- summary(lm(as.vector(y_test - yhat_lava_extra)^2 ~ 1))$coef[1:2]\n",
    "\n",
    "r2_lasso_cv_extra <- 1 - mse_lasso_cv_extra[1] / var(y_test)\n",
    "r2_ridge_extra <- 1 - mse_ridge_extra[1] / var(y_test)\n",
    "r2_elnet_extra <- 1 - mse_elnet_extra[1] / var(y_test)\n",
    "r2_lasso_extra <- 1 - mse_lasso_extra[1] / var(y_test)\n",
    "r2_lasso_post_extra <- 1 - mse_lasso_post_extra[1] / var(y_test)\n",
    "r2_lava_extra <- 1 - mse_lava_extra[1] / var(y_test)\n",
    "\n",
    "# R^2 (extra flexible)\n",
    "cat(\"\\nExtra flexible model R^2 (Lasso): \", r2_lasso_cv_extra)\n",
    "cat(\"\\nExtra flexible model R^2 (Ridge): \", r2_ridge_extra)\n",
    "cat(\"\\nExtra flexible model R^2 (Elastic Net): \", r2_elnet_extra)\n",
    "cat(\"\\nExtra flexible model R^2 (RLasso): \", r2_lasso_extra)\n",
    "cat(\"\\nExtra flexible model R^2 (RLasso post): \", r2_lasso_post_extra)\n",
    "cat(\"\\nExtra flexible model R^2 (LAVA): \", r2_lava_extra) # using CV_min"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Btez-AI8yE7S"
   },
   "source": [
    "<!-- As shown above, the overfitting effect is mitigated with the penalized regression model. -->"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.6.3"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 90.376935,
   "end_time": "2021-02-13T18:21:10.266455",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2021-02-13T18:19:39.889520",
   "version": "2.2.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
