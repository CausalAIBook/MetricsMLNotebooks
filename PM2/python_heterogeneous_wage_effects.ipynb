{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "papermill": {
          "duration": 0.011184,
          "end_time": "2021-02-28T17:17:25.882205",
          "exception": false,
          "start_time": "2021-02-28T17:17:25.871021",
          "status": "completed"
        },
        "tags": [],
        "id": "Z4jKiGPB__ke"
      },
      "source": [
        "# Application: Heterogeneous Effect of Sex on Wage Using Double Lasso\n",
        "\n",
        "We use US census data from the year 2015 to analyse the effect of sex and interaction effects of other variables with sex on wage jointly. The dependent variable is the logarithm of the wage, the target variable is *female* (in combination with other variables). All other variables denote some other socio-economic characteristics, e.g. marital status, education, and experience.  For a detailed description of the variables we refer to the help page.\n",
        "\n",
        "\n",
        "\n",
        "This analysis allows a closer look how discrimination according to sex is related to other socio-economic variables."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hfOxxCgf__kc"
      },
      "outputs": [],
      "source": [
        "# Import relevant packages\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from scipy.stats import norm\n",
        "from sklearn.linear_model import LassoCV, Lasso, LinearRegression\n",
        "import patsy\n",
        "import warnings\n",
        "import statsmodels.api as sm\n",
        "warnings.simplefilter('ignore')\n",
        "np.random.seed(1234)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mn-ywPJb__kf"
      },
      "outputs": [],
      "source": [
        "file = \"https://raw.githubusercontent.com/CausalAIBook/MetricsMLNotebooks/main/data/wage2015_subsample_inference.csv\"\n",
        "data = pd.read_csv(file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JjAg9We1__kf"
      },
      "outputs": [],
      "source": [
        "data.describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kvlmaeFU__kf"
      },
      "source": [
        "Define outcome and regressors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FheFuCL1__kg"
      },
      "outputs": [],
      "source": [
        "y = np.log(data['wage']).values\n",
        "Z = data.drop(['wage', 'lwage'], axis=1)\n",
        "Z.columns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g2vGrvdJ__kg"
      },
      "source": [
        "## Feature Engineering"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JxlhqsDs__kg"
      },
      "source": [
        "Construct all our control variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tjA6vrOf__kg"
      },
      "outputs": [],
      "source": [
        "# Ultra flexible controls of all pair-wise interactions (around 1k variables); un-comment to run this\n",
        "Zcontrols = patsy.dmatrix('0 + (shs+hsg+scl+clg+C(occ2)+C(ind2)+mw+so+we+exp1+exp2+exp3+exp4)**2',\n",
        "                           Z, return_type='dataframe')\n",
        "\n",
        "Zcontrols = Zcontrols - Zcontrols.mean(axis=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hD36OSyb__kg"
      },
      "source": [
        "Construct all the variables that we will use to model heterogeneity of effect in a linear manner"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DNPlH_rs__kh"
      },
      "outputs": [],
      "source": [
        "Zhet = patsy.dmatrix('0 + (shs+hsg+scl+clg+mw+so+we+exp1+exp2+exp3+exp4)',\n",
        "                      Z, return_type='dataframe')\n",
        "Zhet = Zhet - Zhet.mean(axis=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "woskUlaj__kh"
      },
      "source": [
        "Construct all interaction variables between sex and heterogeneity variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kNsQotu6__kh"
      },
      "outputs": [],
      "source": [
        "Zhet['sex'] = Z['sex']\n",
        "Zinteractions = patsy.dmatrix('0 + sex + sex * (shs+hsg+scl+clg+mw+so+we+exp1+exp2+exp3+exp4)',\n",
        "                               Zhet, return_type='dataframe')\n",
        "interaction_cols = [c for c in Zinteractions.columns if c.startswith('sex')]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-iMb_R87__ki"
      },
      "source": [
        "Put all the variables together"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bzFOlDdw__ki"
      },
      "outputs": [],
      "source": [
        "X = pd.concat([Zinteractions, Zcontrols], axis=1)\n",
        "X.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dJDCw_Zc__ki"
      },
      "source": [
        "## Double Lasso for All Interactive Effects"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vQvJnv-v__ki"
      },
      "source": [
        "\\We use \"plug-in\" tuning with a theoretically valid choice of penalty $\\lambda = 2 \\cdot c \\hat{\\sigma}  \\Phi^{-1}(1-\\alpha/2p)/\\sqrt{n}$, where $c>1$ and $1-\\alpha$ is a confidence level, and $\\Phi^{-1}$ denotes the quantile function. Under homoskedasticity, this choice ensures that the Lasso predictor is well behaved, delivering good predictive performance under approximate sparsity. In practice, this formula will work well even in the absence of homoskedasticity, especially when the random variables $\\epsilon$ and $X$ in the regression equation decay quickly at the tails.\n",
        "\n",
        "In practice, many people choose to use cross-validation, which is perfectly fine for predictive tasks. However, when conducting inference, to make our analysis valid we will require cross-fitting in addition to cross-validation. As we have not yet discussed cross-fitting, we rely on this theoretically-driven penalty in order to allow for accurate inference in the upcoming notebooks.\n",
        "\n",
        "Here, we use a convenient, conservative bound $\\hat{\\sigma} = \\sqrt{Var{Y}}$. The iterative estimation of $\\hat{\\sigma}$ is provided by RLasso (see end of the notebook).\n",
        "\n",
        "Note: In the book, we multiply instead of divide by $\\sqrt{n}$. This is because there, Lasso minimizes the sum of errors, versus sklearn's Lasso whose objective minimizes the average errors."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a = 0.05\n",
        "const = 1.1"
      ],
      "metadata": {
        "id": "G8GUa_DW8Xxk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For each target predictive effect estimate it via the partialling out process and calculate the quantities needed for the covariance calculation, which is the residual outcome, the residual target variable and the final stage residual epsilon."
      ],
      "metadata": {
        "id": "0fDhBfmBZbCG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "alpha = {}\n",
        "res_y, res_D, epsilon = {}, {}, {}\n",
        "for c in interaction_cols:\n",
        "    print(f\"Double Lasso for target variable {c}\")\n",
        "    D = X[c].values\n",
        "    W = X.drop([c], axis=1)\n",
        "\n",
        "    # Do the lasso penalty here\n",
        "    hatsigma = np.std(y)\n",
        "    lmbda_theory = 2*const*hatsigma*norm.ppf(1-a/(2*X.shape[1]))/np.sqrt(X.shape[0])\n",
        "    lasso_model = lambda: make_pipeline(StandardScaler(), Lasso(alpha=lmbda_theory))\n",
        "    res_y[c] = y - lasso_model().fit(W, y).predict(W)\n",
        "\n",
        "    # Do the lasso penalty here with Var(D)\n",
        "    hatsigma = np.std(D)\n",
        "    lmbda_theory = 2*const*hatsigma*norm.ppf(1-a/(2*X.shape[1]))/np.sqrt(X.shape[0])\n",
        "    lasso_model = lambda: make_pipeline(StandardScaler(), Lasso(alpha=lmbda_theory))\n",
        "    res_D[c] = D - lasso_model().fit(W, D).predict(W)\n",
        "\n",
        "    # Last Stage\n",
        "    final = LinearRegression(fit_intercept=False).fit(res_D[c].reshape(-1, 1), res_y[c])\n",
        "    epsilon[c] = res_y[c] - final.predict(res_D[c].reshape(-1, 1))\n",
        "    alpha[c] = [final.coef_[0]]"
      ],
      "metadata": {
        "id": "OAR5GRxWAW56"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the covariance matrix of the estimated parameters\n",
        "V = np.zeros((len(interaction_cols), len(interaction_cols)))\n",
        "for it, c in enumerate(interaction_cols):\n",
        "    Jc = np.mean(res_D[c]**2)\n",
        "    for itp, cp in enumerate(interaction_cols):\n",
        "        Jcp = np.mean(res_D[cp]**2)\n",
        "        Sigma = np.mean(res_D[c] * epsilon[c] * epsilon[cp] * res_D[cp])\n",
        "        V[it, itp] = Sigma / (Jc * Jcp)\n",
        "\n",
        "# Calculate standard errors for each parameter\n",
        "n = X.shape[0]\n",
        "for it, c in enumerate(interaction_cols):\n",
        "    alpha[c] += [np.sqrt(V[it, it] / n)]\n",
        "\n",
        "# put all in a dataframe\n",
        "df = pd.DataFrame.from_dict(alpha, orient='index', columns=['point', 'stderr'])\n",
        "\n",
        "# Calculate and Pointwise p-value\n",
        "summary = pd.DataFrame()\n",
        "summary['Estimate'] = df['point']\n",
        "summary['Std. Error'] = df['stderr']\n",
        "summary['p-value'] = norm.sf(np.abs(df['point'] / df['stderr']), loc=0, scale=1) * 2\n",
        "summary['ci_lower'] = df['point'] - 1.96 * df['stderr']\n",
        "summary['ci_upper'] = df['point'] + 1.96 * df['stderr']\n",
        "summary"
      ],
      "metadata": {
        "id": "qEK-SztG-H3P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Joint Intervals\n",
        "Drootinv = np.diagflat(1/np.sqrt(np.diag(V)))\n",
        "scaledCov = Drootinv @ V @ Drootinv\n",
        "np.random.seed(123)\n",
        "U = np.random.multivariate_normal(np.zeros(scaledCov.shape[0]), scaledCov, size=10000)\n",
        "z = np.max(np.abs(U), axis=1)\n",
        "c = np.percentile(z, 95)\n",
        "summary = pd.DataFrame()\n",
        "summary['Estimate'] = df['point']\n",
        "summary['CI lower'] = df['point'] - c * df['stderr']\n",
        "summary['CI upper'] = df['point'] + c * df['stderr']\n",
        "summary"
      ],
      "metadata": {
        "id": "ewxmF-1k-H53"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6eV_tNex__km"
      },
      "source": [
        "We can also set the penalized lasso model to be estimated based on the theoretically motivated penalty level using the hdmpy package. To install it run\n",
        "```\n",
        "!pip install multiprocess\n",
        "!git clone https://github.com/maxhuppertz/hdmpy.git\n",
        "```\n",
        "\n",
        "You can run the cells below and then repeat the whole analysis above using the newly defined `lasso_model` variable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "acQhFTC0__km"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "sys.path.insert(1, \"./hdmpy\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "awA9uVQc__km"
      },
      "outputs": [],
      "source": [
        "# We wrap the package so that it has the familiar sklearn API\n",
        "import hdmpy\n",
        "from sklearn.base import BaseEstimator, clone\n",
        "\n",
        "class RLasso(BaseEstimator):\n",
        "\n",
        "    def __init__(self, *, post=True):\n",
        "        self.post = post\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        self.rlasso_ = hdmpy.rlasso(X, y, post=self.post)\n",
        "        return self\n",
        "\n",
        "    def predict(self, X):\n",
        "        return np.array(X) @ np.array(self.rlasso_.est['beta']).flatten() + np.array(self.rlasso_.est['intercept'])\n",
        "\n",
        "lasso_model = lambda: RLasso(post=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OBmzPIRZ__ks"
      },
      "outputs": [],
      "source": [
        "alpha = {}\n",
        "res_y, res_D, epsilon = {}, {}, {}\n",
        "for c in interaction_cols:\n",
        "    print(f\"Double Lasso for target variable {c}\")\n",
        "    D = X[c].values\n",
        "    W = X.drop([c], axis=1)\n",
        "    res_y[c] = y - lasso_model().fit(W, y).predict(W)\n",
        "    res_D[c] = D - lasso_model().fit(W, D).predict(W)\n",
        "    final = LinearRegression(fit_intercept=False).fit(res_D[c].reshape(-1, 1), res_y[c])\n",
        "    epsilon[c] = res_y[c] - final.predict(res_D[c].reshape(-1, 1))\n",
        "    alpha[c] = [final.coef_[0]]\n",
        "\n",
        "# Calculate the covariance matrix of the estimated parameters\n",
        "V = np.zeros((len(interaction_cols), len(interaction_cols)))\n",
        "for it, c in enumerate(interaction_cols):\n",
        "    Jc = np.mean(res_D[c]**2)\n",
        "    for itp, cp in enumerate(interaction_cols):\n",
        "        Jcp = np.mean(res_D[cp]**2)\n",
        "        Sigma = np.mean(res_D[c] * epsilon[c] * epsilon[cp] * res_D[cp])\n",
        "        V[it, itp] = Sigma / (Jc * Jcp)\n",
        "\n",
        "# Calculate standard errors for each parameter\n",
        "n = X.shape[0]\n",
        "for it, c in enumerate(interaction_cols):\n",
        "    alpha[c] += [np.sqrt(V[it, it] / n)]\n",
        "\n",
        "# put all in a dataframe\n",
        "df = pd.DataFrame.from_dict(alpha, orient='index', columns=['point', 'stderr'])\n",
        "\n",
        "# Calculate and pointwise p-value\n",
        "summary = pd.DataFrame()\n",
        "summary['Estimate'] = df['point']\n",
        "summary['Std. Error'] = df['stderr']\n",
        "summary['p-value'] = norm.sf(np.abs(df['point'] / df['stderr']), loc=0, scale=1) * 2\n",
        "summary['ci_lower'] = df['point'] - 1.96 * df['stderr']\n",
        "summary['ci_upper'] = df['point'] + 1.96 * df['stderr']\n",
        "summary"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Joint Confidence Intervals"
      ],
      "metadata": {
        "id": "DcW2A3JTMBiv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Drootinv = np.diagflat(1/np.sqrt(np.diag(V)))\n",
        "scaledCov = Drootinv @ V @ Drootinv\n",
        "np.random.seed(123)\n",
        "U = np.random.multivariate_normal(np.zeros(scaledCov.shape[0]), scaledCov, size=10000)\n",
        "z = np.max(np.abs(U), axis=1)\n",
        "c = np.percentile(z, 95)\n",
        "c\n",
        "summary = pd.DataFrame()\n",
        "summary['Estimate'] = df['point']\n",
        "summary['CI lower'] = df['point'] - c * df['stderr']\n",
        "summary['CI upper'] = df['point'] + c * df['stderr']\n",
        "summary"
      ],
      "metadata": {
        "id": "IPClP_jTeZlr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In lieu of using $Var[Y]$ and $Var[D]$ as conservative bounds, we can also get close to the rlasso if we pick estimate $\\hat{\\sigma}$ via iterative regularization based on CV. Although CV itself is not valid for inference, we can use it to estimate $\\hat{\\sigma}$ as LassoCV is consistent.\n"
      ],
      "metadata": {
        "id": "_O3eOCkqJTI6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cv = KFold(n_splits=5, shuffle=True, random_state=123)\n",
        "tmp = lambda: make_pipeline(StandardScaler(), LassoCV(cv=cv))"
      ],
      "metadata": {
        "id": "EeUMCAS1J7Ek"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "alpha = {}\n",
        "res_y, res_D, res_y_tmp, res_D_tmp, epsilon = {}, {}, {}, {}, {}\n",
        "for c in interaction_cols:\n",
        "    print(f\"Double Lasso for target variable {c}\")\n",
        "    D = X[c].values\n",
        "    W = X.drop([c], axis=1)\n",
        "    res_y_tmp[c] = y - tmp().fit(W, y).predict(W)\n",
        "    res_D_tmp[c] = D - tmp().fit(W, D).predict(W)\n",
        "\n",
        "    # Do the lasso penalty here\n",
        "    hatsigma = np.std(res_y_tmp[c])\n",
        "    lmbda_theory = 2*const*hatsigma*norm.ppf(1-a/(2*X.shape[1]))/np.sqrt(X.shape[0])\n",
        "    lasso_model = lambda: make_pipeline(StandardScaler(), Lasso(alpha=lmbda_theory))\n",
        "    res_y[c] = y - lasso_model().fit(W, y).predict(W)\n",
        "\n",
        "    # Do the lasso penalty here with Var(D)\n",
        "    hatsigma = np.std(res_D_tmp[c])\n",
        "    lmbda_theory = 2*const*hatsigma*norm.ppf(1-a/(2*X.shape[1]))/np.sqrt(X.shape[0])\n",
        "    lasso_model = lambda: make_pipeline(StandardScaler(), Lasso(alpha=lmbda_theory))\n",
        "    res_D[c] = D - lasso_model().fit(W, D).predict(W)\n",
        "\n",
        "    # final stage\n",
        "    final = LinearRegression(fit_intercept=False).fit(res_D[c].reshape(-1, 1), res_y[c])\n",
        "    epsilon[c] = res_y[c] - final.predict(res_D[c].reshape(-1, 1))\n",
        "    alpha[c] = [final.coef_[0]]"
      ],
      "metadata": {
        "id": "GimwIdl3q5fw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the covariance matrix of the estimated parameters\n",
        "V = np.zeros((len(interaction_cols), len(interaction_cols)))\n",
        "for it, c in enumerate(interaction_cols):\n",
        "    Jc = np.mean(res_D[c]**2)\n",
        "    for itp, cp in enumerate(interaction_cols):\n",
        "        Jcp = np.mean(res_D[cp]**2)\n",
        "        Sigma = np.mean(res_D[c] * epsilon[c] * epsilon[cp] * res_D[cp])\n",
        "        V[it, itp] = Sigma / (Jc * Jcp)\n",
        "\n",
        "# Calculate standard errors for each parameter\n",
        "n = X.shape[0]\n",
        "for it, c in enumerate(interaction_cols):\n",
        "    alpha[c] += [np.sqrt(V[it, it] / n)]\n",
        "\n",
        "# put all in a dataframe\n",
        "df = pd.DataFrame.from_dict(alpha, orient='index', columns=['point', 'stderr'])\n",
        "\n",
        "# Calculate and Pointwise p-value\n",
        "summary = pd.DataFrame()\n",
        "summary['Estimate'] = df['point']\n",
        "summary['Std. Error'] = df['stderr']\n",
        "summary['p-value'] = norm.sf(np.abs(df['point'] / df['stderr']), loc=0, scale=1) * 2\n",
        "summary['ci_lower'] = df['point'] - 1.96 * df['stderr']\n",
        "summary['ci_upper'] = df['point'] + 1.96 * df['stderr']\n",
        "summary"
      ],
      "metadata": {
        "id": "AqW70Vj7NYkW"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "papermill": {
      "default_parameters": {},
      "duration": 89.365707,
      "end_time": "2021-02-28T17:18:51.003711",
      "environment_variables": {},
      "exception": null,
      "input_path": "__notebook__.ipynb",
      "output_path": "__notebook__.ipynb",
      "parameters": {},
      "start_time": "2021-02-28T17:17:21.638004",
      "version": "2.2.2"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}