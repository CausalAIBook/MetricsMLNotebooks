{"cells":[{"cell_type":"markdown","id":"f02fa044","metadata":{"papermill":{"duration":0.012988,"end_time":"2022-04-19T09:06:48.772902","exception":false,"start_time":"2022-04-19T09:06:48.759914","status":"completed"},"tags":[],"id":"f02fa044"},"source":["# Inference on Predictive and Causal Effects in High-Dimensional Nonlinear Models"]},{"cell_type":"markdown","id":"23154404","metadata":{"papermill":{"duration":0.009437,"end_time":"2022-04-19T09:06:48.791895","exception":false,"start_time":"2022-04-19T09:06:48.782458","status":"completed"},"tags":[],"id":"23154404"},"source":["## Impact of 401(k) on  Financial Wealth\n","\n","As a practical illustration of the methods developed in this lecture, we consider estimation of the effect of 401(k) eligibility and participation\n","on accumulated assets. 401(k) plans are pension accounts sponsored by employers. The key problem in determining the effect of participation in 401(k) plans on accumulated assets is saver heterogeneity coupled with the fact that the decision to enroll in a 401(k) is non-random. It is generally recognized that some people have a higher preference for saving than others. It also seems likely that those individuals with high unobserved preference for saving would be most likely to choose to participate in tax-advantaged retirement savings plans and would tend to have otherwise high amounts of accumulated assets. The presence of unobserved savings preferences with these properties then implies that conventional estimates that do not account for saver heterogeneity and endogeneity of participation will be biased upward, tending to overstate the savings effects of 401(k) participation.\n","\n","One can argue that eligibility for enrolling in a 401(k) plan in this data can be taken as exogenous after conditioning on a few observables of which the most important for their argument is income. The basic idea is that, at least around the time 401(k)â€™s initially became available, people were unlikely to be basing their employment decisions on whether an employer offered a 401(k) but would instead focus on income and other aspects of the job."]},{"cell_type":"code","source":["install.packages(\"xtable\")\n","install.packages(\"hdm\")\n","install.packages(\"sandwich\")\n","install.packages(\"ggplot2\")\n","install.packages(\"randomForest\")\n","install.packages(\"glmnet\")\n","install.packages(\"rpart\")\n","install.packages(\"gbm\")\n","\n","library(xtable)\n","library(hdm)\n","library(sandwich)\n","library(ggplot2)\n","library(randomForest)\n","library(data.table)\n","library(glmnet)\n","library(rpart)\n","library(gbm)\n","\n","set.seed(123)"],"metadata":{"id":"KmAkbDiVE7wm"},"id":"KmAkbDiVE7wm","execution_count":null,"outputs":[]},{"cell_type":"markdown","id":"7e23cba0","metadata":{"papermill":{"duration":0.009588,"end_time":"2022-04-19T09:06:48.810853","exception":false,"start_time":"2022-04-19T09:06:48.801265","status":"completed"},"tags":[],"id":"7e23cba0"},"source":["### Data\n","\n","The raw dataset can be found [here](https://raw.githubusercontent.com/CausalAIBook/MetricsMLNotebooks/main/data/401k.csv).\n","The data set can be loaded from the `hdm` package for R directly by typing:\n","\n"]},{"cell_type":"code","execution_count":null,"id":"c442abdc","metadata":{"execution":{"iopub.execute_input":"2022-04-19T09:06:48.833250Z","iopub.status.busy":"2022-04-19T09:06:48.831101Z","iopub.status.idle":"2022-04-19T09:06:49.281559Z","shell.execute_reply":"2022-04-19T09:06:49.279778Z"},"papermill":{"duration":0.46397,"end_time":"2022-04-19T09:06:49.283933","exception":false,"start_time":"2022-04-19T09:06:48.819963","status":"completed"},"tags":[],"id":"c442abdc"},"outputs":[],"source":["data(pension)\n","data <- pension\n","dim(data)"]},{"cell_type":"markdown","id":"e47fa9d3","metadata":{"papermill":{"duration":0.009462,"end_time":"2022-04-19T09:06:49.302928","exception":false,"start_time":"2022-04-19T09:06:49.293466","status":"completed"},"tags":[],"id":"e47fa9d3"},"source":["See the \"Details\" section on the description of the data set, which can be accessed by\n"]},{"cell_type":"code","execution_count":null,"id":"00e04b82","metadata":{"execution":{"iopub.execute_input":"2022-04-19T09:06:49.394579Z","iopub.status.busy":"2022-04-19T09:06:49.323826Z","iopub.status.idle":"2022-04-19T09:06:49.662556Z","shell.execute_reply":"2022-04-19T09:06:49.660433Z"},"papermill":{"duration":0.35227,"end_time":"2022-04-19T09:06:49.664810","exception":false,"start_time":"2022-04-19T09:06:49.312540","status":"completed"},"tags":[],"id":"00e04b82"},"outputs":[],"source":["help(pension)"]},{"cell_type":"markdown","id":"24b41e4a","metadata":{"papermill":{"duration":0.009357,"end_time":"2022-04-19T09:06:49.683784","exception":false,"start_time":"2022-04-19T09:06:49.674427","status":"completed"},"tags":[],"id":"24b41e4a"},"source":["The data consist of 9,915 observations at the household level drawn from the 1991 Survey of Income and Program Participation (SIPP).  All the variables are referred to 1990. We use net financial assets (*net\\_tfa*) as the outcome variable, $Y$,  in our analysis. The net financial assets are computed as the sum of IRA balances, 401(k) balances, checking accounts, saving bonds, other interest-earning accounts, other interest-earning assets, stocks, and mutual funds less non mortgage debts."]},{"cell_type":"markdown","id":"ed9d4e82","metadata":{"papermill":{"duration":0.009242,"end_time":"2022-04-19T09:06:49.702401","exception":false,"start_time":"2022-04-19T09:06:49.693159","status":"completed"},"tags":[],"id":"ed9d4e82"},"source":["Among the $9915$ individuals, $3682$ are eligible to participate in the program. The variable *e401* indicates eligibility and *p401* indicates participation, respectively."]},{"cell_type":"code","execution_count":null,"id":"63519184","metadata":{"execution":{"iopub.execute_input":"2022-04-19T09:06:49.724951Z","iopub.status.busy":"2022-04-19T09:06:49.723401Z","iopub.status.idle":"2022-04-19T09:06:50.327963Z","shell.execute_reply":"2022-04-19T09:06:50.326306Z"},"papermill":{"duration":0.618528,"end_time":"2022-04-19T09:06:50.330218","exception":false,"start_time":"2022-04-19T09:06:49.711690","status":"completed"},"tags":[],"id":"63519184"},"outputs":[],"source":["hist_e401 = ggplot(data, aes(x = e401, fill = factor(e401))) + geom_bar()\n","hist_e401"]},{"cell_type":"markdown","id":"823d2628","metadata":{"papermill":{"duration":0.009686,"end_time":"2022-04-19T09:06:50.349766","exception":false,"start_time":"2022-04-19T09:06:50.340080","status":"completed"},"tags":[],"id":"823d2628"},"source":["Eligibility is highly associated with financial wealth:"]},{"cell_type":"code","execution_count":null,"id":"5d8faf9c","metadata":{"execution":{"iopub.execute_input":"2022-04-19T09:06:50.372330Z","iopub.status.busy":"2022-04-19T09:06:50.370847Z","iopub.status.idle":"2022-04-19T09:06:50.912011Z","shell.execute_reply":"2022-04-19T09:06:50.910336Z"},"papermill":{"duration":0.554613,"end_time":"2022-04-19T09:06:50.914133","exception":false,"start_time":"2022-04-19T09:06:50.359520","status":"completed"},"tags":[],"id":"5d8faf9c"},"outputs":[],"source":["dens_net_tfa = ggplot(data, aes(x = net_tfa, color = factor(e401), fill = factor(e401)) ) +\n","                    geom_density() + xlim(c(-20000, 150000)) +\n","                    facet_wrap(.~e401)\n","\n","dens_net_tfa"]},{"cell_type":"markdown","id":"0f4f86a7","metadata":{"papermill":{"duration":0.010335,"end_time":"2022-04-19T09:06:50.935024","exception":false,"start_time":"2022-04-19T09:06:50.924689","status":"completed"},"tags":[],"id":"0f4f86a7"},"source":["The unconditional APE of e401 is about $19559$:"]},{"cell_type":"code","execution_count":null,"id":"836c6af7","metadata":{"execution":{"iopub.execute_input":"2022-04-19T09:06:50.959110Z","iopub.status.busy":"2022-04-19T09:06:50.957519Z","iopub.status.idle":"2022-04-19T09:06:50.981194Z","shell.execute_reply":"2022-04-19T09:06:50.979530Z"},"papermill":{"duration":0.038096,"end_time":"2022-04-19T09:06:50.983602","exception":false,"start_time":"2022-04-19T09:06:50.945506","status":"completed"},"tags":[],"id":"836c6af7"},"outputs":[],"source":["e1 <- data[data$e401==1,]\n","e0 <- data[data$e401==0,]\n","round(mean(e1$net_tfa)-mean(e0$net_tfa),0)"]},{"cell_type":"markdown","id":"22b09926","metadata":{"papermill":{"duration":0.01047,"end_time":"2022-04-19T09:06:51.004618","exception":false,"start_time":"2022-04-19T09:06:50.994148","status":"completed"},"tags":[],"id":"22b09926"},"source":["Among the $3682$ individuals that  are eligible, $2594$ decided to participate in the program. The unconditional APE of p401 is about $27372$:"]},{"cell_type":"code","execution_count":null,"id":"e78aaa58","metadata":{"execution":{"iopub.execute_input":"2022-04-19T09:06:51.029140Z","iopub.status.busy":"2022-04-19T09:06:51.027462Z","iopub.status.idle":"2022-04-19T09:06:51.052361Z","shell.execute_reply":"2022-04-19T09:06:51.050591Z"},"papermill":{"duration":0.039305,"end_time":"2022-04-19T09:06:51.054616","exception":false,"start_time":"2022-04-19T09:06:51.015311","status":"completed"},"tags":[],"id":"e78aaa58"},"outputs":[],"source":["p1 <- data[data$p401==1,]\n","p0 <- data[data$p401==0,]\n","round(mean(p1$net_tfa)-mean(p0$net_tfa),0)"]},{"cell_type":"markdown","id":"e0af3c81","metadata":{"papermill":{"duration":0.010831,"end_time":"2022-04-19T09:06:51.076114","exception":false,"start_time":"2022-04-19T09:06:51.065283","status":"completed"},"tags":[],"id":"e0af3c81"},"source":["As discussed, these estimates are biased since they do not account for saver heterogeneity and endogeneity of participation."]},{"cell_type":"code","source":["# instrument variable\n","Z <- data[,'e401']\n","# treatment variable\n","D <- data[, 'p401']\n","# outcome variable\n","y <- data[,'net_tfa']"],"metadata":{"id":"A03YWrvUW0Sm"},"id":"A03YWrvUW0Sm","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### We construct the engineered features for controls"],"metadata":{"id":"RVUbOMRRWwBm"},"id":"RVUbOMRRWwBm"},{"cell_type":"code","source":["# Constructing the controls\n","X_formula = \"~ poly(age, 6, raw=TRUE) + poly(inc, 8, raw=TRUE) + poly(educ, 4, raw=TRUE) + poly(fsize, 2, raw=TRUE) + male + marr + twoearn + db + pira + hown\"\n","X = as.data.table(model.frame(X_formula, pension))\n","head(X)"],"metadata":{"id":"7vt1hbdBG8cb"},"id":"7vt1hbdBG8cb","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Effect of Eligibility on Financial Assets"],"metadata":{"id":"yYm0gTlsN5a9"},"id":"yYm0gTlsN5a9"},{"cell_type":"code","source":["set.seed(1)\n","yfit.lasso.cv   <- cv.glmnet(as.matrix(X), y, family=\"gaussian\", alpha=1)  # family gaussian means that we'll be using square loss\n","yhat.lasso.cv   <- predict(yfit.lasso.cv, newx = as.matrix(X))       # predictions\n","Zfit.lasso.cv   <- cv.glmnet(as.matrix(X), Z, family=\"gaussian\", alpha=1)  # family gaussian means that we'll be using square loss\n","Zhat.lasso.cv   <- predict(Zfit.lasso.cv, newx = as.matrix(X))       # predictions\n","\n","resy <- y-yhat.lasso.cv\n","resZ <- Z-Zhat.lasso.cv\n","\n","# Estimate\n","mean(resy * resZ) / mean(resZ^2)"],"metadata":{"id":"rtKcHfVHN2UV"},"id":"rtKcHfVHN2UV","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Recall if we want to do inference, we need to either use the theoretically driven penalty paramter for Lasso or perform cross-fitting."],"metadata":{"id":"MsJTCA26ZQnG"},"id":"MsJTCA26ZQnG"},{"cell_type":"markdown","source":["# Instrumental Variables: Effect of 401k Participation on Financial Assets"],"metadata":{"id":"yzNigd7YYVuA"},"id":"yzNigd7YYVuA"},{"cell_type":"markdown","source":["## Double ML IV under Partial Linearity"],"metadata":{"id":"FI2u5KU7YWIF"},"id":"FI2u5KU7YWIF"},{"cell_type":"markdown","source":["Now, we consider estimation of average treatment effects of participation in 401k, i.e. `p401`, with the binary instrument being eligibility in 401k, i.e. `e401`. As before, $Y$ denotes the outcome `net_tfa`, and $X$ is the vector of covariates. We consider a partially linear structural equation model:\n","\\begin{eqnarray*}\n","Y & := & g_Y(\\epsilon_Y) D + f_Y(A, X, \\epsilon_Y),  \\\\\n","D & := & f_D(Z, X, A, \\epsilon_D), \\\\\n","Z & := & f_Z(X, \\epsilon_Z),\\\\\n","A & : =  & f_A(X, \\epsilon_A), \\\\\n","X & := &  \\epsilon_X,\n","\\end{eqnarray*}\n","where $A$ is a vector of un-observed confounders.\n","\n","Under this structural equation model, the average treatment effect:\n","\\begin{align}\n","\\alpha = E[Y(1) - Y(0)]\n","\\end{align}\n","can be identified by the moment restriction:\n","\\begin{align}\n","E[(\\tilde{Y} - \\alpha \\tilde{D}) \\tilde{Z}] = 0\n","\\end{align}\n","where for any variable $V$, we denote with $\\tilde{V} = V - E[V|X]$."],"metadata":{"id":"uhDK6Em_YWSm"},"id":"uhDK6Em_YWSm"},{"cell_type":"code","source":["set.seed(1)\n","yfit.lasso.cv   <- cv.glmnet(as.matrix(X), y, family=\"gaussian\", alpha=1)  # family gaussian means that we'll be using square loss\n","Dfit.lasso.cv   <- cv.glmnet(as.matrix(X), D, family=\"gaussian\", alpha=1)  # family gaussian means that we'll be using square loss\n","Zfit.lasso.cv   <- cv.glmnet(as.matrix(X), Z, family=\"gaussian\", alpha=1)  # family gaussian means that we'll be using square loss\n","\n","\n","yhat.lasso.cv   <- predict(yfit.lasso.cv, newx = as.matrix(X))       # predictions\n","Dhat.lasso.cv   <- predict(Dfit.lasso.cv, newx = as.matrix(X))       # predictions\n","Zhat.lasso.cv   <- predict(Zfit.lasso.cv, newx = as.matrix(X))       # predictions\n","\n","resy <- y-yhat.lasso.cv\n","resD <- D-Dhat.lasso.cv\n","resZ <- Z-Zhat.lasso.cv\n","\n","# Estimate\n","mean(resy * resZ) / mean(resZ*resD)"],"metadata":{"id":"bdUGB53AYf3S"},"id":"bdUGB53AYf3S","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Recall if we want to do inference, we need to either use the theoretically driven penalty paramter for Lasso or perform cross-fitting."],"metadata":{"id":"Fw1ZxeBKZcRm"},"id":"Fw1ZxeBKZcRm"},{"cell_type":"markdown","source":["### DML with Non-Linear ML Models and Cross-fitting"],"metadata":{"id":"jnBOtXXuZnkz"},"id":"jnBOtXXuZnkz"},{"cell_type":"code","source":["# DML for PLIVM with D and Z as classifiers or regressors\n","DML2.for.PLIVM <- function(x, d, z, y, dreg, yreg, zreg, nfold=5, method=\"regression\") {\n","  nobs <- nrow(x)\n","  foldid <- rep.int(1:nfold,times = ceiling(nobs/nfold))[sample.int(nobs)]\n","  I <- split(1:nobs, foldid)\n","  # create residualized objects to fill\n","  ytil <- dtil <- ztil<- rep(NA, nobs)\n","  # obtain cross-fitted residuals\n","  cat(\"fold: \")\n","  for(b in 1:length(I)){\n","    if (method == \"randomforest\"){\n","        # take a fold out\n","        dfit <- dreg(x[-I[[b]],], d[-I[[b]]])\n","        zfit <- zreg(x[-I[[b]],], z[-I[[b]]])\n","        yfit <- yreg(x[-I[[b]],], y[-I[[b]]])\n","        # predict the fold out\n","        dhat <- predict(dfit, x[I[[b]],], type=\"prob\")[,2]  # type = \"prob\" is like predict_proba in scikitlearn\n","        zhat <- predict(zfit, x[I[[b]],], type=\"prob\")[,2]\n","        yhat <- predict(yfit, x[I[[b]],])  # default type = \"response\" for regression for RF, type = \"vector\" for regression for Decision Trees\n","        # record residual\n","        dtil[I[[b]]] <- (as.numeric(d[I[[b]]])-1 - dhat) # as.numeric will turn d = as.factor(d) from 0,1 to 1,2 so subtract 1!\n","        ztil[I[[b]]] <- (as.numeric(z[I[[b]]])-1 - zhat)\n","        ytil[I[[b]]] <- (y[I[[b]]] - yhat)\n","    } else if (method == \"regression\") { # works for both boosted trees and glmnet\n","        # take a fold out\n","        dfit <- dreg(x[-I[[b]],], d[-I[[b]]])\n","        zfit <- zreg(x[-I[[b]],], z[-I[[b]]])\n","        yfit <- yreg(x[-I[[b]],], y[-I[[b]]])\n","        # predict the fold out\n","        dhat <- predict(dfit, x[I[[b]],], type=\"response\")\n","        zhat <- predict(zfit, x[I[[b]],], type=\"response\")\n","        yhat <- predict(yfit, x[I[[b]],], type=\"response\")\n","        # record residual\n","        dtil[I[[b]]] <- (d[I[[b]]] - dhat)\n","        ztil[I[[b]]] <- (z[I[[b]]] - zhat)\n","        ytil[I[[b]]] <- (y[I[[b]]] - yhat)\n","    } else if (method == \"decisiontrees\"){\n","        # take a fold out\n","        dfit <- dreg(x[-I[[b]],], as.factor(d)[-I[[b]]])\n","        zfit <- zreg(x[-I[[b]],], as.factor(z)[-I[[b]]])\n","        yfit <- yreg(x[-I[[b]],], y[-I[[b]]])\n","        # predict the fold out\n","        dhat <- predict(dfit, x[I[[b]],])[,2]\n","        zhat <- predict(zfit, x[I[[b]],])[,2]\n","        yhat <- predict(yfit, x[I[[b]],])\n","        # record residual\n","        dtil[I[[b]]] <- (d[I[[b]]] - dhat)\n","        ztil[I[[b]]] <- (z[I[[b]]] - zhat)\n","        ytil[I[[b]]] <- (y[I[[b]]] - yhat)\n","    }\n","\n","    cat(b,\" \")\n","  }\n","  ivfit = tsls(y=ytil,d=dtil, x=NULL, z=ztil, intercept=FALSE)\n","  coef.est <- ivfit$coef          #extract coefficient\n","  se <- ivfit$se                  #record standard error\n","  cat(sprintf(\"\\ncoef (se) = %g (%g)\\n\", coef.est , se))\n","\n","  return( list(coef.est=coef.est, se=se, dtil=dtil, ytil=ytil, ztil=ztil) )\n","}"],"metadata":{"id":"K_vQlMYmz91I"},"id":"K_vQlMYmz91I","execution_count":null,"outputs":[]},{"cell_type":"code","source":["summary <- function(point, stderr, resy, resD, resZ, name) {\n","  data <- data.frame(\n","    estimate = point, # point estimate\n","    stderr = stderr, # standard error\n","    lower = point - 1.96 * stderr, # lower end of 95% confidence interval\n","    upper = point + 1.96 * stderr, # upper end of 95% confidence interval\n","    `rmse y` = sqrt(mean(resy^2)), # RMSE of model that predicts outcome y\n","    `rmse D` = sqrt(mean(resD^2)), # RMSE of model that predicts treatment D\n","    `rmse Z` = sqrt(mean(resZ^2)), # RMSE of model that predicts treatment D\n","    `accuracy D` = mean(abs(resD) < 0.5), # binary classification accuracy of model for D\n","    `accuracy Z` = mean(abs(resZ) < 0.5) # binary classification accuracy of model for Z\n","  )\n","  rownames(data) <- name\n","  return(data)\n","}"],"metadata":{"id":"puSCLNvofQxA"},"id":"puSCLNvofQxA","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Double Lasso with Cross-Fitting"],"metadata":{"id":"1Z5vrvrlbuPj"},"id":"1Z5vrvrlbuPj"},{"cell_type":"code","source":["# DML with LassoCV\n","set.seed(123)\n","cat(sprintf(\"\\nDML with Lasso CV \\n\"))\n","\n","dreg.lasso.cv <- function(x,d){ cv.glmnet(x, d, family=\"gaussian\", alpha=1, nfolds=5)}\n","yreg.lasso.cv <- function(x,y){ cv.glmnet(x, y, family=\"gaussian\", alpha=1, nfolds=5)}\n","zreg.lasso.cv <- function(x,z){ cv.glmnet(x, z, family=\"gaussian\", alpha=1, nfolds=5)}\n","\n","DML2.results <- DML2.for.PLIVM(as.matrix(X), D, Z, y, dreg.lasso.cv, yreg.lasso.cv, zreg.lasso.cv, nfold=3, method=\"regression\")\n","sum.lasso.cv <- summary(DML2.results$coef.est, DML2.results$se, DML2.results$ytil, DML2.results$dtil, DML2.results$ztil, name = 'LassoCV')\n","tableplr <- data.frame()\n","tableplr <- rbind(sum.lasso.cv)\n","tableplr"],"metadata":{"id":"vBJm7BkUYgsG"},"id":"vBJm7BkUYgsG","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Using a $\\ell_2$ Penalized Logistic Regression for D and Z"],"metadata":{"id":"pyrem2YniNls"},"id":"pyrem2YniNls"},{"cell_type":"code","source":["# DML with Lasso/Ridge\n","set.seed(123)\n","cat(sprintf(\"\\nDML with Lasso/Logistic \\n\"))\n","\n","dreg.lasso.cv <- function(x,d){cv.glmnet(x, d, family=\"binomial\", alpha=0, nfolds=5)}\n","yreg.ridge.cv <- function(x,y){cv.glmnet(x, y, family=\"gaussian\", alpha=1, nfolds=5)}\n","zreg.ridge.cv <- function(x,z){cv.glmnet(x, z, family=\"binomial\", alpha=0, nfolds=5)}\n","\n","DML2.results <- DML2.for.PLIVM(as.matrix(X), D, Z, y, dreg.lasso.cv, yreg.ridge.cv, zreg.ridge.cv, nfold=3, method=\"regression\")\n","sum.lasso_ridge.cv <- summary(DML2.results$coef.est, DML2.results$se, DML2.results$ytil, DML2.results$dtil, DML2.results$ztil, name = 'LassoCV/LogisticCV')\n","tableplr <- rbind(tableplr, sum.lasso_ridge.cv)\n","tableplr"],"metadata":{"id":"FM6WvQXKYgxL"},"id":"FM6WvQXKYgxL","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Random Forests"],"metadata":{"id":"yfTdX3__jcwI"},"id":"yfTdX3__jcwI"},{"cell_type":"code","source":["# DML with Random Forest\n","set.seed(123)\n","cat(sprintf(\"\\nDML with Random Forest \\n\"))\n","\n","dreg.rf <- function(x,d){randomForest(x, d, ntree=1000, nodesize=10)}  #ML method=Forest\n","yreg.rf <- function(x,y){randomForest(x, y, ntree=1000, nodesize=10)}  #ML method=Forest\n","zreg.rf <- function(x,z){randomForest(x, z, ntree=1000, nodesize=10)}  #ML method=Forest\n","\n","DML2.results = DML2.for.PLIVM(as.matrix(X), as.factor(D), as.factor(Z), y, dreg.rf, yreg.rf, zreg.rf, nfold=3, method=\"randomforest\")\n","sum.rf <- summary(DML2.results$coef.est, DML2.results$se, DML2.results$ytil, DML2.results$dtil, DML2.results$ztil, name = 'RF')\n","tableplr <- rbind(tableplr, sum.rf)\n","tableplr"],"metadata":{"id":"mMvJT6NZHW1_"},"id":"mMvJT6NZHW1_","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Decision Trees"],"metadata":{"id":"4I1oVQutjeqE"},"id":"4I1oVQutjeqE"},{"cell_type":"code","source":["# DML with Decision Trees\n","set.seed(123)\n","cat(sprintf(\"\\nDML with Decision Trees \\n\"))\n","\n","dreg.tr <- function(x,d){rpart(as.formula(\"D~.\"), cbind(data.frame(D=d),x), method = \"class\", minbucket=10, cp = 0.001)}\n","yreg.tr <- function(x,y){rpart(as.formula(\"y~.\"), cbind(data.frame(y=y),x), minbucket=10, cp = 0.001)}\n","zreg.tr <- function(x,z){rpart(as.formula(\"Z~.\"), cbind(data.frame(Z=z),x), method = \"class\", minbucket=10, cp = 0.001)}\n","\n","DML2.results = DML2.for.PLIVM(X, D, Z, y, dreg.tr, yreg.tr, zreg.tr, nfold=3, method=\"decisiontrees\") # decision tree takes in X as dataframe, not matrix/array\n","sum.tr <- summary(DML2.results$coef.est, DML2.results$se, DML2.results$ytil, DML2.results$dtil, DML2.results$ztil, name = 'Decision Trees')\n","tableplr <- rbind(tableplr, sum.tr)\n","tableplr"],"metadata":{"id":"ayrnTPeBHW88"},"id":"ayrnTPeBHW88","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Boosted Trees"],"metadata":{"id":"h7Jo_WXUjgjb"},"id":"h7Jo_WXUjgjb"},{"cell_type":"code","source":["# DML with Boosted Trees\n","set.seed(123)\n","cat(sprintf(\"\\nDML with Boosted Trees \\n\"))\n","\n","# NB: early stopping cannot easily be implemented with gbm\n","## set n.trees = best, where best <- gbm.perf(dreg.boost, plot.it = FALSE)\n","dreg.boost  <- function(x,d){gbm(as.formula(\"D~.\"), cbind(data.frame(D=d),x), distribution= \"bernoulli\", interaction.depth=2, n.trees=100, shrinkage=.1)}\n","yreg.boost  <- function(x,y){gbm(as.formula(\"y~.\"), cbind(data.frame(y=y),x), distribution= \"gaussian\", interaction.depth=2, n.trees=100, shrinkage=.1)}\n","zreg.boost  <- function(x,z){gbm(as.formula(\"Z~.\"), cbind(data.frame(Z=z),x), distribution= \"bernoulli\", interaction.depth=2, n.trees=100, shrinkage=.1)}\n","\n","# passing these through regression as type=\"response\", and D and Z should not be factors!\n","DML2.results = DML2.for.PLIVM(X, D, Z, y, dreg.boost, yreg.boost, zreg.boost, nfold=3, method = \"regression\")\n","sum.boost <- summary(DML2.results$coef.est, DML2.results$se, DML2.results$ytil, DML2.results$dtil, DML2.results$ztil, name = 'Boosted Trees')\n","tableplr <- rbind(tableplr, sum.boost)\n","tableplr"],"metadata":{"id":"nzlszy9zjiSy"},"id":"nzlszy9zjiSy","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Inference Robust to Weak Identification\n","\n","Now we turn toward robustness when the instrument is weak.\n","\n","Ideally, we would do (semi) cross-fitting with AutoML in order to find good first-stage models and re-run DML with these models. Unfortunately this is not easy to do in R. In the case of semi cross-fitting, we can use R's H20 AutoML trained on the entire training set $y\\sim X$, $D \\sim X$, $Z\\sim X$ to determine the best model (eg ensemble), but H20 does not allow you to extract the best model so we can re-use that in DML.\n","\n","Thus, in the below analysis of robust inference, we choose Boosted Trees as they perform well."],"metadata":{"id":"8OUusM2BpZH4"},"id":"8OUusM2BpZH4"},{"cell_type":"code","source":["robust_inference <- function(point, stderr, resD, resy, resZ, grid, alpha = 0.05) {\n","    # Inference in the partially linear IV model that is robust to weak identification.\n","    # grid: grid of theta values to search over when trying to identify the confidence region\n","    # alpha: confidence level\n","\n","    n <- dim(X)[1]\n","    thr <- qchisq(1 - alpha, df = 1)\n","    accept <- c()\n","\n","    for (theta in grid) {\n","        moment <- (resy - theta * resD) * resZ\n","        test <- n * mean(moment)^2 / var(moment)\n","        if (test <= thr) {\n","            accept <- c(accept, theta)\n","        }\n","    }\n","\n","    return(accept)\n","}\n"],"metadata":{"id":"UeNF5j1ApYYy"},"id":"UeNF5j1ApYYy","execution_count":null,"outputs":[]},{"cell_type":"code","source":["grid <- seq(0, 20000, length.out = 10000)\n","region <- robust_inference(DML2.results$coef.est, DML2.results$stderr, DML2.results$dtil, DML2.results$ytil, DML2.results$ztil, grid=grid)"],"metadata":{"id":"X21PuuUnsa25"},"id":"X21PuuUnsa25","execution_count":null,"outputs":[]},{"cell_type":"code","source":["grid <- seq(0, 20000, length.out = 10000)\n","region <- robust_inference(DML2.results$coef.est, DML2.results$stderr, DML2.results$dtil, DML2.results$ytil, DML2.results$ztil, grid=grid)# Calculate min and max\n","min_region <- min(region)\n","max_region <- max(region)\n","\n","print(min_region)\n","print(max_region)"],"metadata":{"id":"x-ZSzMkVqI45"},"id":"x-ZSzMkVqI45","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Interactive IV Model and LATE"],"metadata":{"id":"nKQGPfXWIKmh"},"id":"nKQGPfXWIKmh"},{"cell_type":"markdown","source":["Now, we consider estimation of local average treatment effects (LATE) of participation `p401`, with the binary instrument `e401`. As before, $Y$ denotes the outcome `net_tfa`, and $X$ is the vector of covariates.  Here the structural equation model is:\n","\\begin{eqnarray}\n","Y &:=&  f_Y (D, X, A, \\epsilon_Y) \\\\\n","D &:= & f_D(Z, X, A, \\epsilon_D) \\in \\{0,1\\},  \\\\\n","Z  &:= & f_Z(X,\\epsilon_Z) \\in \\{0,1\\},  \\\\\n","X &:=&  \\epsilon_X, \\quad A = \\epsilon_A,\n","\\end{eqnarray}\n","where $\\epsilon$'s are all exogenous and independent,\n","and\n","$$\n","z \\mapsto f_D(z , A, X, \\epsilon_D) \\text{ is weakly increasing (weakly monotone)}.\n","$$\n","and $A$ is a vector of unobserved confounders. Note that in our setting monotonicity is satisfied, since participation is only feasible when it is eligible. Thus we have that $D=0$ whenever $Z=0$. Thus it can only be that $f_D(1, A, X, \\epsilon_D) \\geq 0 = f_D(0, A, X, \\epsilon_D)$.\n","\n","In this case, we can estimate the local average treatment effect (LATE):\n","$$\n","\\alpha = E[Y(1) - Y(0) | D(1) > D(0)]\n","$$\n","This can be identified using the Neyman orthogonal moment equation:\n","\\begin{align}\n","E\\left[g(1, X) - g(0, X) + H(Z) (Y - g(Z, X)) - \\alpha \\cdot  (m(1, X) - m(0, X) + H(Z) (D - m(Z, X))\\right] = 0\n","\\end{align}\n","where\n","\\begin{align}\n","g(Z,X) =~& E[Y|Z,X],\\\\\n","m(Z,X) =~& E[D|Z,X],\\\\\n","H(Z) =~& \\frac{Z}{Pr(Z=1|X)} - \\frac{1 - Z}{1 - Pr(Z=1|X)}\n","\\end{align}"],"metadata":{"id":"bCayhAlaINjL"},"id":"bCayhAlaINjL"},{"cell_type":"code","source":["# DML for IIVM with D and Z as classifiers or regressors\n","DML2.for.IIVM <- function(x, d, z, y, dreg0, dreg1, yreg0, yreg1, zreg, trimming=0.01, nfold=5, method=\"classification\", dt=0, bt=0) {\n","  # this implements DML2 algorithm, where there moments are estimated via DML, before constructing\n","  # the pooled estimate of theta randomly split data into folds\n","\n","  ## NB This method has many if statements to accommodate the various estimators we will use.\n","  ## Unlike Python's sklearn, all methods have idfferent default arguments in their predict functions.\n","  ## See official R documentation for details.\n","\n","  yhat0 <- rep(0, length(y))\n","  yhat1 <- rep(0, length(y))\n","  dhat0 <- rep(0, length(d))\n","  dhat1 <- rep(0, length(d))\n","  zhat <- rep(0, length(Z))\n","\n","  nobs <- nrow(X)\n","  foldid <- rep.int(1:nfold,times = ceiling(nobs/nfold))[sample.int(nobs)]\n","  I <- split(1:nobs, foldid)\n","  # create residualized objects to fill\n","  ytil <- dtil <- ztil<- rep(NA, nobs)\n","\n","  # obtain cross-fitted residuals\n","  cat(\"fold: \")\n","  for(b in 1:length(I)){\n","\n","    # define helpful variables\n","    Xb = X[I[[b]],]\n","    Xnotb = X[-I[[b]],]\n","    Znotb = Z[-I[[b]]]\n","\n","    # training dfs subsetted on the -I[[b]] fold\n","    XZ0 = X[-I[[b]],][Z[-I[[b]]]==0]\n","    yZ0 = y[-I[[b]]][Z[-I[[b]]]==0]\n","    XZ1 = X[-I[[b]],][Z[-I[[b]]]==1]\n","    yZ1 = y[-I[[b]]][Z[-I[[b]]]==1]\n","    DZ0 = d[-I[[b]]][Z[-I[[b]]]==0]\n","    DZ1 = d[-I[[b]]][Z[-I[[b]]]==1]\n","\n","\n","    if (method == \"regression\") {\n","        XZ0 = as.matrix(XZ0)\n","        XZ1 = as.matrix(XZ1)\n","        Xb = as.matrix(Xb)\n","        Xnotb = as.matrix(Xnotb)\n","\n","        # Train an outcome model on training data that received Z=0 and predict outcome on all data in the test set\n","        yfit0 <- yreg0((XZ0), yZ0)\n","        yhat0[I[[b]]] <- predict(yfit0, (Xb)) # default type = \"response\"\n","\n","        # train an outcome model on training data that received Z=1 and predict outcome on all data in test set\n","        yfit1 <- yreg1((XZ1), yZ1)\n","        yhat1[I[[b]]] <- predict(yfit1, (Xb))\n","\n","        # train a treatment model on training data that received Z=0 and predict treatment on all data in test set\n","        if (mean(DZ0) > 0) { # it could be that D=0, whenever Z=0 deterministically\n","            dreg0_ <- dreg0\n","            dfit0 <- dreg0_((XZ0), DZ0)\n","            dhat0[I[[b]]] <- predict(dfit0, (Xb), type=\"response\") # default type = \"response\", but for family binomial it's logg odds\n","        }\n","        # train a treamtent model on training data that received Z=1 and predict treatment on all data in test set\n","        if (mean(DZ1) < 1) { # it could be that D=1, whenever Z=1 deterministically\n","            dreg1_ <- dreg1\n","            dfit1 <- dreg1_((XZ1), DZ1)\n","            dhat1[I[[b]]] <- predict(dfit1, (Xb), type=\"response\")\n","        } else {\n","            dhat1[I[[b]]] <- 1\n","        }\n","\n","    } else if (method == \"randomforest\") {\n","        DZ0factor = as.factor(D)[-I[[b]]][Z[-I[[b]]]==0]\n","        DZ1factor = as.factor(D)[-I[[b]]][Z[-I[[b]]]==1]\n","        Znotb = as.factor(Znotb)\n","\n","        yfit0 <- yreg0((XZ0), yZ0)\n","        yhat0[I[[b]]] <- predict(yfit0, (Xb), type=\"response\")\n","        yfit1 <- yreg1((XZ1), yZ1)\n","        yhat1[I[[b]]] <- predict(yfit1, (Xb), type=\"response\")\n","\n","        if (mean(DZ0) > 0) {\n","            dreg0_ <- dreg0\n","            dfit0 <- dreg0_((XZ0), DZ0factor)\n","            dhat0[I[[b]]] <- predict(dfit0, (Xb), type=\"prob\")[,2] # get second column because type = \"prob\"\n","        }\n","        if (mean(DZ1) < 1) {\n","            dreg1_ <- dreg1\n","            dfit1 <- dreg1_((XZ1), DZ1factor)\n","            dhat1[I[[b]]] <- predict(dfit1, (Xb), type=\"prob\")[,2]\n","        } else {\n","            dhat1[I[[b]]] <- 1\n","        }\n","\n","    } else if (method == \"decisiontrees\") {\n","        XZ0 = as.data.frame(XZ0)\n","        XZ1 = as.data.frame(XZ1)\n","        Xb = as.data.frame(Xb)\n","        Xnotb = as.data.frame(Xnotb)\n","\n","        yfit0 <- yreg0((XZ0), yZ0)\n","        yhat0[I[[b]]] <- predict(yfit0, (Xb)) # default type = \"response\" for decision trees for continuous response\n","\n","        yfit1 <- yreg1((XZ1), yZ1)\n","        yhat1[I[[b]]] <- predict(yfit1, (Xb))\n","\n","        if (mean(DZ0) > 0) {\n","            dreg0_ <- dreg0\n","            dfit0 <- dreg0_((XZ0), as.factor(DZ0))\n","            dhat0[I[[b]]] <- predict(dfit0, (Xb))[,2] # for decision trees, default = \"prob\" for decision trees with factor responses\n","        }\n","\n","        if (mean(DZ1) < 1) {\n","            dreg1_ <- dreg1\n","            dfit1 <- dreg1_((XZ1), as.factor(DZ1))\n","            dhat1[I[[b]]] <- predict(dfit1, (Xb))[,2]\n","        } else {\n","            dhat1[I[[b]]] <- 1\n","        }\n","\n","    } else if (method == \"boostedtrees\") {\n","        XZ0 = as.data.frame(XZ0)\n","        XZ1 = as.data.frame(XZ1)\n","        Xb = as.data.frame(Xb)\n","        Xnotb = as.data.frame(Xnotb)\n","\n","        yfit0 <- yreg0((XZ0), yZ0)\n","        yhat0[I[[b]]] <- predict(yfit0, (Xb)) # default type = \"response\" for boosted trees\n","        yfit1 <- yreg1((XZ1), yZ1)\n","        yhat1[I[[b]]] <- predict(yfit1, (Xb))\n","\n","        if (mean(DZ0) > 0) {\n","            dreg0_ <- dreg0\n","            dfit0 <- dreg0_((XZ0), DZ0)\n","            dhat0[I[[b]]] <- predict(dfit0, (Xb), type = \"response\") # default for boosted trees is log odds.\n","        }\n","        if (mean(DZ1) < 1) {\n","            dreg1_ <- dreg1\n","            dfit1 <- dreg1_((XZ1), DZ1)\n","            dhat1[I[[b]]] <- predict(dfit1, (Xb), type = \"response\")\n","        } else {\n","          dhat1[I[[b]]] <- 1\n","        }\n","\n","    }\n","\n","    # propensity scores:\n","    if (method == \"regression\"){\n","      zfit_b <- zreg((Xnotb), Znotb)\n","      zhat_b <- predict(zfit_b, (Xb), type=\"response\")\n","    } else if (method == \"randomforest\"){\n","      zfit_b <- zreg((Xnotb), Znotb)\n","      zhat_b <- predict(zfit_b, (Xb), type = \"prob\")[,2]\n","    } else if (method == \"decisiontrees\"){\n","      zfit_b <- zreg((Xnotb), as.factor(Znotb))\n","      zhat_b <- predict(zfit_b, (Xb)) # default is prob, so get second column\n","      zhat_b = zhat_b[,2]\n","    } else if (method == \"boostedtrees\"){\n","      zfit_b <- zreg((Xnotb), Znotb)\n","      zhat_b <- predict(zfit_b, (Xb), type = \"response\")\n","    }\n","    zhat_b <- pmax(pmin(zhat_b, 1 - trimming), trimming) # trimming so scores are between [trimming, (1-trimming)]\n","    zhat[I[[b]]] <- zhat_b\n","\n","    cat(b,\" \")\n","  }\n","\n","\n","  # Prediction of treatment and outcome for observed instrument\n","  yhat <- yhat0 * (1 - Z) + yhat1 * Z\n","  dhat <- dhat0 * (1 - Z) + dhat1 * Z\n","\n","  # residuals\n","  ytil <- y-yhat\n","  dtil <- D-dhat\n","  ztil <- Z-zhat\n","\n","  # doubly robust quantity for every sample\n","  HZ <- Z / zhat - (1 - Z) / (1 - zhat)\n","  drZ <- yhat1 - yhat0 + (y - yhat) * HZ\n","  drD <- dhat1 - dhat0 + (D - dhat) * HZ\n","  coef.est <- mean(drZ) / mean(drD)\n","  cat(\"point\", coef.est)\n","  psi <- drZ - coef.est * drD\n","  Jhat <- mean(drD)\n","  variance <- mean(psi^2) / Jhat^2\n","  se <- sqrt(variance / nrow(X))\n","  cat(\"se\", se)\n","\n","  return(list(coef.est = coef.est, se = se, yhat = yhat, dhat = dhat, zhat = zhat, ytil = ytil, dtil = dtil, ztil = ztil, drZ = drZ, drD = drD))\n","}"],"metadata":{"id":"rQYifUnFIt5z"},"id":"rQYifUnFIt5z","execution_count":null,"outputs":[]},{"cell_type":"code","source":["summary <- function(coef.est, se, yhat, dhat, zhat, ytil, dtil, ztil, drZ, drD, X, Z, D, y, name) {\n","  summary_data <- data.frame(estimate = coef.est,  # point estimate\n","                             se = se,  # standard error\n","                             lower = coef.est - 1.96 * se,  # lower end of 95% confidence interval\n","                             upper = coef.est + 1.96 * se,  # upper end of 95% confidence interval\n","                             rmse_y = sqrt(mean(ytil^2)),  # res of model that predicts outcome y\n","                             rmse_D = sqrt(mean(dtil^2)),  # res of model that predicts treatment D\n","                             rmse_Z = sqrt(mean(ztil^2)),  # res of model that predicts instrument Z\n","                             accuracy_D = mean(abs(dtil) < 0.5),  # binary classification accuracy of model for D\n","                             accuracy_Z = mean(abs(ztil) < 0.5)  # binary classification accuracy of model for Z\n","  )\n","  row.names(summary_data) <- name\n","  return(summary_data)\n","}\n"],"metadata":{"id":"iArB2WQHBXuV"},"id":"iArB2WQHBXuV","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# DML with Lasso/Ridge\n","set.seed(123)\n","cat(sprintf(\"\\nDML with Lasso/Logistic \\n\"))\n","# DML with Lasso/Ridge\n","dreg0 <- function(x,d){cv.glmnet(x, d, family=\"binomial\", alpha=0, nfolds=5)}\n","dreg1 <- function(x,d){cv.glmnet(x, d, family=\"binomial\", alpha=0, nfolds=5)}\n","yreg0 <- function(x,y){cv.glmnet(x, y, family=\"gaussian\", alpha=1, nfolds=5)}\n","yreg1 <- function(x,y){cv.glmnet(x, y, family=\"gaussian\", alpha=1, nfolds=5)}\n","zreg <- function(x,z){cv.glmnet(x, z, family=\"binomial\", alpha=0, nfolds=5)}\n","\n","DML2.results <- DML2.for.IIVM(as.matrix(X), D, Z, y, dreg0, dreg1, yreg0, yreg1, zreg, trimming=0.01, nfold=3, method=\"regression\")\n","sum.lasso_ridge.cv <-summary(DML2.results$coef.est,  DML2.results$se, DML2.results$yhat, DML2.results$dhat, DML2.results$zhat, DML2.results$ytil, DML2.results$dtil, DML2.results$ztil, DML2.results$drZ, DML2.results$drD, name = 'LassoCV/LogisticCV')\n","table <- data.frame()\n","table <- rbind(table, sum.lasso_ridge.cv)\n","table"],"metadata":{"id":"Tj-8FFF3BXxV"},"id":"Tj-8FFF3BXxV","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# DML with Random Forest\n","set.seed(123)\n","cat(sprintf(\"\\nDML with Random Forest \\n\"))\n","\n","dreg0 <- function(x,d){randomForest(x, d, ntree=1000, nodesize=10)}  #ML method=Forest\n","dreg1 <- function(x,d){randomForest(x, d, ntree=1000, nodesize=10)}  #ML method=Forest\n","yreg0 <- function(x,y){randomForest(x, y, ntree=1000, nodesize=10)}  #ML method=Forest\n","yreg1 <- function(x,y){randomForest(x, y, ntree=1000, nodesize=10)}  #ML method=Forest\n","zreg <- function(x,z){randomForest(x, z, ntree=1000, nodesize=10)}  #ML method=Forest\n","\n","DML2.results <- DML2.for.IIVM(X,D,Z, y, dreg0, dreg1, yreg0, yreg1, zreg, trimming=0.01, nfold=3, method=\"randomforest\")\n","sum.rf <- summary(DML2.results$coef.est,  DML2.results$se, DML2.results$yhat, DML2.results$dhat, DML2.results$zhat, DML2.results$ytil, DML2.results$dtil, DML2.results$ztil, DML2.results$drZ, DML2.results$drD, name = 'RF')\n","table <- rbind(table, sum.rf)\n","table"],"metadata":{"id":"sXjbvMbEkYJd"},"id":"sXjbvMbEkYJd","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# DML with Decision Trees\n","set.seed(123)\n","cat(sprintf(\"\\nDML with Decision Trees \\n\"))\n","\n","dreg0 <- function(x,d){rpart(as.formula(\"D~.\"), cbind(data.frame(D=d),x), method = \"class\", minbucket=10, cp = 0.001)}\n","dreg1 <- function(x,d){rpart(as.formula(\"D~.\"), cbind(data.frame(D=d),x), method = \"class\", minbucket=10, cp = 0.001)}\n","yreg0 <- function(x,y){rpart(as.formula(\"y~.\"), cbind(data.frame(y=y),x), minbucket=10, cp = 0.001)}\n","yreg1 <- function(x,y){rpart(as.formula(\"y~.\"), cbind(data.frame(y=y),x), minbucket=10, cp = 0.001)}\n","zreg <- function(x,z){rpart(as.formula(\"Z~.\"), cbind(data.frame(Z=z),x), method = \"class\", minbucket=10, cp = 0.001)}\n","\n","DML2.results <- DML2.for.IIVM(X, D, Z, y, dreg0, dreg1, yreg0, yreg1, zreg, trimming=0.01, nfold=3, method=\"decisiontrees\")\n","sum.tr <- summary(DML2.results$coef.est,  DML2.results$se, DML2.results$yhat, DML2.results$dhat, DML2.results$zhat, DML2.results$ytil, DML2.results$dtil, DML2.results$ztil, DML2.results$drZ, DML2.results$drD, name = 'Decision Trees')\n","table <- rbind(table, sum.tr)\n","table"],"metadata":{"id":"ZZRXpY8YkYNN"},"id":"ZZRXpY8YkYNN","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# DML with Boosted Trees\n","set.seed(123)\n","cat(sprintf(\"\\nDML with Boosted Trees \\n\"))\n","\n","# NB: early stopping cannot easily be implemented with gbm\n","## set n.trees = best, where best <- gbm.perf(dreg.boost, plot.it = FALSE)\n","dreg0  <- function(x,d){gbm(as.formula(\"D~.\"), cbind(data.frame(D=d),x), distribution= \"bernoulli\", interaction.depth=2, n.trees=100, shrinkage=.1)}\n","dreg1  <- function(x,d){gbm(as.formula(\"D~.\"), cbind(data.frame(D=d),x), distribution= \"bernoulli\", interaction.depth=2, n.trees=100, shrinkage=.1)}\n","yreg0  <- function(x,y){gbm(as.formula(\"y~.\"), cbind(data.frame(y=y),x), distribution= \"gaussian\", interaction.depth=2, n.trees=100, shrinkage=.1)}\n","yreg1  <- function(x,y){gbm(as.formula(\"y~.\"), cbind(data.frame(y=y),x), distribution= \"gaussian\", interaction.depth=2, n.trees=100, shrinkage=.1)}\n","zreg  <- function(x,z){gbm(as.formula(\"Z~.\"), cbind(data.frame(Z=z),x), distribution= \"bernoulli\", interaction.depth=2, n.trees=100, shrinkage=.1)}\n","\n","# passing these through regression as type=\"response\", and D and Z should not be factors!\n","DML2.results <- DML2.for.IIVM(X, D, Z, y, dreg0, dreg1, yreg0, yreg1, zreg, trimming=0.01, nfold=3, method=\"boostedtrees\")\n","sum.boost <- summary(DML2.results$coef.est,  DML2.results$se, DML2.results$yhat, DML2.results$dhat, DML2.results$zhat, DML2.results$ytil, DML2.results$dtil, DML2.results$ztil, DML2.results$drZ, DML2.results$drD, name = 'Boosted Trees')\n","table <- rbind(table, sum.boost)\n","table"],"metadata":{"id":"RYqykjPskYQJ"},"id":"RYqykjPskYQJ","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Comparing with the PLR model"],"metadata":{"id":"UflbjTEG5SXV"},"id":"UflbjTEG5SXV"},{"cell_type":"code","source":["tableplr"],"metadata":{"id":"CIS-58oi4sa1"},"id":"CIS-58oi4sa1","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We find that the PLR model overestimates the effect by around 1k; though both sets of results have overlapping confidence intervals."],"metadata":{"id":"M4Zi0FPH5VZG"},"id":"M4Zi0FPH5VZG"},{"cell_type":"markdown","source":["\n","Again as before, ideally we would do (semi) cross-fitting with AutoML in order to find good first-stage models and re-run DML with these models. Unfortunately this is not easy to do in R.\n","\n","As before, in the below analysis of robust inference, we choose Boosted Trees as they perform well in RMSE and accuracy on first-stage models."],"metadata":{"id":"VrBkj_pc5qgm"},"id":"VrBkj_pc5qgm"},{"cell_type":"code","source":["iivm_robust_inference <- function(point, stderr, yhat, Dhat, Zhat, resy, resD, resZ, drZ, drD, X, Z, D, y, grid, alpha = 0.05) {\n","    # Inference in the partially linear IV model that is robust to weak identification.\n","    # grid: grid of theta values to search over when trying to identify the confidence region\n","    # alpha: confidence level\n","\n","    n <- dim(X)[1]\n","    thr <- qchisq(1 - alpha, df = 1)\n","    accept <- c()\n","\n","    for (theta in grid) {\n","        moment <- drZ - theta * drD\n","        test <- n * mean(moment)^2 / var(moment)\n","        if (test <= thr) {\n","            accept <- c(accept, theta)\n","        }\n","    }\n","\n","    return(accept)\n","}\n"],"metadata":{"id":"bj67nsgcCDoS"},"id":"bj67nsgcCDoS","execution_count":null,"outputs":[]},{"cell_type":"code","source":["grid <- seq(0, 20000, length.out = 10000)\n","region <- iivm_robust_inference(point = DML2.results$coef.est, stderr = DML2.results$se, yhat = DML2.results$yhat, Dhat = DML2.results$dhat, Zhat = DML2.results$zhat, resy = DML2.results$ytil, resD = DML2.results$dtil, resZ = DML2.results$ztil, drZ = DML2.results$drZ, drD = DML2.results$drD, X=X, Z=Z, D=D, y=y, grid=grid)\n","\n","# Calculate min and max\n","min_region <- min(region)\n","max_region <- max(region)\n","\n","print(min_region)\n","print(max_region)"],"metadata":{"id":"KqgPk1Jm4sdo"},"id":"KqgPk1Jm4sdo","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We find again that the robust inference confidence region is almost identical to the normal based inference. We are most probably in the strong instrument regime. We can check the t-statistic for the effect of the instrument on the treatment to verify this."],"metadata":{"id":"akCGDMZJCN3h"},"id":"akCGDMZJCN3h"},{"cell_type":"markdown","id":"01de9f24","metadata":{"papermill":{"duration":0.010725,"end_time":"2022-04-19T09:06:51.098483","exception":false,"start_time":"2022-04-19T09:06:51.087758","status":"completed"},"tags":[],"id":"01de9f24"},"source":["# DoubleML package\n","\n","There exist nice packages out there that can help us do our estimation with the simple call of a function. Such packages include `EconML` (Python) and `DoubleML` (Python and R).\n","\n","We run through IIVM using `DoubleML` below to illustrate. The `DoubleML` package internally builds on `mlr3`. We use the meta package `mlr3` to generate predictions with machine learning methods. A comprehensive introduction and description of the `mlr3` package is provided in the [mlr3book](https://mlr3book.mlr-org.com/). A list of all learners that you can use in `mlr3` can be found [here](https://mlr3extralearners.mlr-org.com/articles/learners/list_learners.html). The entry in the columns *mlr3 Package* and *Packages* indicate which packages must be installed/loaded in your R session.\n","\n","You find additional information about `DoubleML` on the package on the package website https://docs.doubleml.org/ and the R documentation page https://docs.doubleml.org/r/stable/."]},{"cell_type":"code","execution_count":null,"id":"2846a36a","metadata":{"execution":{"iopub.execute_input":"2022-04-19T09:06:51.144230Z","iopub.status.busy":"2022-04-19T09:06:51.142682Z","iopub.status.idle":"2022-04-19T09:07:11.366508Z","shell.execute_reply":"2022-04-19T09:07:11.364676Z"},"papermill":{"duration":20.239271,"end_time":"2022-04-19T09:07:11.369618","exception":false,"start_time":"2022-04-19T09:06:51.130347","status":"completed"},"tags":[],"id":"2846a36a"},"outputs":[],"source":["install.packages(\"DoubleML\")\n","install.packages(\"mlr3learners\")\n","install.packages(\"mlr3\")\n","install.packages(\"data.table\")\n","install.packages(\"ranger\")\n","\n","library(DoubleML)\n","library(mlr3learners)\n","library(mlr3)\n","library(data.table)\n","library(ranger)"]},{"cell_type":"markdown","id":"2259ae1c","metadata":{"papermill":{"duration":0.015455,"end_time":"2022-04-19T09:12:00.920079","exception":false,"start_time":"2022-04-19T09:12:00.904624","status":"completed"},"tags":[],"id":"2259ae1c"},"source":["## Local Average Treatment Effects of 401(k) Participation on Net Financial Assets"]},{"cell_type":"markdown","id":"9c27e413","metadata":{"papermill":{"duration":0.015158,"end_time":"2022-04-19T09:12:00.950542","exception":false,"start_time":"2022-04-19T09:12:00.935384","status":"completed"},"tags":[],"id":"9c27e413"},"source":["## Interactive IV Model (IIVM)"]},{"cell_type":"markdown","id":"4fa23c70","metadata":{"papermill":{"duration":0.015304,"end_time":"2022-04-19T09:12:00.981285","exception":false,"start_time":"2022-04-19T09:12:00.965981","status":"completed"},"tags":[],"id":"4fa23c70"},"source":["Now, we consider estimation of local average treatment effects (LATE) of participation with the binary instrument `e401`. As before, $Y$ denotes the outcome `net_tfa`, and $X$ is the vector of covariates.  Here the structural equation model is:\n","\n","\\begin{eqnarray}\n","& Y = g_0(Z,X) + U, &\\quad E[U\\mid Z,X] = 0,\\\\\n","& D = r_0(Z,X) + V, &\\quad E[V\\mid Z, X] = 0,\\\\\n","& Z = m_0(X) + \\zeta, &\\quad E[\\zeta \\mid X] = 0.\n","\\end{eqnarray}"]},{"cell_type":"code","execution_count":null,"id":"cb223b75","metadata":{"execution":{"iopub.execute_input":"2022-04-19T09:12:01.015264Z","iopub.status.busy":"2022-04-19T09:12:01.013761Z","iopub.status.idle":"2022-04-19T09:12:01.062714Z","shell.execute_reply":"2022-04-19T09:12:01.060993Z"},"papermill":{"duration":0.06823,"end_time":"2022-04-19T09:12:01.064957","exception":false,"start_time":"2022-04-19T09:12:00.996727","status":"completed"},"tags":[],"id":"cb223b75"},"outputs":[],"source":["# Constructing the data (as DoubleMLData)\n","formula_flex2 = \"net_tfa ~ p401+ e401 + poly(age, 6, raw=TRUE) + poly(inc, 8, raw=TRUE) + poly(educ, 4, raw=TRUE) + poly(fsize, 2, raw=TRUE) + male + marr + twoearn + db + pira + hown\"\n","model_flex2 = as.data.table(model.frame(formula_flex2, data))\n","x_cols = colnames(model_flex2)[-c(1,2,3)]\n","data_IV = DoubleMLData$new(model_flex2, y_col = \"net_tfa\", d_cols = \"p401\", z_cols =\"e401\",x_cols=x_cols)"]},{"cell_type":"code","execution_count":null,"id":"e652ffad","metadata":{"execution":{"iopub.execute_input":"2022-04-19T09:12:01.099480Z","iopub.status.busy":"2022-04-19T09:12:01.097954Z","iopub.status.idle":"2022-04-19T09:12:22.012186Z","shell.execute_reply":"2022-04-19T09:12:22.010302Z"},"papermill":{"duration":20.934595,"end_time":"2022-04-19T09:12:22.014866","exception":false,"start_time":"2022-04-19T09:12:01.080271","status":"completed"},"tags":[],"id":"e652ffad"},"outputs":[],"source":["lgr::get_logger(\"mlr3\")$set_threshold(\"warn\")\n","lasso <- lrn(\"regr.cv_glmnet\",nfolds = 5, s = \"lambda.min\")\n","lasso_class <- lrn(\"classif.cv_glmnet\", nfolds = 5, s = \"lambda.min\")\n","dml_MLIIVM = DoubleMLIIVM$new(data_IV, ml_g = lasso,\n","                       ml_m = lasso_class, ml_r = lasso_class,n_folds=5, subgroups = list(always_takers = FALSE,\n","                                         never_takers = TRUE))\n","dml_MLIIVM$fit(store_predictions=TRUE)\n","dml_MLIIVM$summary()\n","lasso_MLIIVM <- dml_MLIIVM$coef\n","lasso_std_MLIIVM <- dml_MLIIVM$se"]},{"cell_type":"markdown","id":"63103667","metadata":{"papermill":{"duration":0.015382,"end_time":"2022-04-19T09:12:22.045989","exception":false,"start_time":"2022-04-19T09:12:22.030607","status":"completed"},"tags":[],"id":"63103667"},"source":["The confidence interval for the local average treatment effect of participation is given by"]},{"cell_type":"code","execution_count":null,"id":"322855c4","metadata":{"execution":{"iopub.execute_input":"2022-04-19T09:12:22.080639Z","iopub.status.busy":"2022-04-19T09:12:22.079018Z","iopub.status.idle":"2022-04-19T09:12:22.101731Z","shell.execute_reply":"2022-04-19T09:12:22.100047Z"},"papermill":{"duration":0.042067,"end_time":"2022-04-19T09:12:22.103953","exception":false,"start_time":"2022-04-19T09:12:22.061886","status":"completed"},"tags":[],"id":"322855c4"},"outputs":[],"source":["dml_MLIIVM$confint(level = 0.95)"]},{"cell_type":"markdown","id":"2965410d","metadata":{"papermill":{"duration":0.015374,"end_time":"2022-04-19T09:12:22.134875","exception":false,"start_time":"2022-04-19T09:12:22.119501","status":"completed"},"tags":[],"id":"2965410d"},"source":["Here we can also check the accuracy of the model:"]},{"cell_type":"code","execution_count":null,"id":"1476fd27","metadata":{"execution":{"iopub.execute_input":"2022-04-19T09:12:22.168618Z","iopub.status.busy":"2022-04-19T09:12:22.167046Z","iopub.status.idle":"2022-04-19T09:12:22.203398Z","shell.execute_reply":"2022-04-19T09:12:22.201502Z"},"papermill":{"duration":0.056054,"end_time":"2022-04-19T09:12:22.206157","exception":false,"start_time":"2022-04-19T09:12:22.150103","status":"completed"},"tags":[],"id":"1476fd27"},"outputs":[],"source":["# variables\n","y <- as.matrix(pension$net_tfa) # true observations\n","d <- as.matrix(pension$p401)\n","z <- as.matrix(pension$e401)\n","\n","# predictions\n","dml_MLIIVM$params_names()\n","g0_hat <- as.matrix(dml_MLIIVM$predictions$ml_g0) # predictions of g_0(z=0, X)\n","g1_hat <- as.matrix(dml_MLIIVM$predictions$ml_g1) # predictions of g_0(z=1, X)\n","g_hat <- z*g1_hat+(1-z)*g0_hat # predictions of g_0\n","r0_hat <- as.matrix(dml_MLIIVM$predictions$ml_r0) # predictions of r_0(z=0, X)\n","r1_hat <- as.matrix(dml_MLIIVM$predictions$ml_r1) # predictions of r_0(z=1, X)\n","r_hat <- z*r1_hat+(1-z)*r0_hat # predictions of r_0\n","m_hat <- as.matrix(dml_MLIIVM$predictions$ml_m) # predictions of m_o"]},{"cell_type":"code","execution_count":null,"id":"444c53f4","metadata":{"execution":{"iopub.execute_input":"2022-04-19T09:12:22.241038Z","iopub.status.busy":"2022-04-19T09:12:22.239185Z","iopub.status.idle":"2022-04-19T09:12:22.275804Z","shell.execute_reply":"2022-04-19T09:12:22.273813Z"},"papermill":{"duration":0.056945,"end_time":"2022-04-19T09:12:22.278593","exception":false,"start_time":"2022-04-19T09:12:22.221648","status":"completed"},"tags":[],"id":"444c53f4"},"outputs":[],"source":["# cross-fitted RMSE: outcome\n","lasso_y_MLIIVM <- sqrt(mean((y-g_hat)^2))\n","lasso_y_MLIIVM\n","\n","# cross-fitted RMSE: treatment\n","lasso_d_MLIIVM <- sqrt(mean((d-r_hat)^2))\n","lasso_d_MLIIVM\n","\n","# cross-fitted RMSE: instrument\n","lasso_z_MLIIVM <- sqrt(mean((z-m_hat)^2))\n","lasso_z_MLIIVM\n"]},{"cell_type":"markdown","id":"a7461966","metadata":{"papermill":{"duration":0.016468,"end_time":"2022-04-19T09:12:22.311250","exception":false,"start_time":"2022-04-19T09:12:22.294782","status":"completed"},"tags":[],"id":"a7461966"},"source":["Again, we repeat the procedure for the other machine learning methods:"]},{"cell_type":"code","source":["# needed to run boosting\n","remotes::install_github(\"mlr-org/mlr3extralearners\")\n","install.packages(\"mlr3extralearners\")\n","install.packages(\"mboost\")\n","library(mlr3extralearners)\n","library(mboost)"],"metadata":{"id":"59YzwIcpEnyV"},"id":"59YzwIcpEnyV","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Forest\n","randomForest <- lrn(\"regr.ranger\")\n","randomForest_class <- lrn(\"classif.ranger\")\n","\n","# Trees\n","trees <- lrn(\"regr.rpart\")\n","trees_class <- lrn(\"classif.rpart\")\n","\n","# Boosting\n","boost <- lrn(\"regr.glmboost\")\n","boost_class <- lrn(\"classif.glmboost\")"],"metadata":{"id":"Ec0g3ch3EjAl"},"id":"Ec0g3ch3EjAl","execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"id":"3935dfc5","metadata":{"execution":{"iopub.execute_input":"2022-04-19T09:12:22.346966Z","iopub.status.busy":"2022-04-19T09:12:22.345453Z","iopub.status.idle":"2022-04-19T09:13:31.739923Z","shell.execute_reply":"2022-04-19T09:13:31.738086Z"},"papermill":{"duration":69.414354,"end_time":"2022-04-19T09:13:31.742249","exception":false,"start_time":"2022-04-19T09:12:22.327895","status":"completed"},"tags":[],"id":"3935dfc5"},"outputs":[],"source":["### random forest ###\n","\n","lgr::get_logger(\"mlr3\")$set_threshold(\"warn\")\n","dml_MLIIVM = DoubleMLIIVM$new(data_IV, ml_g = randomForest,\n","                       ml_m = randomForest_class, ml_r = randomForest_class,n_folds=3, subgroups = list(always_takers = FALSE,\n","                                         never_takers = TRUE))\n","dml_MLIIVM$fit(store_predictions=TRUE)\n","dml_MLIIVM$summary()\n","forest_MLIIVM <- dml_MLIIVM$coef\n","forest_std_MLIIVM <- dml_MLIIVM$se\n","\n","# predictions\n","g0_hat <- as.matrix(dml_MLIIVM$predictions$ml_g0) # predictions of g_0(Z=0, X)\n","g1_hat <- as.matrix(dml_MLIIVM$predictions$ml_g1) # predictions of g_0(Z=1, X)\n","g_hat <- z*g1_hat+(1-z)*g0_hat # predictions of g_0\n","r0_hat <- as.matrix(dml_MLIIVM$predictions$ml_r0) # predictions of r_0(Z=0, X)\n","r1_hat <- as.matrix(dml_MLIIVM$predictions$ml_r1) # predictions of r_0(Z=1, X)\n","r_hat <- z*r1_hat+(1-z)*r0_hat # predictions of r_0\n","m_hat <- as.matrix(dml_MLIIVM$predictions$ml_m) # predictions of m_o\n","\n","# cross-fitted RMSE: outcome\n","forest_y_MLIIVM <- sqrt(mean((y-g_hat)^2))\n","forest_y_MLIIVM\n","\n","# cross-fitted RMSE: treatment\n","forest_d_MLIIVM <- sqrt(mean((d-r_hat)^2))\n","forest_d_MLIIVM\n","\n","# cross-fitted RMSE: instrument\n","forest_z_MLIIVM <- sqrt(mean((z-m_hat)^2))\n","forest_z_MLIIVM\n","\n","### trees ###\n","\n","dml_MLIIVM = DoubleMLIIVM$new(data_IV, ml_g = trees,\n","                       ml_m = trees_class, ml_r = trees_class,n_folds=3, subgroups = list(always_takers = FALSE,\n","                                         never_takers = TRUE))\n","dml_MLIIVM$fit(store_predictions=TRUE)\n","dml_MLIIVM$summary()\n","tree_MLIIVM <- dml_MLIIVM$coef\n","tree_std_MLIIVM <- dml_MLIIVM$se\n","\n","# predictions\n","g0_hat <- as.matrix(dml_MLIIVM$predictions$ml_g0) # predictions of g_0(Z=0, X)\n","g1_hat <- as.matrix(dml_MLIIVM$predictions$ml_g1) # predictions of g_0(Z=1, X)\n","g_hat <- z*g1_hat+(1-z)*g0_hat # predictions of g_0\n","r0_hat <- as.matrix(dml_MLIIVM$predictions$ml_r0) # predictions of r_0(Z=0, X)\n","r1_hat <- as.matrix(dml_MLIIVM$predictions$ml_r1) # predictions of r_0(Z=1, X)\n","r_hat <- z*r1_hat+(1-z)*r0_hat # predictions of r_0\n","m_hat <- as.matrix(dml_MLIIVM$predictions$ml_m) # predictions of m_o\n","\n","# cross-fitted RMSE: outcome\n","tree_y_MLIIVM <- sqrt(mean((y-g_hat)^2))\n","tree_y_MLIIVM\n","\n","# cross-fitted RMSE: treatment\n","tree_d_MLIIVM <- sqrt(mean((d-r_hat)^2))\n","tree_d_MLIIVM\n","\n","# cross-fitted RMSE: instrument\n","tree_z_MLIIVM <- sqrt(mean((z-m_hat)^2))\n","tree_z_MLIIVM\n","\n","\n","### boosting ###\n","dml_MLIIVM = DoubleMLIIVM$new(data_IV, ml_g = boost,\n","                       ml_m = boost_class, ml_r = boost_class,n_folds=3, subgroups = list(always_takers = FALSE,\n","                                         never_takers = TRUE))\n","dml_MLIIVM$fit(store_predictions=TRUE)\n","dml_MLIIVM$summary()\n","boost_MLIIVM <- dml_MLIIVM$coef\n","boost_std_MLIIVM <- dml_MLIIVM$se\n","\n","# predictions\n","g0_hat <- as.matrix(dml_MLIIVM$predictions$ml_g0) # predictions of g_0(Z=0, X)\n","g1_hat <- as.matrix(dml_MLIIVM$predictions$ml_g1) # predictions of g_0(Z=1, X)\n","g_hat <- z*g1_hat+(1-z)*g0_hat # predictions of g_0\n","r0_hat <- as.matrix(dml_MLIIVM$predictions$ml_r0) # predictions of r_0(Z=0, X)\n","r1_hat <- as.matrix(dml_MLIIVM$predictions$ml_r1) # predictions of r_0(Z=1, X)\n","r_hat <- z*r1_hat+(1-z)*r0_hat # predictions of r_0\n","m_hat <- as.matrix(dml_MLIIVM$predictions$ml_m) # predictions of m_o\n","\n","# cross-fitted RMSE: outcome\n","boost_y_MLIIVM <- sqrt(mean((y-g_hat)^2))\n","boost_y_MLIIVM\n","\n","# cross-fitted RMSE: treatment\n","boost_d_MLIIVM <- sqrt(mean((d-r_hat)^2))\n","boost_d_MLIIVM\n","\n","# cross-fitted RMSE: instrument\n","boost_z_MLIIVM <- sqrt(mean((z-m_hat)^2))\n","boost_z_MLIIVM"]},{"cell_type":"code","execution_count":null,"id":"7187fc74","metadata":{"execution":{"iopub.execute_input":"2022-04-19T09:13:31.782847Z","iopub.status.busy":"2022-04-19T09:13:31.781148Z","iopub.status.idle":"2022-04-19T09:13:31.821873Z","shell.execute_reply":"2022-04-19T09:13:31.820166Z"},"papermill":{"duration":0.061872,"end_time":"2022-04-19T09:13:31.824148","exception":false,"start_time":"2022-04-19T09:13:31.762276","status":"completed"},"tags":[],"id":"7187fc74"},"outputs":[],"source":["table <- matrix(0, 5, 4)\n","table[1,1:4]   <- c(lasso_MLIIVM,forest_MLIIVM,tree_MLIIVM,boost_MLIIVM)\n","table[2,1:4]   <- c(lasso_std_MLIIVM,forest_std_MLIIVM,tree_std_MLIIVM,boost_std_MLIIVM)\n","table[3,1:4]   <- c(lasso_y_MLIIVM,forest_y_MLIIVM,tree_y_MLIIVM,boost_y_MLIIVM)\n","table[4,1:4]   <- c(lasso_d_MLIIVM,forest_d_MLIIVM,tree_d_MLIIVM,boost_d_MLIIVM)\n","table[5,1:4]   <- c(lasso_z_MLIIVM,forest_z_MLIIVM,tree_z_MLIIVM,boost_z_MLIIVM)\n","rownames(table) <- c(\"Estimate\",\"Std.Error\",\"RMSE Y\",\"RMSE D\",\"RMSE Z\")\n","colnames(table) <- c(\"Lasso\",\"Random Forest\",\"Trees\",\"Boosting\")\n","tab<- xtable(table, digits = 2)\n","tab"]},{"cell_type":"markdown","id":"f4ce7be1","metadata":{"papermill":{"duration":0.017437,"end_time":"2022-04-19T09:13:31.859052","exception":false,"start_time":"2022-04-19T09:13:31.841615","status":"completed"},"tags":[],"id":"f4ce7be1"},"source":["We report results based on four ML methods for estimating the nuisance functions used in\n","forming the orthogonal estimating equations. We find again that the estimates of the treatment effect are stable across ML methods. The estimates are highly significant, hence we would reject the hypothesis\n","that the effect of 401(k) participation has no effect on financial health."]},{"cell_type":"markdown","id":"4939cd9c","metadata":{"papermill":{"duration":0.017163,"end_time":"2022-04-19T09:13:31.893361","exception":false,"start_time":"2022-04-19T09:13:31.876198","status":"completed"},"tags":[],"id":"4939cd9c"},"source":["We might rerun the model using the best ML method for each equation to get a final estimate for the treatment effect of participation:"]},{"cell_type":"code","execution_count":null,"id":"ca612b71","metadata":{"execution":{"iopub.execute_input":"2022-04-19T09:13:31.931238Z","iopub.status.busy":"2022-04-19T09:13:31.929630Z","iopub.status.idle":"2022-04-19T09:13:52.687242Z","shell.execute_reply":"2022-04-19T09:13:52.685428Z"},"papermill":{"duration":20.780029,"end_time":"2022-04-19T09:13:52.690594","exception":false,"start_time":"2022-04-19T09:13:31.910565","status":"completed"},"tags":[],"id":"ca612b71"},"outputs":[],"source":["lgr::get_logger(\"mlr3\")$set_threshold(\"warn\")\n","dml_MLIIVM = DoubleMLIIVM$new(data_IV, ml_g = randomForest,\n","                       ml_m = lasso_class, ml_r = lasso_class,n_folds=5, subgroups = list(always_takers = FALSE,\n","                                         never_takers = TRUE))\n","dml_MLIIVM$fit(store_predictions=TRUE)\n","dml_MLIIVM$summary()\n","best_MLIIVM <- dml_MLIIVM$coef\n","best_std_MLIIVM <- dml_MLIIVM$se"]}],"metadata":{"kernelspec":{"display_name":"R","language":"R","name":"ir"},"language_info":{"codemirror_mode":"r","file_extension":".r","mimetype":"text/x-r-source","name":"R","pygments_lexer":"r","version":"4.0.5"},"papermill":{"default_parameters":{},"duration":427.936706,"end_time":"2022-04-19T09:13:53.230849","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2022-04-19T09:06:45.294143","version":"2.3.4"},"colab":{"provenance":[{"file_id":"19I5Y0xpkT43Sqs3tDlwNQQUrJ0qPszVJ","timestamp":1701716317102}]}},"nbformat":4,"nbformat_minor":5}