---
title: An R Markdown document converted from "AC2/r-dml-401k-IV.irnb"
output: html_document
---

# Inference on Predictive and Causal Effects in High-Dimensional Nonlinear Models

## Impact of 401(k) on  Financial Wealth

We consider estimation of the effect of 401(k) participation
on accumulated assets. 401(k) plans are pension accounts sponsored by employers. The key problem in determining the effect of participation in 401(k) plans on accumulated assets is saver heterogeneity coupled with the fact that the decision to enroll in a 401(k) is non-random. It is generally recognized that some people have a higher preference for saving than others. It also seems likely that those individuals with high unobserved preference for saving would be most likely to choose to participate in tax-advantaged retirement savings plans and would tend to have otherwise high amounts of accumulated assets. The presence of unobserved savings preferences with these properties then implies that conventional estimates that do not account for saver heterogeneity and endogeneity of participation will be biased upward, tending to overstate the savings effects of 401(k) participation.

One can argue that eligibility for enrolling in a 401(k) plan in this data can be taken as exogenous after conditioning on a few observables of which the most important for their argument is income. The basic idea is that, at least around the time 401(k)â€™s initially became available, people were unlikely to be basing their employment decisions on whether an employer offered a 401(k) but would instead focus on income and other aspects of the job.

```{r}
install.packages("xtable")
install.packages("hdm")
install.packages("sandwich")
install.packages("ggplot2")
install.packages("randomForest")
install.packages("glmnet")
install.packages("rpart")
install.packages("gbm")

library(xtable)
library(hdm)
library(sandwich)
library(ggplot2)
library(randomForest)
library(data.table)
library(glmnet)
library(rpart)
library(gbm)

set.seed(123)
```

### Data

The raw dataset can be found [here](https://raw.githubusercontent.com/CausalAIBook/MetricsMLNotebooks/main/data/401k.csv).
The data set can be loaded from the `hdm` package for R directly by typing:


```{r}
data(pension)
data <- pension
dim(data)
```

See the "Details" section on the description of the data set, which can be accessed by

```{r}
help(pension)
```

The data consist of 9,915 observations at the household level drawn from the 1991 Survey of Income and Program Participation (SIPP).  All the variables are referred to 1990. We use net financial assets (*net\_tfa*) as the outcome variable, $Y$,  in our analysis. The net financial assets are computed as the sum of IRA balances, 401(k) balances, checking accounts, saving bonds, other interest-earning accounts, other interest-earning assets, stocks, and mutual funds less non mortgage debts.

Among the $9915$ individuals, $3682$ are eligible to participate in the program. The variable *e401* indicates eligibility and *p401* indicates participation, respectively.

```{r}
hist_e401 = ggplot(data, aes(x = e401, fill = factor(e401))) + geom_bar()
hist_e401
```

Eligibility is highly associated with financial wealth:

```{r}
dens_net_tfa = ggplot(data, aes(x = net_tfa, color = factor(e401), fill = factor(e401)) ) +
                    geom_density() + xlim(c(-20000, 150000)) +
                    facet_wrap(.~e401)

dens_net_tfa
```

The unconditional APE of e401 is about $19559$:

```{r}
e1 <- data[data$e401==1,]
e0 <- data[data$e401==0,]
round(mean(e1$net_tfa)-mean(e0$net_tfa),0)
```

Among the $3682$ individuals that  are eligible, $2594$ decided to participate in the program. The unconditional APE of p401 is about $27372$:

```{r}
p1 <- data[data$p401==1,]
p0 <- data[data$p401==0,]
round(mean(p1$net_tfa)-mean(p0$net_tfa),0)
```

As discussed, these estimates are biased since they do not account for saver heterogeneity and endogeneity of participation.

```{r}
# instrument variable
Z <- data[,'e401']
# treatment variable
D <- data[, 'p401']
# outcome variable
y <- data[,'net_tfa']
```

### We construct the engineered features for controls

```{r}
# Constructing the controls
X_formula = "~ poly(age, 6, raw=TRUE) + poly(inc, 8, raw=TRUE) + poly(educ, 4, raw=TRUE) + poly(fsize, 2, raw=TRUE) + male + marr + twoearn + db + pira + hown"
X = as.data.table(model.frame(X_formula, pension))
head(X)
```

# Instrumental Variables: Effect of 401k Participation on Financial Assets

## Double ML IV under Partial Linearity

Now, we consider estimation of average treatment effects of participation in 401k, i.e. `p401`, with the binary instrument being eligibility in 401k, i.e. `e401`. As before, $Y$ denotes the outcome `net_tfa`, and $X$ is the vector of covariates. We consider a partially linear structural equation model:
\begin{eqnarray*}
Y & := & g_Y(\epsilon_Y) D + f_Y(A, X, \epsilon_Y),  \\
D & := & f_D(Z, X, A, \epsilon_D), \\
Z & := & f_Z(X, \epsilon_Z),\\
A & : =  & f_A(X, \epsilon_A), \\
X & := &  \epsilon_X,
\end{eqnarray*}
where $A$ is a vector of un-observed confounders.

Under this structural equation model, the average treatment effect:
\begin{align}
\alpha = E[Y(1) - Y(0)]
\end{align}
can be identified by the moment restriction:
\begin{align}
E[(\tilde{Y} - \alpha \tilde{D}) \tilde{Z}] = 0
\end{align}
where for any variable $V$, we denote with $\tilde{V} = V - E[V|X]$.

```{r}
set.seed(1)
yfit.lasso.cv   <- cv.glmnet(as.matrix(X), y, family="gaussian", alpha=1)  # family gaussian means that we'll be using square loss
Dfit.lasso.cv   <- cv.glmnet(as.matrix(X), D, family="gaussian", alpha=1)  # family gaussian means that we'll be using square loss
Zfit.lasso.cv   <- cv.glmnet(as.matrix(X), Z, family="gaussian", alpha=1)  # family gaussian means that we'll be using square loss


yhat.lasso.cv   <- predict(yfit.lasso.cv, newx = as.matrix(X))       # predictions
Dhat.lasso.cv   <- predict(Dfit.lasso.cv, newx = as.matrix(X))       # predictions
Zhat.lasso.cv   <- predict(Zfit.lasso.cv, newx = as.matrix(X))       # predictions

resy <- y-yhat.lasso.cv
resD <- D-Dhat.lasso.cv
resZ <- Z-Zhat.lasso.cv

# Estimate
mean(resy * resZ) / mean(resZ*resD)
```

Recall if we want to do inference, we need to either use the theoretically driven penalty paramter for Lasso or perform cross-fitting.

### DML with Non-Linear ML Models and Cross-fitting

```{r}
# DML for PLIVM with D and Z as classifiers or regressors
DML2.for.PLIVM <- function(x, d, z, y, dreg, yreg, zreg, nfold=5, method="regression") {
  nobs <- nrow(x)
  foldid <- rep.int(1:nfold,times = ceiling(nobs/nfold))[sample.int(nobs)]
  I <- split(1:nobs, foldid)
  # create residualized objects to fill
  ytil <- dtil <- ztil<- rep(NA, nobs)
  # obtain cross-fitted residuals
  cat("fold: ")
  for(b in 1:length(I)){
    if (method == "randomforest"){
        # take a fold out
        dfit <- dreg(x[-I[[b]],], d[-I[[b]]])
        zfit <- zreg(x[-I[[b]],], z[-I[[b]]])
        yfit <- yreg(x[-I[[b]],], y[-I[[b]]])
        # predict the fold out
        dhat <- predict(dfit, x[I[[b]],], type="prob")[,2]  # type = "prob" is like predict_proba in scikitlearn
        zhat <- predict(zfit, x[I[[b]],], type="prob")[,2]
        yhat <- predict(yfit, x[I[[b]],])  # default type = "response" for regression for RF, type = "vector" for regression for Decision Trees
        # record residual
        dtil[I[[b]]] <- (as.numeric(d[I[[b]]])-1 - dhat) # as.numeric will turn d = as.factor(d) from 0,1 to 1,2 so subtract 1!
        ztil[I[[b]]] <- (as.numeric(z[I[[b]]])-1 - zhat)
        ytil[I[[b]]] <- (y[I[[b]]] - yhat)
    } else if (method == "regression") { # works for both boosted trees and glmnet
        # take a fold out
        dfit <- dreg(x[-I[[b]],], d[-I[[b]]])
        zfit <- zreg(x[-I[[b]],], z[-I[[b]]])
        yfit <- yreg(x[-I[[b]],], y[-I[[b]]])
        # predict the fold out
        dhat <- predict(dfit, x[I[[b]],], type="response")
        zhat <- predict(zfit, x[I[[b]],], type="response")
        yhat <- predict(yfit, x[I[[b]],], type="response")
        # record residual
        dtil[I[[b]]] <- (d[I[[b]]] - dhat)
        ztil[I[[b]]] <- (z[I[[b]]] - zhat)
        ytil[I[[b]]] <- (y[I[[b]]] - yhat)
    } else if (method == "decisiontrees"){
        # take a fold out
        dfit <- dreg(x[-I[[b]],], as.factor(d)[-I[[b]]])
        zfit <- zreg(x[-I[[b]],], as.factor(z)[-I[[b]]])
        yfit <- yreg(x[-I[[b]],], y[-I[[b]]])
        # predict the fold out
        dhat <- predict(dfit, x[I[[b]],])[,2]
        zhat <- predict(zfit, x[I[[b]],])[,2]
        yhat <- predict(yfit, x[I[[b]],])
        # record residual
        dtil[I[[b]]] <- (d[I[[b]]] - dhat)
        ztil[I[[b]]] <- (z[I[[b]]] - zhat)
        ytil[I[[b]]] <- (y[I[[b]]] - yhat)
    }

    cat(b," ")
  }
  ivfit = tsls(y=ytil,d=dtil, x=NULL, z=ztil, intercept=FALSE)
  coef.est <- ivfit$coef          #extract coefficient
  se <- ivfit$se                  #record standard error
  cat(sprintf("\ncoef (se) = %g (%g)\n", coef.est , se))

  return( list(coef.est=coef.est, se=se, dtil=dtil, ytil=ytil, ztil=ztil) )
}
```

```{r}
summary <- function(point, stderr, resy, resD, resZ, name) {
  data <- data.frame(
    estimate = point, # point estimate
    stderr = stderr, # standard error
    lower = point - 1.96 * stderr, # lower end of 95% confidence interval
    upper = point + 1.96 * stderr, # upper end of 95% confidence interval
    `rmse y` = sqrt(mean(resy^2)), # RMSE of model that predicts outcome y
    `rmse D` = sqrt(mean(resD^2)), # RMSE of model that predicts treatment D
    `rmse Z` = sqrt(mean(resZ^2)), # RMSE of model that predicts treatment D
    `accuracy D` = mean(abs(resD) < 0.5), # binary classification accuracy of model for D
    `accuracy Z` = mean(abs(resZ) < 0.5) # binary classification accuracy of model for Z
  )
  rownames(data) <- name
  return(data)
}
```

#### Double Lasso with Cross-Fitting

```{r}
# DML with LassoCV
set.seed(123)
cat(sprintf("\nDML with Lasso CV \n"))

dreg.lasso.cv <- function(x,d){ cv.glmnet(x, d, family="gaussian", alpha=1, nfolds=5)}
yreg.lasso.cv <- function(x,y){ cv.glmnet(x, y, family="gaussian", alpha=1, nfolds=5)}
zreg.lasso.cv <- function(x,z){ cv.glmnet(x, z, family="gaussian", alpha=1, nfolds=5)}

DML2.results <- DML2.for.PLIVM(as.matrix(X), D, Z, y, dreg.lasso.cv, yreg.lasso.cv, zreg.lasso.cv, nfold=5, method="regression")
sum.lasso.cv <- summary(DML2.results$coef.est, DML2.results$se, DML2.results$ytil, DML2.results$dtil, DML2.results$ztil, name = 'LassoCV')
tableplr <- data.frame()
tableplr <- rbind(sum.lasso.cv)
tableplr

ytil.lasso <- DML2.results$ytil
dtil.lasso <- DML2.results$dtil
ztil.lasso <- DML2.results$ztil
```

#### Using a $\ell_2$ Penalized Logistic Regression for D and Z

```{r}
# DML with Lasso/Ridge
set.seed(123)
cat(sprintf("\nDML with Lasso/Logistic \n"))

dreg.ridge.cv <- function(x,d){cv.glmnet(x, d, family="binomial", alpha=0, nfolds=5)}
yreg.ridge.cv <- function(x,y){cv.glmnet(x, y, family="gaussian", alpha=1, nfolds=5)}
zreg.ridge.cv <- function(x,z){cv.glmnet(x, z, family="binomial", alpha=0, nfolds=5)}

DML2.results <- DML2.for.PLIVM(as.matrix(X), D, Z, y, dreg.ridge.cv, yreg.ridge.cv, zreg.ridge.cv, nfold=5, method="regression")
sum.lasso_ridge.cv <- summary(DML2.results$coef.est, DML2.results$se, DML2.results$ytil, DML2.results$dtil, DML2.results$ztil, name = 'LassoCV/LogisticCV')
tableplr <- rbind(tableplr, sum.lasso_ridge.cv)
tableplr

ytil.ridge <- DML2.results$ytil
dtil.ridge <- DML2.results$dtil
ztil.ridge <- DML2.results$ztil
```

### Random Forests

```{r}
# DML with Random Forest
set.seed(123)
cat(sprintf("\nDML with Random Forest \n"))

dreg.rf <- function(x,d){randomForest(x, d, ntree=1000, nodesize=10)}  #ML method=Forest
yreg.rf <- function(x,y){randomForest(x, y, ntree=1000, nodesize=10)}  #ML method=Forest
zreg.rf <- function(x,z){randomForest(x, z, ntree=1000, nodesize=10)}  #ML method=Forest

DML2.results = DML2.for.PLIVM(as.matrix(X), as.factor(D), as.factor(Z), y, dreg.rf, yreg.rf, zreg.rf, nfold=5, method="randomforest")
sum.rf <- summary(DML2.results$coef.est, DML2.results$se, DML2.results$ytil, DML2.results$dtil, DML2.results$ztil, name = 'RF')
tableplr <- rbind(tableplr, sum.rf)
tableplr

ytil.rf <- DML2.results$ytil
dtil.rf <- DML2.results$dtil
ztil.rf <- DML2.results$ztil
```

### Decision Trees

```{r}
# DML with Decision Trees
set.seed(123)
cat(sprintf("\nDML with Decision Trees \n"))

dreg.tr <- function(x,d){rpart(as.formula("D~."), cbind(data.frame(D=d),x), method = "class", minbucket=10, cp = 0.001)}
yreg.tr <- function(x,y){rpart(as.formula("y~."), cbind(data.frame(y=y),x), minbucket=10, cp = 0.001)}
zreg.tr <- function(x,z){rpart(as.formula("Z~."), cbind(data.frame(Z=z),x), method = "class", minbucket=10, cp = 0.001)}

DML2.results = DML2.for.PLIVM(X, D, Z, y, dreg.tr, yreg.tr, zreg.tr, nfold=5, method="decisiontrees") # decision tree takes in X as dataframe, not matrix/array
sum.tr <- summary(DML2.results$coef.est, DML2.results$se, DML2.results$ytil, DML2.results$dtil, DML2.results$ztil, name = 'Decision Trees')
tableplr <- rbind(tableplr, sum.tr)
tableplr

ytil.tr <- DML2.results$ytil
dtil.tr <- DML2.results$dtil
ztil.tr <- DML2.results$ztil
```

### Boosted Trees

```{r}
# DML with Boosted Trees
set.seed(123)
cat(sprintf("\nDML with Boosted Trees \n"))

# NB: early stopping cannot easily be implemented with gbm
## set n.trees = best, where best <- gbm.perf(dreg.boost, plot.it = FALSE)
dreg.boost  <- function(x,d){gbm(as.formula("D~."), cbind(data.frame(D=d),x), distribution= "bernoulli", interaction.depth=2, n.trees=100, shrinkage=.1)}
yreg.boost  <- function(x,y){gbm(as.formula("y~."), cbind(data.frame(y=y),x), distribution= "gaussian", interaction.depth=2, n.trees=100, shrinkage=.1)}
zreg.boost  <- function(x,z){gbm(as.formula("Z~."), cbind(data.frame(Z=z),x), distribution= "bernoulli", interaction.depth=2, n.trees=100, shrinkage=.1)}

# passing these through regression as type="response", and D and Z should not be factors!
DML2.results = DML2.for.PLIVM(X, D, Z, y, dreg.boost, yreg.boost, zreg.boost, nfold=5, method = "regression")
sum.boost <- summary(DML2.results$coef.est, DML2.results$se, DML2.results$ytil, DML2.results$dtil, DML2.results$ztil, name = 'Boosted Trees')
tableplr <- rbind(tableplr, sum.boost)
tableplr

ytil.boost <- DML2.results$ytil
dtil.boost <- DML2.results$dtil
ztil.boost <- DML2.results$ztil
```

## Ensembles

Boosted trees give the best RMSE for Y, D, and Z, so the ensemble based on choosing the best performing prediction rule is identical to boosting in this case.

```{r}
# Best fit is boosted trees for D, Z, Y

sum.best <- summary(DML2.results$coef.est, DML2.results$se, DML2.results$ytil, DML2.results$dtil, DML2.results$dtil, name = 'Best')
tableplr <- rbind(tableplr, sum.best)
tableplr
```

We'll form a model average with unconstrained least squares weights.

```{r}
# Least squares model average

dhat.lasso <- D - dtil.lasso
dhat.ridge <- D - dtil.ridge
dhat.rf <- D - dtil.rf
dhat.tr <- D - dtil.tr
dhat.boost <- D - dtil.boost

yhat.lasso <- y - ytil.lasso
yhat.ridge <- y - ytil.ridge
yhat.rf <- y - ytil.rf
yhat.tr <- y - ytil.tr
yhat.boost <- y - ytil.boost

zhat.lasso <- Z - ztil.lasso
zhat.ridge <- Z - ztil.ridge
zhat.rf <- Z - ztil.rf
zhat.tr <- Z - ztil.tr
zhat.boost <- Z - ztil.boost

ma.dtil <- lm(D~dhat.lasso+dhat.ridge+dhat.rf+dhat.tr+dhat.boost)$residuals
ma.ytil <- lm(y~yhat.lasso+yhat.ridge+yhat.rf+yhat.tr+yhat.boost)$residuals
ma.ztil <- lm(Z~zhat.lasso+zhat.ridge+zhat.rf+zhat.tr+zhat.boost)$residuals

ivfit = tsls(y=ma.ytil,d=ma.dtil, x=NULL, z=ma.ztil, intercept=FALSE)
coef.est <- ivfit$coef          #extract coefficient
se <- ivfit$se                  #record standard error

sum.ma <- summary(coef.est, se, ma.ytil, ma.dtil, ma.ztil, name = 'Model Average')
tableplr <- rbind(tableplr, sum.ma)
tableplr
```

## Inference Robust to Weak Identification

Now we turn toward robustness when the instrument is weak.

Ideally, we would do (semi) cross-fitting with AutoML in order to find good first-stage models and re-run DML with these models. Unfortunately this is not easy to do in R. In the case of semi cross-fitting, we can use R's H20 AutoML trained on the entire training set $y\sim X$, $D \sim X$, $Z\sim X$ to determine the best model (eg ensemble), but H20 does not allow you to extract the best model so we can re-use that in DML.

Thus, in the below analysis of robust inference, we choose Boosted Trees as they perform well.

```{r}
robust_inference <- function(point, stderr, resD, resy, resZ, grid, alpha = 0.05) {
    # Inference in the partially linear IV model that is robust to weak identification.
    # grid: grid of theta values to search over when trying to identify the confidence region
    # alpha: confidence level

    n <- dim(X)[1]
    thr <- qchisq(1 - alpha, df = 1)
    accept <- c()

    for (theta in grid) {
        moment <- (resy - theta * resD) * resZ
        test <- n * mean(moment)^2 / var(moment)
        if (test <= thr) {
            accept <- c(accept, theta)
        }
    }

    return(accept)
}
```

```{r}
grid <- seq(0, 20000, length.out = 10000)
region <- robust_inference(DML2.results$coef.est, DML2.results$stderr, DML2.results$dtil, DML2.results$ytil, DML2.results$ztil, grid=grid)
```

```{r}
grid <- seq(0, 20000, length.out = 10000)
region <- robust_inference(DML2.results$coef.est, DML2.results$stderr, DML2.results$dtil, DML2.results$ytil, DML2.results$ztil, grid=grid)# Calculate min and max
min_region <- min(region)
max_region <- max(region)

print(min_region)
print(max_region)
```

# Interactive IV Model and LATE

Now, we consider estimation of local average treatment effects (LATE) of participation `p401`, with the binary instrument `e401`. As before, $Y$ denotes the outcome `net_tfa`, and $X$ is the vector of covariates.  Here the structural equation model is:
\begin{eqnarray}
Y &:=&  f_Y (D, X, A, \epsilon_Y) \\
D &:= & f_D(Z, X, A, \epsilon_D) \in \{0,1\},  \\
Z  &:= & f_Z(X,\epsilon_Z) \in \{0,1\},  \\
X &:=&  \epsilon_X, \quad A = \epsilon_A,
\end{eqnarray}
where $\epsilon$'s are all exogenous and independent,
and
$$
z \mapsto f_D(z , A, X, \epsilon_D) \text{ is weakly increasing (weakly monotone)}.
$$
and $A$ is a vector of unobserved confounders. Note that in our setting monotonicity is satisfied, since participation is only feasible when it is eligible. Thus we have that $D=0$ whenever $Z=0$. Thus it can only be that $f_D(1, A, X, \epsilon_D) \geq 0 = f_D(0, A, X, \epsilon_D)$.

In this case, we can estimate the local average treatment effect (LATE):
$$
\alpha = E[Y(1) - Y(0) | D(1) > D(0)]
$$
This can be identified using the Neyman orthogonal moment equation:
\begin{align}
E\left[g(1, X) - g(0, X) + H(Z) (Y - g(Z, X)) - \alpha \cdot  (m(1, X) - m(0, X) + H(Z) (D - m(Z, X))\right] = 0
\end{align}
where
\begin{align}
g(Z,X) =~& E[Y|Z,X],\\
m(Z,X) =~& E[D|Z,X],\\
H(Z) =~& \frac{Z}{Pr(Z=1|X)} - \frac{1 - Z}{1 - Pr(Z=1|X)}
\end{align}

```{r}
# DML for IIVM with D and Z as classifiers or regressors
DML2.for.IIVM <- function(x, d, z, y, dreg0, dreg1, yreg0, yreg1, zreg, trimming=0.01, nfold=5, method="classification", dt=0, bt=0) {
  # this implements DML2 algorithm, where there moments are estimated via DML, before constructing
  # the pooled estimate of theta randomly split data into folds

  ## NB This method has many if statements to accommodate the various estimators we will use.
  ## Unlike Python's sklearn, all methods have idfferent default arguments in their predict functions.
  ## See official R documentation for details.

  yhat0 <- rep(0, length(y))
  yhat1 <- rep(0, length(y))
  dhat0 <- rep(0, length(d))
  dhat1 <- rep(0, length(d))
  zhat <- rep(0, length(Z))

  nobs <- nrow(X)
  foldid <- rep.int(1:nfold,times = ceiling(nobs/nfold))[sample.int(nobs)]
  I <- split(1:nobs, foldid)
  # create residualized objects to fill
  ytil <- dtil <- ztil<- rep(NA, nobs)

  # obtain cross-fitted residuals
  cat("fold: ")
  for(b in 1:length(I)){

    # define helpful variables
    Xb = X[I[[b]],]
    Xnotb = X[-I[[b]],]
    Znotb = Z[-I[[b]]]

    # training dfs subsetted on the -I[[b]] fold
    XZ0 = X[-I[[b]],][Z[-I[[b]]]==0]
    yZ0 = y[-I[[b]]][Z[-I[[b]]]==0]
    XZ1 = X[-I[[b]],][Z[-I[[b]]]==1]
    yZ1 = y[-I[[b]]][Z[-I[[b]]]==1]
    DZ0 = d[-I[[b]]][Z[-I[[b]]]==0]
    DZ1 = d[-I[[b]]][Z[-I[[b]]]==1]


    if (method == "regression") {
        XZ0 = as.matrix(XZ0)
        XZ1 = as.matrix(XZ1)
        Xb = as.matrix(Xb)
        Xnotb = as.matrix(Xnotb)

        # Train an outcome model on training data that received Z=0 and predict outcome on all data in the test set
        yfit0 <- yreg0((XZ0), yZ0)
        yhat0[I[[b]]] <- predict(yfit0, (Xb)) # default type = "response"

        # train an outcome model on training data that received Z=1 and predict outcome on all data in test set
        yfit1 <- yreg1((XZ1), yZ1)
        yhat1[I[[b]]] <- predict(yfit1, (Xb))

        # train a treatment model on training data that received Z=0 and predict treatment on all data in test set
        if (mean(DZ0) > 0) { # it could be that D=0, whenever Z=0 deterministically
            dreg0_ <- dreg0
            dfit0 <- dreg0_((XZ0), DZ0)
            dhat0[I[[b]]] <- predict(dfit0, (Xb), type="response") # default type = "response", but for family binomial it's logg odds
        }
        # train a treamtent model on training data that received Z=1 and predict treatment on all data in test set
        if (mean(DZ1) < 1) { # it could be that D=1, whenever Z=1 deterministically
            dreg1_ <- dreg1
            dfit1 <- dreg1_((XZ1), DZ1)
            dhat1[I[[b]]] <- predict(dfit1, (Xb), type="response")
        } else {
            dhat1[I[[b]]] <- 1
        }

    } else if (method == "randomforest") {
        DZ0factor = as.factor(D)[-I[[b]]][Z[-I[[b]]]==0]
        DZ1factor = as.factor(D)[-I[[b]]][Z[-I[[b]]]==1]
        Znotb = as.factor(Znotb)

        yfit0 <- yreg0((XZ0), yZ0)
        yhat0[I[[b]]] <- predict(yfit0, (Xb), type="response")
        yfit1 <- yreg1((XZ1), yZ1)
        yhat1[I[[b]]] <- predict(yfit1, (Xb), type="response")

        if (mean(DZ0) > 0) {
            dreg0_ <- dreg0
            dfit0 <- dreg0_((XZ0), DZ0factor)
            dhat0[I[[b]]] <- predict(dfit0, (Xb), type="prob")[,2] # get second column because type = "prob"
        }
        if (mean(DZ1) < 1) {
            dreg1_ <- dreg1
            dfit1 <- dreg1_((XZ1), DZ1factor)
            dhat1[I[[b]]] <- predict(dfit1, (Xb), type="prob")[,2]
        } else {
            dhat1[I[[b]]] <- 1
        }

    } else if (method == "decisiontrees") {
        XZ0 = as.data.frame(XZ0)
        XZ1 = as.data.frame(XZ1)
        Xb = as.data.frame(Xb)
        Xnotb = as.data.frame(Xnotb)

        yfit0 <- yreg0((XZ0), yZ0)
        yhat0[I[[b]]] <- predict(yfit0, (Xb)) # default type = "response" for decision trees for continuous response

        yfit1 <- yreg1((XZ1), yZ1)
        yhat1[I[[b]]] <- predict(yfit1, (Xb))

        if (mean(DZ0) > 0) {
            dreg0_ <- dreg0
            dfit0 <- dreg0_((XZ0), as.factor(DZ0))
            dhat0[I[[b]]] <- predict(dfit0, (Xb))[,2] # for decision trees, default = "prob" for decision trees with factor responses
        }

        if (mean(DZ1) < 1) {
            dreg1_ <- dreg1
            dfit1 <- dreg1_((XZ1), as.factor(DZ1))
            dhat1[I[[b]]] <- predict(dfit1, (Xb))[,2]
        } else {
            dhat1[I[[b]]] <- 1
        }

    } else if (method == "boostedtrees") {
        XZ0 = as.data.frame(XZ0)
        XZ1 = as.data.frame(XZ1)
        Xb = as.data.frame(Xb)
        Xnotb = as.data.frame(Xnotb)

        yfit0 <- yreg0((XZ0), yZ0)
        yhat0[I[[b]]] <- predict(yfit0, (Xb)) # default type = "response" for boosted trees
        yfit1 <- yreg1((XZ1), yZ1)
        yhat1[I[[b]]] <- predict(yfit1, (Xb))

        if (mean(DZ0) > 0) {
            dreg0_ <- dreg0
            dfit0 <- dreg0_((XZ0), DZ0)
            dhat0[I[[b]]] <- predict(dfit0, (Xb), type = "response") # default for boosted trees is log odds.
        }
        if (mean(DZ1) < 1) {
            dreg1_ <- dreg1
            dfit1 <- dreg1_((XZ1), DZ1)
            dhat1[I[[b]]] <- predict(dfit1, (Xb), type = "response")
        } else {
          dhat1[I[[b]]] <- 1
        }

    }

    # propensity scores:
    if (method == "regression"){
      zfit_b <- zreg((Xnotb), Znotb)
      zhat_b <- predict(zfit_b, (Xb), type="response")
    } else if (method == "randomforest"){
      zfit_b <- zreg((Xnotb), Znotb)
      zhat_b <- predict(zfit_b, (Xb), type = "prob")[,2]
    } else if (method == "decisiontrees"){
      zfit_b <- zreg((Xnotb), as.factor(Znotb))
      zhat_b <- predict(zfit_b, (Xb)) # default is prob, so get second column
      zhat_b = zhat_b[,2]
    } else if (method == "boostedtrees"){
      zfit_b <- zreg((Xnotb), Znotb)
      zhat_b <- predict(zfit_b, (Xb), type = "response")
    }
    zhat_b <- pmax(pmin(zhat_b, 1 - trimming), trimming) # trimming so scores are between [trimming, (1-trimming)]
    zhat[I[[b]]] <- zhat_b

    cat(b," ")
  }


  # Prediction of treatment and outcome for observed instrument
  yhat <- yhat0 * (1 - Z) + yhat1 * Z
  dhat <- dhat0 * (1 - Z) + dhat1 * Z

  # residuals
  ytil <- y-yhat
  dtil <- D-dhat
  ztil <- Z-zhat

  # doubly robust quantity for every sample
  HZ <- Z / zhat - (1 - Z) / (1 - zhat)
  drZ <- yhat1 - yhat0 + (y - yhat) * HZ
  drD <- dhat1 - dhat0 + (D - dhat) * HZ
  coef.est <- mean(drZ) / mean(drD)
  cat("point", coef.est)
  psi <- drZ - coef.est * drD
  Jhat <- mean(drD)
  variance <- mean(psi^2) / Jhat^2
  se <- sqrt(variance / nrow(X))
  cat("se", se)

  return(list(coef.est = coef.est, se = se, yhat = yhat, dhat = dhat, zhat = zhat, ytil = ytil, dtil = dtil, ztil = ztil, drZ = drZ, drD = drD, yhat0 = yhat0, yhat1 = yhat1, dhat0 = dhat0, dhat1 = dhat1))
}
```

```{r}
summary <- function(coef.est, se, yhat, dhat, zhat, ytil, dtil, ztil, drZ, drD, X, Z, D, y, name) {
  summary_data <- data.frame(estimate = coef.est,  # point estimate
                             se = se,  # standard error
                             lower = coef.est - 1.96 * se,  # lower end of 95% confidence interval
                             upper = coef.est + 1.96 * se,  # upper end of 95% confidence interval
                             rmse_y = sqrt(mean(ytil^2)),  # res of model that predicts outcome y
                             rmse_D = sqrt(mean(dtil^2)),  # res of model that predicts treatment D
                             rmse_Z = sqrt(mean(ztil^2)),  # res of model that predicts instrument Z
                             accuracy_D = mean(abs(dtil) < 0.5),  # binary classification accuracy of model for D
                             accuracy_Z = mean(abs(ztil) < 0.5)  # binary classification accuracy of model for Z
  )
  row.names(summary_data) <- name
  return(summary_data)
}
```

```{r}
# DML with Lasso/Ridge
set.seed(123)
cat(sprintf("\nDML with Lasso/Logistic \n"))
# DML with Lasso/Ridge
dreg0 <- function(x,d){cv.glmnet(x, d, family="binomial", alpha=0, nfolds=5)}
dreg1 <- function(x,d){cv.glmnet(x, d, family="binomial", alpha=0, nfolds=5)}
yreg0 <- function(x,y){cv.glmnet(x, y, family="gaussian", alpha=1, nfolds=5)}
yreg1 <- function(x,y){cv.glmnet(x, y, family="gaussian", alpha=1, nfolds=5)}
zreg <- function(x,z){cv.glmnet(x, z, family="binomial", alpha=0, nfolds=5)}

DML2.results <- DML2.for.IIVM(as.matrix(X), D, Z, y, dreg0, dreg1, yreg0, yreg1, zreg, trimming=0.01, nfold=5, method="regression")
sum.lasso_ridge.cv <-summary(DML2.results$coef.est,  DML2.results$se, DML2.results$yhat, DML2.results$dhat, DML2.results$zhat, DML2.results$ytil, DML2.results$dtil, DML2.results$ztil, DML2.results$drZ, DML2.results$drD, name = 'LassoCV/LogisticCV')
table <- data.frame()
table <- rbind(table, sum.lasso_ridge.cv)
table

yhat.lasso = DML2.results$yhat
dhat.lasso = DML2.results$dhat
yhat0.lasso = DML2.results$yhat0
yhat1.lasso = DML2.results$yhat1
dhat0.lasso = DML2.results$dhat0
dhat1.lasso = DML2.results$dhat1
zhat.lasso = DML2.results$zhat
```

```{r}
# DML with Random Forest
set.seed(123)
cat(sprintf("\nDML with Random Forest \n"))

dreg0 <- function(x,d){randomForest(x, d, ntree=1000, nodesize=10)}  #ML method=Forest
dreg1 <- function(x,d){randomForest(x, d, ntree=1000, nodesize=10)}  #ML method=Forest
yreg0 <- function(x,y){randomForest(x, y, ntree=1000, nodesize=10)}  #ML method=Forest
yreg1 <- function(x,y){randomForest(x, y, ntree=1000, nodesize=10)}  #ML method=Forest
zreg <- function(x,z){randomForest(x, z, ntree=1000, nodesize=10)}  #ML method=Forest

DML2.results <- DML2.for.IIVM(X,D,Z, y, dreg0, dreg1, yreg0, yreg1, zreg, trimming=0.01, nfold=5, method="randomforest")
sum.rf <- summary(DML2.results$coef.est,  DML2.results$se, DML2.results$yhat, DML2.results$dhat, DML2.results$zhat, DML2.results$ytil, DML2.results$dtil, DML2.results$ztil, DML2.results$drZ, DML2.results$drD, name = 'RF')
table <- rbind(table, sum.rf)
table

yhat.rf = DML2.results$yhat
dhat.rf = DML2.results$dhat
yhat0.rf = DML2.results$yhat0
yhat1.rf = DML2.results$yhat1
dhat0.rf = DML2.results$dhat0
dhat1.rf = DML2.results$dhat1
zhat.rf = DML2.results$zhat
```

```{r}
# DML with Decision Trees
set.seed(123)
cat(sprintf("\nDML with Decision Trees \n"))

dreg0 <- function(x,d){rpart(as.formula("D~."), cbind(data.frame(D=d),x), method = "class", minbucket=10, cp = 0.001)}
dreg1 <- function(x,d){rpart(as.formula("D~."), cbind(data.frame(D=d),x), method = "class", minbucket=10, cp = 0.001)}
yreg0 <- function(x,y){rpart(as.formula("y~."), cbind(data.frame(y=y),x), minbucket=10, cp = 0.001)}
yreg1 <- function(x,y){rpart(as.formula("y~."), cbind(data.frame(y=y),x), minbucket=10, cp = 0.001)}
zreg <- function(x,z){rpart(as.formula("Z~."), cbind(data.frame(Z=z),x), method = "class", minbucket=10, cp = 0.001)}

DML2.results <- DML2.for.IIVM(X, D, Z, y, dreg0, dreg1, yreg0, yreg1, zreg, trimming=0.01, nfold=5, method="decisiontrees")
sum.tr <- summary(DML2.results$coef.est,  DML2.results$se, DML2.results$yhat, DML2.results$dhat, DML2.results$zhat, DML2.results$ytil, DML2.results$dtil, DML2.results$ztil, DML2.results$drZ, DML2.results$drD, name = 'Decision Trees')
table <- rbind(table, sum.tr)
table

yhat.tr = DML2.results$yhat
dhat.tr = DML2.results$dhat
yhat0.tr = DML2.results$yhat0
yhat1.tr = DML2.results$yhat1
dhat0.tr = DML2.results$dhat0
dhat1.tr = DML2.results$dhat1
zhat.tr = DML2.results$zhat
```

```{r}
# DML with Boosted Trees
set.seed(123)
cat(sprintf("\nDML with Boosted Trees \n"))

# NB: early stopping cannot easily be implemented with gbm
## set n.trees = best, where best <- gbm.perf(dreg.boost, plot.it = FALSE)
dreg0  <- function(x,d){gbm(as.formula("D~."), cbind(data.frame(D=d),x), distribution= "bernoulli", interaction.depth=2, n.trees=100, shrinkage=.1)}
dreg1  <- function(x,d){gbm(as.formula("D~."), cbind(data.frame(D=d),x), distribution= "bernoulli", interaction.depth=2, n.trees=100, shrinkage=.1)}
yreg0  <- function(x,y){gbm(as.formula("y~."), cbind(data.frame(y=y),x), distribution= "gaussian", interaction.depth=2, n.trees=100, shrinkage=.1)}
yreg1  <- function(x,y){gbm(as.formula("y~."), cbind(data.frame(y=y),x), distribution= "gaussian", interaction.depth=2, n.trees=100, shrinkage=.1)}
zreg  <- function(x,z){gbm(as.formula("Z~."), cbind(data.frame(Z=z),x), distribution= "bernoulli", interaction.depth=2, n.trees=100, shrinkage=.1)}

# passing these through regression as type="response", and D and Z should not be factors!
DML2.results <- DML2.for.IIVM(X, D, Z, y, dreg0, dreg1, yreg0, yreg1, zreg, trimming=0.01, nfold=5, method="boostedtrees")
sum.boost <- summary(DML2.results$coef.est,  DML2.results$se, DML2.results$yhat, DML2.results$dhat, DML2.results$zhat, DML2.results$ytil, DML2.results$dtil, DML2.results$ztil, DML2.results$drZ, DML2.results$drD, name = 'Boosted Trees')
table <- rbind(table, sum.boost)
table

yhat.boost = DML2.results$yhat
dhat.boost = DML2.results$dhat
yhat0.boost = DML2.results$yhat0
yhat1.boost = DML2.results$yhat1
dhat0.boost = DML2.results$dhat0
dhat1.boost = DML2.results$dhat1
zhat.boost = DML2.results$zhat
```

## Ensembles

Boosted trees give the best RMSE for D and Z and random forests give the best RMSE for Y.

```{r}
# Best fit is boosted trees for D, Z and random forests for Y

best.yhat0 <- yhat0.rf
best.yhat1 <- yhat1.rf
best.yhat <- yhat.rf

best.dhat0 <- dhat0.boost
best.dhat1 <- dhat1.boost
best.dhat <- dhat.boost

best.zhat <- zhat.boost

ytil.best <- y - best.yhat
dtil.best <- D - best.dhat
ztil.best <- Z - best.zhat

# doubly robust quantity for every sample
HZ <- Z / best.zhat - (1 - Z) / (1 - best.zhat)
drZ <- best.yhat1 - best.yhat0 + (y - best.yhat) * HZ
drD <- best.dhat1 - best.dhat0 + (D - best.dhat) * HZ
coef.est <- mean(drZ) / mean(drD)
psi <- drZ - coef.est * drD
Jhat <- mean(drD)
variance <- mean(psi^2) / Jhat^2
se <- sqrt(variance / nrow(X))

sum.best <- summary(coef.est, se, best.yhat, best.dhat, best.zhat, ytil.best, dtil.best, ztil.best, drZ, drD, name = 'Best')
table <- rbind(table, sum.best)
table
```

We'll form a model average with unconstrained least squares weights.

```{r}
# Least squares model average
ma.dcoef <- lm(D~dhat.lasso+dhat.rf+dhat.tr+dhat.boost-1)$coef
ma.ycoef <- lm(y~yhat.lasso+yhat.rf+yhat.tr+yhat.boost-1)$coef
ma.zcoef <- lm(Z~zhat.lasso+zhat.rf+zhat.tr+zhat.boost-1)$coef

ma.yhat0 <- cbind(yhat0.lasso,yhat0.rf,yhat0.tr,yhat0.boost)%*%as.matrix(ma.ycoef)
ma.yhat1 <- cbind(yhat1.lasso,yhat1.rf,yhat1.tr,yhat1.boost)%*%as.matrix(ma.ycoef)
ma.dhat0 <- cbind(dhat0.lasso,dhat0.rf,dhat0.tr,dhat0.boost)%*%as.matrix(ma.dcoef)
ma.dhat1 <- cbind(dhat1.lasso,dhat1.rf,dhat1.tr,dhat1.boost)%*%as.matrix(ma.dcoef)
ma.zhat <- cbind(zhat.lasso,zhat.rf,zhat.tr,zhat.boost)%*%as.matrix(ma.zcoef)

# Prediction of treatment and outcome for observed instrument
ma.yhat <- ma.yhat0 * (1 - Z) + ma.yhat1 * Z
ma.dhat <- ma.dhat0 * (1 - Z) + ma.dhat1 * Z

# residuals
ma.ytil <- y-ma.yhat
ma.dtil <- D-ma.dhat
ma.ztil <- Z-ma.zhat

# doubly robust quantity for every sample
HZ <- Z / ma.zhat - (1 - Z) / (1 - ma.zhat)
drZ <- ma.yhat1 - ma.yhat0 + (y - ma.yhat) * HZ
drD <- ma.dhat1 - ma.dhat0 + (D - ma.dhat) * HZ
coef.est <- mean(drZ) / mean(drD)
psi <- drZ - coef.est * drD
Jhat <- mean(drD)
variance <- mean(psi^2) / Jhat^2
se <- sqrt(variance / nrow(X))

sum.ma <- summary(coef.est, se, ma.yhat, ma.dhat, ma.zhat, ma.ytil, ma.dtil, ma.ztil, drZ, drD, name = 'Model Average')
table <- rbind(table, sum.ma)
table
```

Comparing with the PLR model

```{r}
tableplr
```

We find that the PLR model overestimates the effect by around 1k; though both sets of results have overlapping confidence intervals.


Again as before, ideally we would do (semi) cross-fitting with AutoML in order to find good first-stage models and re-run DML with these models. Unfortunately this is not easy to do in R.

As before, in the below analysis of robust inference, we choose Boosted Trees as they perform well in RMSE and accuracy on first-stage models.

```{r}
iivm_robust_inference <- function(point, stderr, yhat, Dhat, Zhat, resy, resD, resZ, drZ, drD, X, Z, D, y, grid, alpha = 0.05) {
    # Inference in the partially linear IV model that is robust to weak identification.
    # grid: grid of theta values to search over when trying to identify the confidence region
    # alpha: confidence level

    n <- dim(X)[1]
    thr <- qchisq(1 - alpha, df = 1)
    accept <- c()

    for (theta in grid) {
        moment <- drZ - theta * drD
        test <- n * mean(moment)^2 / var(moment)
        if (test <= thr) {
            accept <- c(accept, theta)
        }
    }

    return(accept)
}
```

```{r}
grid <- seq(0, 20000, length.out = 10000)
region <- iivm_robust_inference(point = DML2.results$coef.est, stderr = DML2.results$se, yhat = DML2.results$yhat, Dhat = DML2.results$dhat, Zhat = DML2.results$zhat, resy = DML2.results$ytil, resD = DML2.results$dtil, resZ = DML2.results$ztil, drZ = DML2.results$drZ, drD = DML2.results$drD, X=X, Z=Z, D=D, y=y, grid=grid)

# Calculate min and max
min_region <- min(region)
max_region <- max(region)

print(min_region)
print(max_region)
```

We find again that the robust inference confidence region is almost identical to the normal based inference. We are most probably in the strong instrument regime. We can check the t-statistic for the effect of the instrument on the treatment to verify this.

# DoubleML package

There exist nice packages out there that can help us do our estimation with the simple call of a function. Such packages include `EconML` (Python) and `DoubleML` (Python and R).

We run through IIVM using `DoubleML` below to illustrate. The `DoubleML` package internally builds on `mlr3`. We use the meta package `mlr3` to generate predictions with machine learning methods. A comprehensive introduction and description of the `mlr3` package is provided in the [mlr3book](https://mlr3book.mlr-org.com/). A list of all learners that you can use in `mlr3` can be found [here](https://mlr3extralearners.mlr-org.com/articles/learners/list_learners.html). The entry in the columns *mlr3 Package* and *Packages* indicate which packages must be installed/loaded in your R session.

You find additional information about `DoubleML` on the package on the package website https://docs.doubleml.org/ and the R documentation page https://docs.doubleml.org/r/stable/.

```{r}
install.packages("DoubleML")
install.packages("mlr3learners")
install.packages("mlr3")
install.packages("data.table")
install.packages("ranger")

library(DoubleML)
library(mlr3learners)
library(mlr3)
library(data.table)
library(ranger)
```

## Local Average Treatment Effects of 401(k) Participation on Net Financial Assets

## Interactive IV Model (IIVM)

Now, we consider estimation of local average treatment effects (LATE) of participation with the binary instrument `e401`. As before, $Y$ denotes the outcome `net_tfa`, and $X$ is the vector of covariates.  Here the structural equation model is:

\begin{eqnarray}
& Y = g_0(Z,X) + U, &\quad E[U\mid Z,X] = 0,\\
& D = r_0(Z,X) + V, &\quad E[V\mid Z, X] = 0,\\
& Z = m_0(X) + \zeta, &\quad E[\zeta \mid X] = 0.
\end{eqnarray}

```{r}
# Constructing the data (as DoubleMLData)
formula_flex2 = "net_tfa ~ p401+ e401 + poly(age, 6, raw=TRUE) + poly(inc, 8, raw=TRUE) + poly(educ, 4, raw=TRUE) + poly(fsize, 2, raw=TRUE) + male + marr + twoearn + db + pira + hown"
model_flex2 = as.data.table(model.frame(formula_flex2, data))
x_cols = colnames(model_flex2)[-c(1,2,3)]
data_IV = DoubleMLData$new(model_flex2, y_col = "net_tfa", d_cols = "p401", z_cols ="e401",x_cols=x_cols)
```

```{r}
lgr::get_logger("mlr3")$set_threshold("warn")
lasso <- lrn("regr.cv_glmnet",nfolds = 5, s = "lambda.min")
lasso_class <- lrn("classif.cv_glmnet", nfolds = 5, s = "lambda.min")
dml_MLIIVM = DoubleMLIIVM$new(data_IV, ml_g = lasso,
                       ml_m = lasso_class, ml_r = lasso_class,n_folds=5, subgroups = list(always_takers = FALSE,
                                         never_takers = TRUE))
dml_MLIIVM$fit(store_predictions=TRUE)
dml_MLIIVM$summary()
lasso_MLIIVM <- dml_MLIIVM$coef
lasso_std_MLIIVM <- dml_MLIIVM$se
```

The confidence interval for the local average treatment effect of participation is given by

```{r}
dml_MLIIVM$confint(level = 0.95)
```

Here we can also check the accuracy of the model:

```{r}
# variables
y <- as.matrix(pension$net_tfa) # true observations
d <- as.matrix(pension$p401)
z <- as.matrix(pension$e401)

# predictions
dml_MLIIVM$params_names()
g0_hat <- as.matrix(dml_MLIIVM$predictions$ml_g0) # predictions of g_0(z=0, X)
g1_hat <- as.matrix(dml_MLIIVM$predictions$ml_g1) # predictions of g_0(z=1, X)
g_hat <- z*g1_hat+(1-z)*g0_hat # predictions of g_0
r0_hat <- as.matrix(dml_MLIIVM$predictions$ml_r0) # predictions of r_0(z=0, X)
r1_hat <- as.matrix(dml_MLIIVM$predictions$ml_r1) # predictions of r_0(z=1, X)
r_hat <- z*r1_hat+(1-z)*r0_hat # predictions of r_0
m_hat <- as.matrix(dml_MLIIVM$predictions$ml_m) # predictions of m_o
```

```{r}
# cross-fitted RMSE: outcome
lasso_y_MLIIVM <- sqrt(mean((y-g_hat)^2))
lasso_y_MLIIVM

# cross-fitted RMSE: treatment
lasso_d_MLIIVM <- sqrt(mean((d-r_hat)^2))
lasso_d_MLIIVM

# cross-fitted RMSE: instrument
lasso_z_MLIIVM <- sqrt(mean((z-m_hat)^2))
lasso_z_MLIIVM
```

Again, we repeat the procedure for the other machine learning methods:

```{r}
# needed to run boosting
remotes::install_github("mlr-org/mlr3extralearners")
install.packages("mlr3extralearners")
install.packages("mboost")
library(mlr3extralearners)
library(mboost)
```

```{r}
# Forest
randomForest <- lrn("regr.ranger")
randomForest_class <- lrn("classif.ranger")

# Trees
trees <- lrn("regr.rpart")
trees_class <- lrn("classif.rpart")

# Boosting
boost <- lrn("regr.glmboost")
boost_class <- lrn("classif.glmboost")
```

```{r}
### random forest ###

lgr::get_logger("mlr3")$set_threshold("warn")
dml_MLIIVM = DoubleMLIIVM$new(data_IV, ml_g = randomForest,
                       ml_m = randomForest_class, ml_r = randomForest_class,n_folds=3, subgroups = list(always_takers = FALSE,
                                         never_takers = TRUE))
dml_MLIIVM$fit(store_predictions=TRUE)
dml_MLIIVM$summary()
forest_MLIIVM <- dml_MLIIVM$coef
forest_std_MLIIVM <- dml_MLIIVM$se

# predictions
g0_hat <- as.matrix(dml_MLIIVM$predictions$ml_g0) # predictions of g_0(Z=0, X)
g1_hat <- as.matrix(dml_MLIIVM$predictions$ml_g1) # predictions of g_0(Z=1, X)
g_hat <- z*g1_hat+(1-z)*g0_hat # predictions of g_0
r0_hat <- as.matrix(dml_MLIIVM$predictions$ml_r0) # predictions of r_0(Z=0, X)
r1_hat <- as.matrix(dml_MLIIVM$predictions$ml_r1) # predictions of r_0(Z=1, X)
r_hat <- z*r1_hat+(1-z)*r0_hat # predictions of r_0
m_hat <- as.matrix(dml_MLIIVM$predictions$ml_m) # predictions of m_o

# cross-fitted RMSE: outcome
forest_y_MLIIVM <- sqrt(mean((y-g_hat)^2))
forest_y_MLIIVM

# cross-fitted RMSE: treatment
forest_d_MLIIVM <- sqrt(mean((d-r_hat)^2))
forest_d_MLIIVM

# cross-fitted RMSE: instrument
forest_z_MLIIVM <- sqrt(mean((z-m_hat)^2))
forest_z_MLIIVM

### trees ###

dml_MLIIVM = DoubleMLIIVM$new(data_IV, ml_g = trees,
                       ml_m = trees_class, ml_r = trees_class,n_folds=3, subgroups = list(always_takers = FALSE,
                                         never_takers = TRUE))
dml_MLIIVM$fit(store_predictions=TRUE)
dml_MLIIVM$summary()
tree_MLIIVM <- dml_MLIIVM$coef
tree_std_MLIIVM <- dml_MLIIVM$se

# predictions
g0_hat <- as.matrix(dml_MLIIVM$predictions$ml_g0) # predictions of g_0(Z=0, X)
g1_hat <- as.matrix(dml_MLIIVM$predictions$ml_g1) # predictions of g_0(Z=1, X)
g_hat <- z*g1_hat+(1-z)*g0_hat # predictions of g_0
r0_hat <- as.matrix(dml_MLIIVM$predictions$ml_r0) # predictions of r_0(Z=0, X)
r1_hat <- as.matrix(dml_MLIIVM$predictions$ml_r1) # predictions of r_0(Z=1, X)
r_hat <- z*r1_hat+(1-z)*r0_hat # predictions of r_0
m_hat <- as.matrix(dml_MLIIVM$predictions$ml_m) # predictions of m_o

# cross-fitted RMSE: outcome
tree_y_MLIIVM <- sqrt(mean((y-g_hat)^2))
tree_y_MLIIVM

# cross-fitted RMSE: treatment
tree_d_MLIIVM <- sqrt(mean((d-r_hat)^2))
tree_d_MLIIVM

# cross-fitted RMSE: instrument
tree_z_MLIIVM <- sqrt(mean((z-m_hat)^2))
tree_z_MLIIVM


### boosting ###
dml_MLIIVM = DoubleMLIIVM$new(data_IV, ml_g = boost,
                       ml_m = boost_class, ml_r = boost_class,n_folds=3, subgroups = list(always_takers = FALSE,
                                         never_takers = TRUE))
dml_MLIIVM$fit(store_predictions=TRUE)
dml_MLIIVM$summary()
boost_MLIIVM <- dml_MLIIVM$coef
boost_std_MLIIVM <- dml_MLIIVM$se

# predictions
g0_hat <- as.matrix(dml_MLIIVM$predictions$ml_g0) # predictions of g_0(Z=0, X)
g1_hat <- as.matrix(dml_MLIIVM$predictions$ml_g1) # predictions of g_0(Z=1, X)
g_hat <- z*g1_hat+(1-z)*g0_hat # predictions of g_0
r0_hat <- as.matrix(dml_MLIIVM$predictions$ml_r0) # predictions of r_0(Z=0, X)
r1_hat <- as.matrix(dml_MLIIVM$predictions$ml_r1) # predictions of r_0(Z=1, X)
r_hat <- z*r1_hat+(1-z)*r0_hat # predictions of r_0
m_hat <- as.matrix(dml_MLIIVM$predictions$ml_m) # predictions of m_o

# cross-fitted RMSE: outcome
boost_y_MLIIVM <- sqrt(mean((y-g_hat)^2))
boost_y_MLIIVM

# cross-fitted RMSE: treatment
boost_d_MLIIVM <- sqrt(mean((d-r_hat)^2))
boost_d_MLIIVM

# cross-fitted RMSE: instrument
boost_z_MLIIVM <- sqrt(mean((z-m_hat)^2))
boost_z_MLIIVM
```

```{r}
table <- matrix(0, 5, 4)
table[1,1:4]   <- c(lasso_MLIIVM,forest_MLIIVM,tree_MLIIVM,boost_MLIIVM)
table[2,1:4]   <- c(lasso_std_MLIIVM,forest_std_MLIIVM,tree_std_MLIIVM,boost_std_MLIIVM)
table[3,1:4]   <- c(lasso_y_MLIIVM,forest_y_MLIIVM,tree_y_MLIIVM,boost_y_MLIIVM)
table[4,1:4]   <- c(lasso_d_MLIIVM,forest_d_MLIIVM,tree_d_MLIIVM,boost_d_MLIIVM)
table[5,1:4]   <- c(lasso_z_MLIIVM,forest_z_MLIIVM,tree_z_MLIIVM,boost_z_MLIIVM)
rownames(table) <- c("Estimate","Std.Error","RMSE Y","RMSE D","RMSE Z")
colnames(table) <- c("Lasso","Random Forest","Trees","Boosting")
tab<- xtable(table, digits = 2)
tab
```

We report results based on four ML methods for estimating the nuisance functions used in
forming the orthogonal estimating equations. We find again that the estimates of the treatment effect are stable across ML methods. The estimates are highly significant, hence we would reject the hypothesis
that the effect of 401(k) participation has no effect on financial health.

We might rerun the model using the best ML method for each equation to get a final estimate for the treatment effect of participation:

```{r}
lgr::get_logger("mlr3")$set_threshold("warn")
dml_MLIIVM = DoubleMLIIVM$new(data_IV, ml_g = randomForest,
                       ml_m = lasso_class, ml_r = lasso_class,n_folds=5, subgroups = list(always_takers = FALSE,
                                         never_takers = TRUE))
dml_MLIIVM$fit(store_predictions=TRUE)
dml_MLIIVM$summary()
best_MLIIVM <- dml_MLIIVM$coef
best_std_MLIIVM <- dml_MLIIVM$se
```

