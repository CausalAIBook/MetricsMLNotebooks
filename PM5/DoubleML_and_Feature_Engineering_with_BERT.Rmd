---
title: An R Markdown document converted from "PM5/DoubleML_and_Feature_Engineering_with_BERT.irnb"
output: html_document
---

# BERT

**Bidirectional Encoder Representations from Transformers.**

_ | _
- | -
![alt](https://pytorch.org/assets/images/bert1.png) | ![alt](https://pytorch.org/assets/images/bert2.png)


### **Overview**

BERT was released together with the paper [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805) by Jacob Devlin *et al.* The model is based on the Transformer architecture introduced in [Attention Is All You Need](https://arxiv.org/abs/1706.03762) by Ashish Vaswani *et al.* and has led to significant improvements in a wide range of natural language tasks.

At the highest level, BERT maps from a block of text to a numeric vector which summarizes the relevant information in the text.

What is remarkable is that numeric summary is sufficiently informative that, for example, the numeric summary of a paragraph followed by a reading comprehension question contains all the information necessary to satisfactorily answer the question.

#### **Transfer Learning**

BERT is a great example of a paradigm called *transfer learning*, which has proved very effective in recent years. In the first step, a network is trained on an unsupervised task using massive amounts of data. In the case of BERT, it was trained to predict missing words and to detect when pairs of sentences are presented in reversed order using all of Wikipedia. This was initially done by Google, using intense computational resources.

Once this network has been trained, it is then used to perform many other supervised tasks using only limited data and computational resources: for example, sentiment classification in tweets or quesiton answering. The network is re-trained to perform these other tasks in such a way that only the final, output parts of the network are allowed to adjust by very much, so that most of the "information'' originally learned the network is preserved. This process is called *fine tuning*.

##Getting to know BERT

BERT, and many of its variants, are made avialable to the public by the open source [Huggingface Transformers](https://huggingface.co/transformers/) project. This is an amazing resource, giving researchers and practitioners easy-to-use access to this technology.

In order to use BERT for modeling, we simply need to download the pre-trained neural network and fine tune it on our dataset, which is illustrated below.

```{r}
install.packages("remotes")
remotes::install_github("rstudio/tensorflow")
install.packages("dplyr")
install.packages("DBI")
install.packages("ggplot2")
install.packages("reticulate")
install.packages("readr")
install.packages("stringr")
install.packages("tidyr")
install.packages("purrr")
install.packages("glmnet")
install.packages("caret")
install.packages("keras")
```

##Getting to know BERT

BERT, and many of its variants, are made avialable to the public by the open source [Huggingface Transformers](https://huggingface.co/transformers/) project. This is an amazing resource, giving researchers and practitioners easy-to-use access to this technology.

In order to use BERT for modeling, we simply need to download the pre-trained neural network and fine tune it on our dataset, which is illustrated below.

```{r}
library(reticulate)
library(ggplot2)
library(DBI)
library(dplyr)
theme_set(theme_bw())
```

```{r}
use_python("/usr/bin/python3", required = TRUE)  # Adjust the path as needed
```

```{r}
py_run_string('
import tensorflow as tf
import numpy as np
import pandas as pd
from transformers import BertTokenizer, TFBertModel
import warnings
warnings.simplefilter("ignore")
')
```

```{r}
# Check GPU availability
# py_run_string('
# device_name = tf.test.gpu_device_name()
# if device_name != "/device:GPU:0":
#   raise SystemError("GPU device not found")
# print("Found GPU at:", device_name)
# ')
```

```{r}
ssq <- function(x) sum(x * x)

get_r2 <- function(y, yhat) {
  resids <- yhat - y
  flucs <- y - mean(y)
  rss <- ssq(resids)
  tss <- ssq(flucs)
  cat(sprintf("RSS: %f, TSS + MEAN^2: %f, TSS: %f, R^2: %f", rss, tss + mean(y)^2, tss, 1 - rss/tss))
}
```

```{r}
py_run_string('
tokenizer = BertTokenizer.from_pretrained("bert-base-uncased")
bert = TFBertModel.from_pretrained("bert-base-uncased")
')
```

### Tokenization

The first step in using BERT (or any similar text embedding tool) is to *tokenize* the data. This step standardizes blocks of text, so that meaningless differences in text presentation don't affect the behavior of our algorithm.

Typically the text is transformed into a sequence of 'tokens,' each of which corresponds to a numeric code.

```{r}
py_run_string('
s = "What happens to this string?"
tensors = tokenizer.encode_plus(s, add_special_tokens = True, return_tensors = "tf")
output = bert(tensors)
')
```

```{r}
# Let's try it out!
s <- "What happens to this string?"
py_run_string
input_ids <- py$tensors$input_ids
attention_mask <- py$tensors$attention_mask
token_type_ids <- py$tensors$token_type_ids

print(sprintf('Original String: "%s"', s))
print("Numeric encoding: ")
print(list(
  input_ids = input_ids,
  attention_mask = attention_mask,
  token_type_ids = token_type_ids
))
# What does this mean?
py_run_string('tokens = tokenizer.convert_ids_to_tokens(tensors["input_ids"].numpy().flatten().tolist())')
tokens <- py$tokens
print("Actual tokens:")
print(tokens)
```

### BERT in a nutshell

Once we have our numeric tokens, we can simply plug them into the BERT network and get a numeric vector summary. Note that in applications, the BERT summary will be "fine tuned" to a particular task, which hasn't happened yet.

```{r}
# Load the reticulate library
library(reticulate)

input_text <- "What happens to this string?"


cat(sprintf("Input: \"%s\"\n\n", input_text))

py_run_string(sprintf('
tensors_tf = tokenizer("%s", return_tensors="tf")
output = bert(tensors_tf)
', input_text))

output <- py$output

py_run_string('
from pprint import pformat
output_type = str(type(output["pooler_output"]))
output_shape = output["pooler_output"].shape
output_preview = pformat(output["pooler_output"].numpy())
')

output_type <- py$output_type
output_shape <- py$output_shape
output_preview <- py$output_preview

cat(sprintf(
"Output type: %s\n\nOutput shape: %s\n\nOutput preview: %s\n",
output_type,
paste(output_shape, collapse=", "),
output_preview
))
```

# A practical introduction to BERT

In the next part of the notebook, we are going to explore how a tool like BERT may be useful for causal inference.

In particular, we are going to apply BERT to a subset of data from the Amazon marketplace consisting of roughly 10,000 listings for products in the toy category. Each product comes with a text description, a price, and a number of times reviewed (which we'll use as a proxy for demand / market share).

For more information on the dataset, checkout the [Dataset README](https://github.com/CausalAIBook/MetricsMLNotebooks/blob/main/data/amazon_toys.md).

**For thought**:
What are some issues you may anticipate when using number of reviews as a proxy for demand or market share?

### Getting to know the data

First, we'll download and clean up the data, and do some preliminary inspection.

```{r}
library(readr)
library(stringr)
library(tidyr)
library(purrr)
data_url <- "https://github.com/CausalAIBook/MetricsMLNotebooks/raw/main/data/amazon_toys.csv"
data <- read_csv(data_url, show_col_types = FALSE)
problems(data)

data <- data %>%
  mutate(
    number_of_reviews = as.numeric(str_replace_all(number_of_reviews, ",", ""))
  )
```

```{r}
data <- data %>%
  mutate(
    number_of_reviews = as.numeric(str_replace_all(number_of_reviews, "\\D+", "")),
    price = as.numeric(str_extract(price, "\\d+\\.?\\d*"))
  ) %>%
  filter(number_of_reviews > 0) %>%
  mutate(
    ln_p = log(price),
    ln_q = log(number_of_reviews / sum(number_of_reviews)),
    text = str_c(product_name, manufacturer, product_description, sep = " | ")
  ) %>%
  select(text, ln_p, ln_q, amazon_category_and_sub_category) %>%
  drop_na()
print(head(data))
data$text_num_words <- str_split(data$text, "\\s+") %>% map_int(length)
print(quantile(data$text_num_words, 0.99, na.rm = TRUE))
```

```{r}
ggplot(data, aes(x = text_num_words)) +
  geom_density() +
  labs(title = "Density Plot of Text Lengths in Words")
```

Let's make a two-way scatter plot of prices and (proxied) market shares.

```{r}
p1 <- ggplot(data, aes(x = ln_p, y = ln_q)) +
  geom_point() +
  geom_smooth(method = "lm", color = "red") +
  labs(title = "Scatter Plot with Regression Line")
print(p1)
```

```{r}
p2 <- ggplot(data, aes(x = ln_p, y = ln_q)) +
  geom_smooth(method = "lm", color = "red") +
  labs(title = "Regression Line Only")
print(p2)
```

```{r}
model <- lm(ln_q ~ ln_p, data = data)
elasticity <- coef(model)["ln_p"]
se <- summary(model)$coefficients["ln_p", "Std. Error"]
r_squared_adj <- summary(model)$adj.r.squared
cat(sprintf("Elasticity: %f, SE: %f, R2: %f\n\n", elasticity, se, r_squared_adj))
conf_intervals <- confint(model, c("(Intercept)", "ln_p"), level = 0.95)
print(conf_intervals)
```

Let's begin with a simple prediction task. We will discover how well can we explain the price of these products using their textual descriptions.

```{r}
install.packages("caTools")
install.packages("base")
library(caTools)
```

```{r}
library(caTools)
set.seed(124)
split <- sample.split(Y = data$ln_p, SplitRatio = 0.8)
train_main <- data[split, ]
holdout <- data[!split, ]
split_main <- sample.split(Y = train_main$ln_p, SplitRatio = 0.75)
train <- train_main[split_main, ]
val <- train_main[!split_main, ]
```

```{r}
library(reticulate)
use_python("/usr/bin/python3", required = TRUE)
py_run_string('import tensorflow as tf')

py$train_texts <- train$text
train_tensors <- py_run_string("
tensors = tokenizer(
    list(train_texts),
    padding=True,
    truncation=True,
    max_length=128,
    return_tensors='tf'
)")
train_tensors <- py$tensors

py$val_texts <- val$text
val_tensors <- py_run_string("
val_tensors = tokenizer(
    list(val_texts),
    padding=True,
    truncation=True,
    max_length=128,
    return_tensors='tf'
)")
val_tensors <- py$val_tensors

py$holdout_texts <- holdout$text
tensors_holdout <- py_run_string("
tensors_holdout = tokenizer(
    list(holdout_texts),
    padding=True,
    truncation=True,
    max_length=128,
    return_tensors='tf'
)")
tensors_holdout <- py$tensors_holdout
ln_p <- train$ln_p
ln_q <- train$ln_q
val_ln_p <- val$ln_p
val_ln_q <- val$ln_q
```

```{r}
ln_p <- train$ln_p
ln_q <- train$ln_q
val_ln_p <- val$ln_p
val_ln_q <- val$ln_q
```

# Using BERT as Feature Extractor

```{r}
library(reticulate)
#Sys.setenv(RETICULATE_PYTHON = "/usr/bin/python")
library(keras)
#install_keras()
```

```{r}
library(caTools)
library(dplyr)
library(readr)
library(reticulate)
library(keras)
library(caret)
library(glmnet)
library(stringr)

use_python("/usr/bin/python3", required = TRUE)
py_run_string('import tensorflow as tf')
py_run_string('from transformers import BertTokenizer, TFBertModel')
py_run_string('
tokenizer = BertTokenizer.from_pretrained("bert-base-uncased")
bert_model = TFBertModel.from_pretrained("bert-base-uncased")
')

py$train_texts <- train$text
train_tensors <- py_run_string("
tensors = tokenizer(
    list(train_texts),
    padding=True,
    truncation=True,
    max_length=128,
    return_tensors='tf'
)")
train_tensors <- py$tensors

py$val_texts <- val$text
val_tensors <- py_run_string("
val_tensors = tokenizer(
    list(val_texts),
    padding=True,
    truncation=True,
    max_length=128,
    return_tensors='tf'
)")
val_tensors <- py$val_tensors

py$holdout_texts <- holdout$text
tensors_holdout <- py_run_string("
tensors_holdout = tokenizer(
    list(holdout_texts),
    padding=True,
    truncation=True,
    max_length=128,
    return_tensors='tf'
)")
tensors_holdout <- py$tensors_holdout

ln_p <- train$ln_p
val_ln_p <- val$ln_p
holdout_ln_p <- holdout$ln_p

py_run_string('
import tensorflow as tf
from transformers import TFBertModel

# Define the input layers
input_ids = tf.keras.layers.Input(shape=(128,), dtype=tf.int32, name="input_ids")
token_type_ids = tf.keras.layers.Input(shape=(128,), dtype=tf.int32, name="token_type_ids")
attention_mask = tf.keras.layers.Input(shape=(128,), dtype=tf.int32, name="attention_mask")

# Load the pre-trained BERT model
bert_model = TFBertModel.from_pretrained("bert-base-uncased")
outputs = bert_model(input_ids=input_ids, token_type_ids=token_type_ids, attention_mask=attention_mask)

# Define the embedding model
embedding_model = tf.keras.models.Model(inputs=[input_ids, token_type_ids, attention_mask], outputs=outputs.last_hidden_state[:, 0, :])
')

py_run_string('
import numpy as np
embeddings = embedding_model.predict({
    "input_ids": tf.convert_to_tensor(tensors["input_ids"]),
    "token_type_ids": tf.convert_to_tensor(tensors["token_type_ids"]),
    "attention_mask": tf.convert_to_tensor(tensors["attention_mask"])
})
')

embeddings <- py$embeddings

py$ln_p <- ln_p
py_run_string('
from sklearn.linear_model import LassoCV
from sklearn.model_selection import KFold
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import make_pipeline

lcv = make_pipeline(StandardScaler(), LassoCV(cv=KFold(n_splits=5, shuffle=True, random_state=123), random_state=123))
lcv.fit(embeddings, ln_p)
')

py_run_string('
embeddings_val = embedding_model.predict({
    "input_ids": tf.convert_to_tensor(val_tensors["input_ids"]),
    "token_type_ids": tf.convert_to_tensor(val_tensors["token_type_ids"]),
    "attention_mask": tf.convert_to_tensor(val_tensors["attention_mask"])
})
val_predictions = lcv.predict(embeddings_val)
')

val_predictions <- py$val_predictions

r2_val <- caret::R2(val_predictions, val_ln_p)

py_run_string('
embeddings_holdout = embedding_model.predict({
    "input_ids": tf.convert_to_tensor(tensors_holdout["input_ids"]),
    "token_type_ids": tf.convert_to_tensor(tensors_holdout["token_type_ids"]),
    "attention_mask": tf.convert_to_tensor(tensors_holdout["attention_mask"])
})
holdout_predictions = lcv.predict(embeddings_holdout)
')

holdout_predictions <- py$holdout_predictions

r2_holdout <- caret::R2(holdout_predictions, holdout_ln_p)

print(r2_val)
print(r2_holdout)
ln_p_hat_holdout <- holdout_predictions
```

# Linear Probing: Training Only Final Layer after BERT

```{r}
### Now let's prepare our model

from tensorflow.keras import Model, Input
from tensorflow.keras.layers import Dense, Dropout, Concatenate
import tensorflow_addons as tfa
from tensorflow.keras import regularizers

tf.keras.utils.set_random_seed(123)

input_ids = Input(shape=(128,), dtype=tf.int32)
token_type_ids = Input(shape=(128,), dtype=tf.int32)
attention_mask = Input(shape=(128,), dtype=tf.int32)

# # First we compute the text embedding
Z = bert(input_ids, token_type_ids, attention_mask)

for layer in bert.layers:
    layer.trainable=False
    for w in layer.weights: w._trainable=False

# # We want the "pooled / summary" embedding, not individual word embeddings
Z = Z[1]

# # Then we do a regular regression
# Z = Dropout(0.2)(Z)
ln_p_hat = Dense(1, activation='linear',
                 kernel_regularizer=regularizers.L2(1e-3))(Z)

PricePredictionNetwork = Model([
                                input_ids,
                                token_type_ids,
                                attention_mask,
                                ], ln_p_hat)
PricePredictionNetwork.compile(
    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),
    loss=tf.keras.losses.MeanSquaredError(),
    metrics=tfa.metrics.RSquare(),
)
PricePredictionNetwork.summary()
```

```{r}
from livelossplot import PlotLossesKeras

tf.keras.utils.set_random_seed(123)
earlystopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)
modelcheckpoint = tf.keras.callbacks.ModelCheckpoint("/content/gdrive/MyDrive/pweights.hdf5", monitor='val_loss', save_best_only=True, save_weights_only=True)

PricePredictionNetwork.fit(
                x= [tensors['input_ids'],
                    tensors['token_type_ids'],
                    tensors['attention_mask'],],
                y=ln_p,
                validation_data = (
                    [val_tensors['input_ids'],
                     val_tensors['token_type_ids'],
                     val_tensors['attention_mask']], val_ln_p
                ),
                epochs=5,
                callbacks = [earlystopping, modelcheckpoint,
                             PlotLossesKeras(groups = {'train_loss': ['loss'], 'train_rsq':['r_square'], 'val_loss': ['val_loss'], 'val_rsq': ['val_r_square']})],
                batch_size=16,
                shuffle=True)
```

# Fine Tuning starting from the Linear Probing Trained Weights

Now we train the whole network, initializing the weights based on the result of the linear probing phase in the previous section.

```{r}
### Now let's prepare our model

from tensorflow.keras import Model, Input
from tensorflow.keras.layers import Dense, Dropout, Concatenate
import tensorflow_addons as tfa
from tensorflow.keras import regularizers

tf.keras.utils.set_random_seed(123)

input_ids = Input(shape=(128,), dtype=tf.int32)
token_type_ids = Input(shape=(128,), dtype=tf.int32)
attention_mask = Input(shape=(128,), dtype=tf.int32)

# # First we compute the text embedding
Z = bert(input_ids, token_type_ids, attention_mask)

for layer in bert.layers:
    layer.trainable=True
    for w in layer.weights: w._trainable=True

# # We want the "pooled / summary" embedding, not individual word embeddings
Z = Z[1]

# # Then we do a regularized linear regression
ln_p_hat = Dense(1, activation='linear',
                 kernel_regularizer=regularizers.L2(1e-3))(Z)

PricePredictionNetwork = Model([
                                input_ids,
                                token_type_ids,
                                attention_mask,
                                ], ln_p_hat)
PricePredictionNetwork.compile(
    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-5),
    loss=tf.keras.losses.MeanSquaredError(),
    metrics=tfa.metrics.RSquare(),
)
PricePredictionNetwork.summary()
```

```{r}
PricePredictionNetwork.load_weights("/content/gdrive/MyDrive/pweights.hdf5")
```

```{r}
from livelossplot import PlotLossesKeras

tf.keras.utils.set_random_seed(123)

earlystopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)
modelcheckpoint = tf.keras.callbacks.ModelCheckpoint("/content/gdrive/MyDrive/pweights.hdf5", monitor='val_loss', save_best_only=True, save_weights_only=True)

PricePredictionNetwork.fit(
                x= [tensors['input_ids'],
                    tensors['token_type_ids'],
                    tensors['attention_mask'],],
                y=ln_p,
                validation_data = (
                    [val_tensors['input_ids'],
                     val_tensors['token_type_ids'],
                     val_tensors['attention_mask']], val_ln_p
                ),
                epochs=10,
                callbacks = [earlystopping, modelcheckpoint,
                             PlotLossesKeras(groups = {'train_loss': ['loss'], 'train_rsq':['r_square'], 'val_loss': ['val_loss'], 'val_rsq': ['val_r_square']})],
                batch_size=16,
                shuffle=True)
```

```{r}
PricePredictionNetwork.load_weights("/content/gdrive/MyDrive/pweights.hdf5")
```

```{r}
# Compute predictions
ln_p_hat_holdout = PricePredictionNetwork.predict([
                                                   tensors_holdout['input_ids'],
                                                   tensors_holdout['token_type_ids'],
                                                   tensors_holdout['attention_mask'],
                                                   ])
```

```{r}
print('Neural Net R^2, Price Prediction:')
get_r2(holdout['ln_p'], ln_p_hat_holdout)
```

```{r}
import matplotlib.pyplot as plt
plt.hist(ln_p_hat_holdout)
plt.show()
```

Now, let's go one step further and construct a DML estimator of the average price elasticity. In particular, we will model market share $q_i$ as
$$\ln q_i = \alpha + \beta \ln p_i + \psi(d_i) + \epsilon_i,$$ where $d_i$ denotes the description of product $i$ and $\psi$ is the composition of text embedding and a linear layer.

```{r}
## Build the quantity prediction network

tf.keras.utils.set_random_seed(123)

# Initialize new BERT model from original
bert2 = TFBertModel.from_pretrained("bert-base-uncased")

# for layer in bert2.layers:
#     layer.trainable=False
#     for w in layer.weights: w._trainable=False

# Define inputs
input_ids = Input(shape=(128,), dtype=tf.int32)
token_type_ids = Input(shape=(128,), dtype=tf.int32)
attention_mask = Input(shape=(128,), dtype=tf.int32)

# First we compute the text embedding
Z = bert2(input_ids, token_type_ids, attention_mask)

# We want the "pooled / summary" embedding, not individual word embeddings
Z = Z[1]

ln_q_hat = Dense(1, activation='linear', kernel_regularizer=regularizers.L2(1e-3))(Z)

# Compile model and optimization routine
QuantityPredictionNetwork = Model([
                                   input_ids,
                                   token_type_ids,
                                   attention_mask,
                                   ], ln_q_hat)
QuantityPredictionNetwork.compile(
    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-5),
    loss=tf.keras.losses.MeanSquaredError(),
    metrics=tfa.metrics.RSquare(),
)
QuantityPredictionNetwork.summary()
```

```{r}
## Fit the quantity prediction network in the main sample
tf.keras.utils.set_random_seed(123)

earlystopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)
modelcheckpoint = tf.keras.callbacks.ModelCheckpoint("/content/gdrive/MyDrive/qweights.hdf5", monitor='val_loss', save_best_only=True, save_weights_only=True)

QuantityPredictionNetwork.fit(
                [
                 tensors['input_ids'],
                 tensors['token_type_ids'],
                 tensors['attention_mask'],
                 ],
                ln_q,
                validation_data = (
                    [val_tensors['input_ids'],
                 val_tensors['token_type_ids'],
                 val_tensors['attention_mask']], val_ln_q
                ),
                epochs=10,
                callbacks = [earlystopping, modelcheckpoint,
                             PlotLossesKeras(groups = {'train_loss': ['loss'], 'train_rsq':['r_square'], 'val_loss': ['val_loss'], 'val_rsq': ['val_r_square']})],
                batch_size=16,
                shuffle=True)
```

```{r}
QuantityPredictionNetwork.load_weights("/content/gdrive/MyDrive/qweights.hdf5")
```

```{r}
## Predict in the holdout sample, residualize and regress

ln_q_hat_holdout = QuantityPredictionNetwork.predict([
                                                      tensors_holdout['input_ids'],
                                                      tensors_holdout['token_type_ids'],
                                                      tensors_holdout['attention_mask'],
                                                      ])
```

```{r}
print('Neural Net R^2, Quantity Prediction:')
get_r2(holdout['ln_q'], ln_q_hat_holdout)
```

```{r}
# Compute residuals
r_p = holdout["ln_p"] - ln_p_hat_holdout.reshape((-1,))
r_q = holdout["ln_q"] - ln_q_hat_holdout.reshape((-1,))

# Regress to obtain elasticity estimate
beta = np.mean(r_p * r_q) / np.mean(r_p * r_p)

# standard error on elastiticy estimate
se = np.sqrt(np.mean( (r_p* r_q)**2)/(np.mean(r_p*r_p)**2)/holdout["ln_p"].size)

print('Elasticity of Demand with Respect to Price: {}'.format(beta))
print('Standard Error: {}'.format(se))
```

# Heterogeneous Elasticities within Major Product Categories

We now look at the major product categories that have many products and we investigate whether the "within group" price elasticities

```{r}
holdout['category'] = holdout['amazon_category_and_sub_category'].str.split('>').apply(lambda x: x[0])
```

```{r}
# Elasticity within the main product categories
sql.run("""
  SELECT category, COUNT(*)
  FROM holdout
  GROUP BY 1
  HAVING COUNT(*)>=100
  ORDER BY 2 desc
""")
```

```{r}
main_cats = sql.run("""
  SELECT category
  FROM holdout
  GROUP BY 1
  HAVING COUNT(*)>=100
""")['category']

dfs = []
for cat in main_cats:
    r_p = holdout[holdout['category'] == cat]["ln_p"] - ln_p_hat_holdout.reshape((-1,))[holdout['category'] == cat]
    r_q = holdout[holdout['category'] == cat]["ln_q"] - ln_q_hat_holdout.reshape((-1,))[holdout['category'] == cat]
    # Regress to obtain elasticity estimate
    beta = np.mean(r_p * r_q) / np.mean(r_p * r_p)

    # standard error on elastiticy estimate
    se = np.sqrt(np.mean( (r_p* r_q)**2)/(np.mean(r_p*r_p)**2)/holdout["ln_p"].size)

    df = pd.DataFrame({'point': beta, 'se': se, 'lower': beta - 1.96 * se, 'upper': beta + 1.96 * se}, index=[0])
    df['category'] = cat
    df['N'] = holdout[holdout['category'] == cat].shape[0]
    dfs.append(df)

df = pd.concat(dfs)
df
```

## Clustering Products

In this final part of the notebook, we'll illustrate how the BERT text embeddings can be used to cluster products based on their  descriptions.

Intiuitively, our neural network has now learned which aspects of the text description are relevant to predict prices and market shares.
We can therefore use the embeddings produced by our network to cluster products, and we might expect that the clusters reflect market-relevant information.

In the following block of cells, we compute embeddings using our learned models and cluster them using $k$-means clustering with $k=10$. Finally, we will explore how the estimated price elasticity differs across clusters.

### Overview of **$k$-means clustering**
The $k$-means clustering algorithm seeks to divide $n$ data vectors into $k$ groups, each of which contain points that are "close together."

In particular, let $C_1, \ldots, C_k$ be a partitioning of the data into $k$ disjoint, nonempty subsets (clusters), and define
$$\bar{C_i}=\frac{1}{\#C_i}\sum_{x \in C_i} x$$
to be the *centroid* of the cluster $C_i$. The $k$-means clustering score $\mathrm{sc}(C_1 \ldots C_k)$ is defined to be
$$\mathrm{sc}(C_1 \ldots C_k) = \sum_{i=1}^k \sum_{x \in C_i} \left(x - \bar{C_i}\right)^2.$$

The $k$-means clustering is then defined to be any partitioning $C^*_1 \ldots C^*_k$ that minimizes the score $\mathrm{sc}(-)$.

```{r}
## STEP 1: Compute embeddings

input_ids = Input(shape=(128,), dtype=tf.int32)
token_type_ids = Input(shape=(128,), dtype=tf.int32)
attention_mask = Input(shape=(128,), dtype=tf.int32)

Y1 = bert(input_ids, token_type_ids, attention_mask)[1]
Y2 = bert2(input_ids, token_type_ids, attention_mask)[1]
Y = Concatenate()([Y1,Y2])

embedding_model = Model([input_ids, token_type_ids, attention_mask], Y)

embeddings = embedding_model.predict([tensors_holdout['input_ids'],
                                      tensors_holdout['token_type_ids'],
                                      tensors_holdout['attention_mask']])
```

### Dimension reduction and the **Johnson-Lindenstrauss transform**

Our learned embeddings have dimension in the $1000$s, and $k$-means clustering is often an expensive operation. To improve the situation, we will use a neat trick that is used extensively in machine learning applications: the *Johnson-Lindenstrauss transform*.

This trick involves finding a low-dimensional linear projection of the embeddings that approximately preserves pairwise distances.

In fact, Johnson and Lindenstrauss proved a much more interesting statement: a Gaussian random matrix will *almost always* approximately preserve pairwise distances.


```{r}
# STEP 2 Make low-dimensional projections
from sklearn.random_projection import GaussianRandomProjection

jl = GaussianRandomProjection(eps=.25)
embeddings_lowdim = jl.fit_transform(embeddings)
```

```{r}
# STEP 3 Compute clusters
from sklearn.cluster import KMeans

k_means = KMeans(n_clusters=10)
k_means.fit(embeddings_lowdim)
cluster_ids = k_means.labels_
```

```{r}
# STEP 4 Regress within each cluster

betas = np.zeros(10)
ses = np.zeros(10)

r_p = holdout["ln_p"] - ln_p_hat_holdout.reshape((-1,))
r_q = holdout["ln_q"] - ln_q_hat_holdout.reshape((-1,))

for c in range(10):

  r_p_c = r_p[cluster_ids == c]
  r_q_c = r_q[cluster_ids == c]

  # Regress to obtain elasticity estimate
  betas[c] = np.mean(r_p_c * r_q_c) / np.mean(r_p_c * r_p_c)

  # standard error on elastiticy estimate
  ses[c] = np.sqrt(np.mean( (r_p_c * r_q_c)**2)/(np.mean(r_p_c*r_p_c)**2)/r_p_c.size)
```

```{r}
# STEP 5 Plot
from matplotlib import pyplot as plt

plt.bar(range(10), betas, yerr = 1.96 * ses)
```

