{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Autoencoders\n",
        "\n",
        "In this notebook, we'll introduce and explore \"autoencoders,\" which are a very successful family of models in modern deep learning. In particular we will:\n",
        "\n",
        "\n",
        "1.   Illustrate the connection between autoencoders and classical *Principal Component Analysis (PCA)*\n",
        "3.   Train a non-linear auto-encoder that uses a deep neural network\n",
        "\n",
        "### Overview\n",
        "As explained in the text, autoencoders are a way of discovering *latent, low-dimensional structure* in a dataset. In particular, a random data vector $X \\in \\mathbb{R}^d$ can be said to have low-dimensional structure if we can find some functions $f: \\mathbb{R}^d \\to \\mathbb{R}^k$ and $g: \\mathbb{R}^k \\to \\mathbb{R}^d$, with $k \\ll d$, such that $$g(f(X)) \\approx X.$$\n",
        "In other words, $f(X)$ is a parsimonious, $k$-dimensional representation of $X$ that contains all of the information necessary to approximately reconstruct the full vector $X$. Traditionally, $f(X)$ is called an *encoding* of $X$.\n",
        "\n",
        "It turns out that this is meaningless unless we restrict what kinds of functions $f$ and $g$ are allowed to be, because it's possible to write down some (completely ugly) one-to-one function $\\mathbb{R}^d \\to \\mathbb{R}^1$ for any $d$. This gives rise to the notion of *autoencoders* where, given some sets of reasonable functions $F$ and $G$, we aim to minimize\n",
        "$$\\mathbb{E}[\\mathrm{loss}(X, f(g(X))]$$ over functions $f \\in F$ and $g \\in G$. As usual, this is done by minimizing the sample analog.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "QkLbE3GXm1Jo"
      },
      "id": "QkLbE3GXm1Jo"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Linear Autoencoders and PCA: Practice\n",
        "\n",
        "It turns out that linear autoencoders are the same as PCA. Let's do a small sanity check to verify this. In particular, let's perform PCA two ways: first using a standard (linear algebra) toolkit, and second as a linear autoencoder using a neural network library.\n",
        "If all goes well, they should give you the same reconstructions!\n",
        "\n",
        "To make it a bit more fun, we will use the [*Labeled Faces in the Wild*](https://www.kaggle.com/jessicali9530/celeba-dataset) dataset which consists of standardized images of roughly 5,000 celebrities' faces. In this data, PCA amounts to looking for a small number of \"proto-faces\" such that a linear combination of them can accurately reconstruct any celebrity's face."
      ],
      "metadata": {
        "id": "lnwQdyzmm8UU"
      },
      "id": "lnwQdyzmm8UU"
    },
    {
      "cell_type": "code",
      "source": [
        "install.packages(\"keras\")"
      ],
      "metadata": {
        "id": "nf4aybFuwTft"
      },
      "id": "nf4aybFuwTft",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "install.packages(\"reticulate\")\n",
        "install.packages(\"abind\")\n",
        "install.packages(\"grid\")\n",
        "install.packages(\"gridExtra\")\n",
        "install.packages(\"dplyr\")\n",
        "install.packages(\"purrr\")\n",
        "install.packages(\"reshape2\")"
      ],
      "metadata": {
        "id": "ID08-PSOeKRf"
      },
      "id": "ID08-PSOeKRf",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "library(reticulate)\n",
        "\n",
        "# Import Python's sklearn.datasets\n",
        "sklearn <- import(\"sklearn.datasets\")\n",
        "\n",
        "# Fetch the dataset\n",
        "faces <- sklearn$fetch_lfw_people()\n",
        "\n",
        "# Access the images and reshape the data similar to Python's reshape method\n",
        "n_examples <- dim(faces$images)[1]\n",
        "height <- dim(faces$images)[2]\n",
        "width <- dim(faces$images)[3]\n",
        "design_matrix <- array_reshape(faces$images, c(n_examples, height * width))\n",
        "\n",
        "n_features <- dim(design_matrix)[2]\n",
        "\n",
        "# Print the dataset details\n",
        "cat(sprintf(\"Labeled Faces in the Wild Dataset:\\n Number of examples: %d\\n Number of features: %d\\n Image height: %d\\n Image width: %d\",\n",
        "             n_examples, n_features, height, width))\n"
      ],
      "metadata": {
        "id": "Z_ZpuBEBfCeH"
      },
      "id": "Z_ZpuBEBfCeH",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PX_E23v-5yZY"
      },
      "source": [
        "library(ggplot2)\n",
        "library(gridExtra)\n",
        "library(grid)\n",
        "\n",
        "# Find indices where the label is 'Arnold Schwarzenegger'\n",
        "# faces$target uses python style indexing that starts at 0 rather than R style\n",
        "#            indexing that starts at 1, so we subtract 1 so the indexing lines up\n",
        "arnold_labels <- which(faces$target_names == \"Arnold Schwarzenegger\")-1\n",
        "# Get indices of all images corresponding to Arnold\n",
        "arnold_pics <- which(faces$target %in% arnold_labels)\n",
        "\n",
        "plot_faces <- function(images, n_row=2, n_col=3, width, height) {\n",
        "  par(mfrow=c(n_row, n_col), mar=c(0.5, 0.5, 0.5, 0.5))\n",
        "  for (i in seq_len(n_row * n_col)) {\n",
        "    if (i <= length(images)) {\n",
        "      # image needs to be transposed for and then flipped for correct orientation\n",
        "      # using R \"image\"\n",
        "      tmp <- t(images[[i]])\n",
        "      tmp <- tmp[,ncol(tmp):1]\n",
        "      image(tmp, col=gray.colors(256), axes=FALSE, xlab=\"\", ylab=\"\")\n",
        "    }\n",
        "  }\n",
        "}\n",
        "\n",
        "# Ensure arnold_images contains the right amount of data and is not NULL\n",
        "arnold_images <- lapply(arnold_pics[1:min(6, length(arnold_pics))], function(idx) {\n",
        "  faces$images[idx, , ]\n",
        "})\n",
        "\n",
        "plot_faces(arnold_images, n_row = 2, n_col = 3, height = 62, width = 47)\n"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "PX_E23v-5yZY"
    },
    {
      "cell_type": "code",
      "source": [
        "library(stats)\n",
        "\n",
        "# Perform PCA on the design matrix\n",
        "pca <- prcomp(design_matrix, rank. = 128, retx = TRUE, center = TRUE, scale. = FALSE)\n",
        "\n",
        "# Extract the principal components (eigenfaces)\n",
        "eigenfaces <- pca$rotation\n"
      ],
      "metadata": {
        "id": "imSXA7-jsGKl"
      },
      "id": "imSXA7-jsGKl",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sLi4k8t3DrHe"
      },
      "source": [
        "# 2. Plot the first 6 \"eigenfaces,\" the six images whose linear span best explains the variation in our dataset\n",
        "pca_images <- lapply(1:6, function(idx) {\n",
        "  array_reshape(eigenfaces[,idx], c(height,width))\n",
        "})\n",
        "\n",
        "plot_faces(pca_images, height = height, width = width)\n",
        "# we check the first six eigenvectors/projection axes, reshaped (the eigenvectors that captured the highest variation in our dataset of images)\n",
        "# here, eigenvector1 orthog to eigenvector2 and all the others => decorrelation (there's no way to reconstruct eigenvector1 using a linear combination of all the other eigenvectors)"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "sLi4k8t3DrHe"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gmj2lpTfCXKC"
      },
      "source": [
        "reconstruct <- function(image_vector, n_components, eigenfaces) {\n",
        "  components <- eigenfaces[, 1:n_components, drop = FALSE]\n",
        "  compimage <- components %*% (t(components) %*% image_vector)\n",
        "  return(array_reshape(compimage, c(height,width)))\n",
        "}\n",
        "\n",
        "# Select an Arnold image for reconstruction\n",
        "face_vector <- t(design_matrix[arnold_pics[1], , drop = FALSE])\n",
        "\n",
        "# Perform reconstructions with varying number of components\n",
        "reconstructions <- lapply(c(1, 2, 8, 32, 64, 128), function(k) {\n",
        "  reconstruct(face_vector, k, eigenfaces)\n",
        "})\n",
        "\n",
        "# Plot the reconstructed faces\n",
        "plot_faces(reconstructions, height = height, width = width)\n"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "Gmj2lpTfCXKC"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eoZ_BsXYDE7P"
      },
      "source": [
        "# 4. Train linear autoencoder with 64 neurons using Keras\n",
        "# 5. Compare reconstructions of Arnold's face both using MSE and visually"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "eoZ_BsXYDE7P"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "urlMaifVJCDc"
      },
      "source": [
        "library(keras)\n",
        "encoding_dimension <- 64\n",
        "input_image <- layer_input(shape = n_features)\n",
        "encoded <- layer_dense(units = encoding_dimension, activation = 'linear')(input_image)\n",
        "decoded <- layer_dense(units = n_features, activation = 'linear')(encoded)\n",
        "autoencoder <- keras_model(inputs = input_image, outputs = decoded)\n",
        "autoencoder %>% compile(\n",
        "  optimizer = 'adam',\n",
        "  loss = 'mse'\n",
        ")\n",
        "autoencoder %>% fit(\n",
        "  design_matrix,\n",
        "  design_matrix,\n",
        "  epochs = 50,\n",
        "  batch_size = 256,\n",
        "  shuffle = TRUE,\n",
        "  verbose = 0\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "urlMaifVJCDc"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5OTUbWg8NcIE"
      },
      "source": [
        "autoencoder %>% fit(\n",
        "  design_matrix,\n",
        "  design_matrix,\n",
        "  epochs = 50,\n",
        "  batch_size = 256,\n",
        "  shuffle = TRUE,\n",
        "  verbose = 0\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "5OTUbWg8NcIE"
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute neural reconstruction\n",
        "face_vector_flat <- as.numeric(face_vector)\n",
        "reconstruction <- predict(autoencoder, matrix(face_vector_flat, nrow = 1))\n",
        "library(ggplot2)\n",
        "library(gridExtra)\n",
        "library(reshape2)\n",
        "\n",
        "# Do visual comparison\n",
        "image_height <- 62\n",
        "image_width <- 47\n",
        "image1 <- matrix(reconstructions[[4]], nrow = image_height, ncol = image_width)\n",
        "image2 <- t(matrix(reconstruction, nrow = image_width, ncol = image_height))\n",
        "\n",
        "images <- list(image1, image2)\n",
        "plot_faces(images, n_row = 1, n_col = 2, width = image_width, height = image_height)\n",
        "\n",
        "\n",
        "# Do numeric comparison\n",
        "# We also normalize the black/white gradient to take values in [0,1] (divide by 255)\n",
        "img1 <- as.numeric(reconstructions[[4]]) / 255\n",
        "img2 <- as.numeric(reconstruction) / 255\n",
        "mse <- mean((img1 - img2)^2)\n",
        "mse\n"
      ],
      "metadata": {
        "id": "90nSf8Y8yIsl"
      },
      "id": "90nSf8Y8yIsl",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zrdD6A55yJML"
      },
      "source": [
        "## Neural Autoencoders\n",
        "\n",
        "Finally, let's train a nonlinear autoencoder for the same data where $F$ and $G$ are neural networks, and we restrict the dimension to be $k=64$."
      ],
      "id": "zrdD6A55yJML"
    },
    {
      "cell_type": "code",
      "source": [
        "# Use a nonlinear neural network\n",
        "\n",
        "library(tensorflow)\n",
        "n_features <- 2914\n",
        "encoding_dimension <- 64\n",
        "\n",
        "input_image <- layer_input(shape = n_features)\n",
        "encoded <- input_image %>%\n",
        "  layer_dense(units = encoding_dimension, activation = 'relu') %>%\n",
        "  layer_dense(units = encoding_dimension, activation = 'relu')\n",
        "\n",
        "decoded <- encoded %>%\n",
        "  layer_dense(units = encoding_dimension, activation = 'relu') %>%\n",
        "  layer_dense(units = n_features, activation = 'relu')\n",
        "\n",
        "autoencoder <- keras_model(inputs = input_image, outputs = decoded)\n",
        "\n",
        "autoencoder %>% compile(\n",
        "  optimizer = 'adam',\n",
        "  loss = 'mse'\n",
        ")\n",
        "autoencoder %>% fit(\n",
        "  design_matrix,\n",
        "  design_matrix,\n",
        "  epochs = 50,\n",
        "  batch_size = 256,\n",
        "  shuffle = TRUE,\n",
        "  verbose = 0\n",
        ")\n",
        "\n",
        "# Compute neural reconstruction\n",
        "reconstruction <- predict(autoencoder, matrix(face_vector, nrow = 1))\n",
        "\n",
        "# Do visual comparison\n",
        "plot_faces(list(reconstructions[[4]],t(matrix(reconstruction, nrow = image_width, ncol = image_height))), n_row = 1, n_col = 2, width = image_width, height = image_height)\n",
        "\n",
        "# Do numeric comparison\n",
        "# We also normalize the black/white gradient to take values in [0,1] (divide by 255)\n",
        "img1 <- as.numeric(reconstructions[[4]]) / 255\n",
        "img2 <- as.numeric(reconstruction) / 255\n",
        "mse <- mean((img1 - img2)^2)\n",
        "mse\n"
      ],
      "metadata": {
        "id": "KHPoFiS9fuhr"
      },
      "id": "KHPoFiS9fuhr",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "R",
      "language": "R",
      "name": "ir"
    },
    "language_info": {
      "codemirror_mode": "r",
      "file_extension": ".r",
      "mimetype": "text/x-r-source",
      "name": "R",
      "pygments_lexer": "r",
      "version": "4.0.5"
    },
    "papermill": {
      "default_parameters": {},
      "duration": 427.936706,
      "end_time": "2022-04-19T09:13:53.230849",
      "environment_variables": {},
      "exception": null,
      "input_path": "__notebook__.ipynb",
      "output_path": "__notebook__.ipynb",
      "parameters": {},
      "start_time": "2022-04-19T09:06:45.294143",
      "version": "2.3.4"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}