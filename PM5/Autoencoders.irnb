{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {
    "id": "QkLbE3GXm1Jo"
   },
   "source": [
    "# Autoencoders\n",
    "\n",
    "In this notebook, we'll introduce and explore \"autoencoders,\" which are a very successful family of models in modern deep learning. In particular we will:\n",
    "\n",
    "\n",
    "1.   Illustrate the connection between autoencoders and classical *Principal Component Analysis (PCA)*\n",
    "3.   Train a non-linear auto-encoder that uses a deep neural network\n",
    "\n",
    "### Overview\n",
    "As explained in the text, autoencoders are a way of discovering *latent, low-dimensional structure* in a dataset. In particular, a random data vector $X \\in \\mathbb{R}^d$ can be said to have low-dimensional structure if we can find some functions $f: \\mathbb{R}^d \\to \\mathbb{R}^k$ and $g: \\mathbb{R}^k \\to \\mathbb{R}^d$, with $k \\ll d$, such that \n",
    "$$g(f(X)) \\approx X.$$\n",
    "\n",
    "In other words, $f(X)$ is a parsimonious, $k$-dimensional representation of $X$ that contains all of the information necessary to approximately reconstruct the full vector $X$. Traditionally, $f(X)$ is called an *encoding* of $X$.\n",
    "\n",
    "It turns out that this is meaningless unless we restrict what kinds of functions $f$ and $g$ are allowed to be, because it's possible to write down some (completely ugly) one-to-one function $\\mathbb{R}^d \\to \\mathbb{R}^1$ for any $d$. This gives rise to the notion of *autoencoders* where, given some sets of reasonable functions $F$ and $G$, we aim to minimize\n",
    "$$\\mathbb{E}[\\mathrm{loss}(X, f(g(X))]$$\n",
    "over functions $f \\in F$ and $g \\in G$. As usual, this is done by minimizing the sample analog.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {
    "id": "lnwQdyzmm8UU"
   },
   "source": [
    "## Linear Autoencoders and PCA: Practice\n",
    "\n",
    "It turns out that linear autoencoders are the same as PCA. Let's do a small sanity check to verify this. In particular, let's perform PCA two ways: first using a standard (linear algebra) toolkit, and second as a linear autoencoder using a neural network library.\n",
    "If all goes well, they should give you the same reconstructions!\n",
    "\n",
    "To make it a bit more fun, we will use the [*Labeled Faces in the Wild*](https://www.kaggle.com/jessicali9530/celeba-dataset) dataset which consists of standardized images of roughly 5,000 celebrities' faces. In this data, PCA amounts to looking for a small number of \"proto-faces\" such that a linear combination of them can accurately reconstruct any celebrity's face."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {
    "id": "nf4aybFuwTft",
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "install.packages(\"keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {
    "id": "ID08-PSOeKRf",
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "install.packages(\"reticulate\")\n",
    "install.packages(\"abind\")\n",
    "install.packages(\"grid\")\n",
    "install.packages(\"gridExtra\")\n",
    "install.packages(\"dplyr\")\n",
    "install.packages(\"purrr\")\n",
    "install.packages(\"reshape2\")\n",
    "install.packages(\"ggplot2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {
    "id": "Z_ZpuBEBfCeH",
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "library(reticulate)\n",
    "\n",
    "# Import Python's sklearn.datasets\n",
    "sklearn <- import(\"sklearn.datasets\")\n",
    "\n",
    "# Fetch the dataset\n",
    "faces <- sklearn$fetch_lfw_people()\n",
    "\n",
    "# Access the images and reshape the data similar to Python's reshape method\n",
    "n_examples <- dim(faces$images)[1]\n",
    "height <- dim(faces$images)[2]\n",
    "width <- dim(faces$images)[3]\n",
    "design_matrix <- array_reshape(faces$images, c(n_examples, height * width))\n",
    "\n",
    "n_features <- dim(design_matrix)[2]\n",
    "\n",
    "# Print the dataset details\n",
    "cat(sprintf(\n",
    "  paste(\"Labeled Faces in the Wild Dataset:\\n Number of examples: %d\\n \",\n",
    "        \"Number of features: %d\\n Image height: %d\\n Image width: %d\"),\n",
    "  n_examples, n_features, height, width\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {
    "id": "PX_E23v-5yZY",
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "library(ggplot2)\n",
    "library(gridExtra)\n",
    "library(grid)\n",
    "\n",
    "# Find indices where the label is 'Arnold Schwarzenegger'\n",
    "# faces$target uses python style indexing that starts at 0 rather than R style\n",
    "#            indexing that starts at 1, so we subtract 1 so the indexing lines up\n",
    "arnold_labels <- which(faces$target_names == \"Arnold Schwarzenegger\") - 1\n",
    "# Get indices of all images corresponding to Arnold\n",
    "arnold_pics <- which(faces$target %in% arnold_labels)\n",
    "\n",
    "plot_faces <- function(images, n_row = 2, n_col = 3, width, height) {\n",
    "  par(mfrow = c(n_row, n_col), mar = c(0.5, 0.5, 0.5, 0.5))\n",
    "  for (i in seq_len(n_row * n_col)) {\n",
    "    if (i <= length(images)) {\n",
    "      # image needs to be transposed for and then flipped for correct orientation\n",
    "      # using R \"image\"\n",
    "      tmp <- t(images[[i]])\n",
    "      tmp <- tmp[, rev(seq_len(ncol(tmp)))]\n",
    "      image(tmp, col = gray.colors(256), axes = FALSE, xlab = \"\", ylab = \"\")\n",
    "    }\n",
    "  }\n",
    "}\n",
    "\n",
    "# Ensure arnold_images contains the right amount of data and is not NULL\n",
    "arnold_images <- lapply(arnold_pics[seq_len(min(6, length(arnold_pics)))], function(idx) {\n",
    "  faces$images[idx, , ]\n",
    "})\n",
    "\n",
    "plot_faces(arnold_images, n_row = 2, n_col = 3, height = 62, width = 47)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {
    "id": "imSXA7-jsGKl",
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "library(stats)\n",
    "\n",
    "# Perform PCA on the design matrix\n",
    "pca <- prcomp(design_matrix, rank. = 128, retx = TRUE, center = TRUE, scale. = FALSE)\n",
    "\n",
    "# Extract the principal components (eigenfaces)\n",
    "eigenfaces <- pca$rotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {
    "id": "sLi4k8t3DrHe",
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# 2. Plot the first 6 \"eigenfaces,\" the six images whose linear span best explains the variation in our dataset\n",
    "pca_images <- lapply(1:6, function(idx) {\n",
    "  array_reshape(eigenfaces[, idx], c(height, width))\n",
    "})\n",
    "\n",
    "plot_faces(pca_images, height = height, width = width)\n",
    "# we check the first six eigenvectors/projection axes, reshaped (the eigenvectors that\n",
    "# captured the highest variation in our dataset of images)\n",
    "# here, eigenvector1 orthog to eigenvector2 and all the others => decorrelation\n",
    "# (there's no way to reconstruct eigenvector1 using a linear combination of all the other eigenvectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {
    "id": "Gmj2lpTfCXKC",
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "reconstruct <- function(image_vector, n_components, eigenfaces) {\n",
    "  components <- eigenfaces[, 1:n_components, drop = FALSE]\n",
    "  compimage <- components %*% (t(components) %*% image_vector)\n",
    "  return(array_reshape(compimage, c(height, width)))\n",
    "}\n",
    "\n",
    "# Select an Arnold image for reconstruction\n",
    "face_vector <- t(design_matrix[arnold_pics[1], , drop = FALSE])\n",
    "\n",
    "# Perform reconstructions with varying number of components\n",
    "reconstructions <- lapply(c(1, 2, 8, 32, 64, 128), function(k) {\n",
    "  reconstruct(face_vector, k, eigenfaces)\n",
    "})\n",
    "\n",
    "# Plot the reconstructed faces\n",
    "plot_faces(reconstructions, height = height, width = width)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {
    "id": "eoZ_BsXYDE7P",
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# 4. Train linear autoencoder with 64 neurons using Keras\n",
    "# 5. Compare reconstructions of Arnold's face both using MSE and visually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {
    "id": "urlMaifVJCDc",
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "library(keras)\n",
    "\n",
    "encoding_dimension <- 64\n",
    "input_image <- layer_input(shape = n_features)\n",
    "encoded <- layer_dense(units = encoding_dimension, activation = \"linear\")(input_image)\n",
    "decoded <- layer_dense(units = n_features, activation = \"linear\")(encoded)\n",
    "autoencoder <- keras_model(inputs = input_image, outputs = decoded)\n",
    "autoencoder %>% compile(\n",
    "  optimizer = \"adam\",\n",
    "  loss = \"mse\"\n",
    ")\n",
    "autoencoder %>% fit(\n",
    "  design_matrix,\n",
    "  design_matrix,\n",
    "  epochs = 50,\n",
    "  batch_size = 256,\n",
    "  shuffle = TRUE,\n",
    "  verbose = 0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {
    "id": "5OTUbWg8NcIE",
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "autoencoder %>% fit(\n",
    "  design_matrix,\n",
    "  design_matrix,\n",
    "  epochs = 50,\n",
    "  batch_size = 256,\n",
    "  shuffle = TRUE,\n",
    "  verbose = 0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {
    "id": "90nSf8Y8yIsl",
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "library(ggplot2)\n",
    "library(gridExtra)\n",
    "library(reshape2)\n",
    "\n",
    "# Compute neural reconstruction\n",
    "face_vector_flat <- as.numeric(face_vector)\n",
    "reconstruction <- predict(autoencoder, matrix(face_vector_flat, nrow = 1))\n",
    "\n",
    "# Do visual comparison\n",
    "image_height <- 62\n",
    "image_width <- 47\n",
    "image1 <- matrix(reconstructions[[4]], nrow = image_height, ncol = image_width)\n",
    "image2 <- t(matrix(reconstruction, nrow = image_width, ncol = image_height))\n",
    "\n",
    "images <- list(image1, image2)\n",
    "plot_faces(images, n_row = 1, n_col = 2, width = image_width, height = image_height)\n",
    "\n",
    "\n",
    "# Do numeric comparison\n",
    "# We also normalize the black/white gradient to take values in [0,1] (divide by 255)\n",
    "img1 <- as.numeric(reconstructions[[4]]) / 255\n",
    "img2 <- as.numeric(reconstruction) / 255\n",
    "mse <- mean((img1 - img2)^2)\n",
    "mse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {
    "id": "zrdD6A55yJML"
   },
   "source": [
    "## Neural Autoencoders\n",
    "\n",
    "Finally, let's train a nonlinear autoencoder for the same data where $F$ and $G$ are neural networks, and we restrict the dimension to be $k=64$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {
    "id": "KHPoFiS9fuhr",
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "library(tensorflow)\n",
    "\n",
    "# Use a nonlinear neural network\n",
    "n_features <- 2914\n",
    "encoding_dimension <- 64\n",
    "\n",
    "input_image <- layer_input(shape = n_features)\n",
    "encoded <- input_image %>%\n",
    "  layer_dense(units = encoding_dimension, activation = \"relu\") %>%\n",
    "  layer_dense(units = encoding_dimension, activation = \"relu\")\n",
    "\n",
    "decoded <- encoded %>%\n",
    "  layer_dense(units = encoding_dimension, activation = \"relu\") %>%\n",
    "  layer_dense(units = n_features, activation = \"relu\")\n",
    "\n",
    "autoencoder <- keras_model(inputs = input_image, outputs = decoded)\n",
    "\n",
    "autoencoder %>% compile(\n",
    "  optimizer = \"adam\",\n",
    "  loss = \"mse\"\n",
    ")\n",
    "autoencoder %>% fit(\n",
    "  design_matrix,\n",
    "  design_matrix,\n",
    "  epochs = 50,\n",
    "  batch_size = 256,\n",
    "  shuffle = TRUE,\n",
    "  verbose = 0\n",
    ")\n",
    "\n",
    "# Compute neural reconstruction\n",
    "reconstruction <- predict(autoencoder, matrix(face_vector, nrow = 1))\n",
    "\n",
    "# Do visual comparison\n",
    "plot_faces(list(reconstructions[[4]], t(matrix(reconstruction, nrow = image_width, ncol = image_height))),\n",
    "           n_row = 1, n_col = 2, width = image_width, height = image_height)\n",
    "\n",
    "# Do numeric comparison\n",
    "# We also normalize the black/white gradient to take values in [0,1] (divide by 255)\n",
    "img1 <- as.numeric(reconstructions[[4]]) / 255\n",
    "img2 <- as.numeric(reconstruction) / 255\n",
    "mse <- mean((img1 - img2)^2)\n",
    "mse"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.0.5"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 427.936706,
   "end_time": "2022-04-19T09:13:53.230849",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2022-04-19T09:06:45.294143",
   "version": "2.3.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
