{"metadata":{"kernelspec":{"name":"ir","display_name":"R","language":"R"},"language_info":{"name":"R","codemirror_mode":"r","pygments_lexer":"r","mimetype":"text/x-r-source","file_extension":".r","version":"3.6.3"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"\n\nThis notebook contains an example for teaching.\n","metadata":{}},{"cell_type":"markdown","source":"# Deep Neural Networks for Wage Prediction","metadata":{"_uuid":"051d70d956493feee0c6d64651c6a088724dca2a","_execution_state":"idle"}},{"cell_type":"markdown","source":"So far we have considered many machine learning methods such as Lasso and Random Forests for building a predictive model. In this lab, we extend our toolbox by returning to our wage prediction problem and showing how a neural network can be used for prediction.","metadata":{}},{"cell_type":"markdown","source":"## Data preparation","metadata":{}},{"cell_type":"markdown","source":"Again, we consider data from the U.S. March Supplement of the Current Population Survey (CPS) in 2015.","metadata":{}},{"cell_type":"code","source":"load(\"../input/wage2015-inference/wage2015_subsample_inference.Rdata\")\nZ <- subset(data,select=-c(lwage,wage)) # regressors","metadata":{"execution":{"iopub.status.busy":"2021-07-22T17:20:04.096828Z","iopub.execute_input":"2021-07-22T17:20:04.098188Z","iopub.status.idle":"2021-07-22T17:20:04.117076Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"First, we split the data first and normalize it.","metadata":{}},{"cell_type":"code","source":"# split the data into training and testing sets\nset.seed(1234)\ntraining <- sample(nrow(data), nrow(data)*(3/4), replace=FALSE)\n\ndata_train <- data[training,1:16]\ndata_test <- data[-training,1:16]\n\n# data_train <- data[training,]\n# data_test <- data[-training,]\n# X_basic <-  \"sex + exp1 + exp2+ shs + hsg+ scl + clg + mw + so + we + occ2+ ind2\"\n# formula_basic <- as.formula(paste(\"lwage\", \"~\", X_basic))\n# model_X_basic_train <- model.matrix(formula_basic,data_train)[,-1]\n# model_X_basic_test <- model.matrix(formula_basic,data_test)[,-1]\n# data_train <- as.data.frame(cbind(data_train$lwage,model_X_basic_train))\n# data_test <- as.data.frame(cbind(data_test$lwage,model_X_basic_test))\n# colnames(data_train)[1]<-'lwage'\n# colnames(data_test)[1]<-'lwage'","metadata":{"execution":{"iopub.status.busy":"2021-07-22T17:20:04.119485Z","iopub.execute_input":"2021-07-22T17:20:04.121302Z","iopub.status.idle":"2021-07-22T17:20:04.138871Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# normalize the data\nmean <- apply(data_train, 2, mean)\nstd <- apply(data_train, 2, sd)\ndata_train <- scale(data_train, center = mean, scale = std)\ndata_test <- scale(data_test, center = mean, scale = std)\ndata_train <- as.data.frame(data_train)\ndata_test <- as.data.frame(data_test)","metadata":{"execution":{"iopub.status.busy":"2021-07-22T17:20:04.141257Z","iopub.execute_input":"2021-07-22T17:20:04.143097Z","iopub.status.idle":"2021-07-22T17:20:04.16693Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Then, we construct the inputs for our network.","metadata":{}},{"cell_type":"code","source":"X_basic <-  \"sex + exp1 + shs + hsg+ scl + clg + mw + so + we\"\nformula_basic <- as.formula(paste(\"lwage\", \"~\", X_basic))\nmodel_X_basic_train <- model.matrix(formula_basic,data_train)\nmodel_X_basic_test <- model.matrix(formula_basic,data_test)\n\nY_train <- data_train$lwage\nY_test <- data_test$lwage","metadata":{"execution":{"iopub.status.busy":"2021-07-22T17:20:04.169018Z","iopub.execute_input":"2021-07-22T17:20:04.170604Z","iopub.status.idle":"2021-07-22T17:20:04.187817Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Neural Networks","metadata":{}},{"cell_type":"markdown","source":"First, we need to determine the structure of our network. We are using the R package *keras* to build a simple sequential neural network with three dense layers and the ReLU activation function.","metadata":{}},{"cell_type":"code","source":"library(keras)\n\nbuild_model <- function() {\n  model <- keras_model_sequential() %>% \n    layer_dense(units = 20, activation = \"relu\", # ReLU activation function\n                input_shape = dim(model_X_basic_train)[2])%>% \n    layer_dense(units = 10, activation = \"relu\") %>% \n    layer_dense(units = 1) \n  \n  model %>% compile(\n    optimizer = optimizer_adam(lr = 0.005), # Adam optimizer\n    loss = \"mse\", \n    metrics = c(\"mae\")\n  )\n}","metadata":{"execution":{"iopub.status.busy":"2021-07-22T17:20:04.190117Z","iopub.execute_input":"2021-07-22T17:20:04.191835Z","iopub.status.idle":"2021-07-22T17:20:04.2019Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let us have a look at the structure of our network in detail.","metadata":{}},{"cell_type":"code","source":"model <- build_model()\nsummary(model)","metadata":{"execution":{"iopub.status.busy":"2021-07-22T17:20:04.204024Z","iopub.execute_input":"2021-07-22T17:20:04.205597Z","iopub.status.idle":"2021-07-22T17:20:04.267216Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We have $441$ trainable parameters in total.","metadata":{}},{"cell_type":"markdown","source":"Now, let us train the network. Note that this takes substantial computation time. To speed up the computation time, we use GPU as an accelerator. The extent of computational time improvements varies based on a number of factors, including model architecture, batch-size, input pipeline complexity, etc.","metadata":{}},{"cell_type":"code","source":"# training the network \nnum_epochs <- 1000\nmodel %>% fit(model_X_basic_train, Y_train,\n                    epochs = num_epochs, batch_size = 100, verbose = 0)","metadata":{"execution":{"iopub.status.busy":"2021-07-22T17:20:04.269524Z","iopub.execute_input":"2021-07-22T17:20:04.271192Z","iopub.status.idle":"2021-07-22T17:21:23.79291Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"After training the neural network, we can evaluate the performance of our model on the test sample.","metadata":{}},{"cell_type":"code","source":"# evaluating performance\nmodel %>% evaluate(model_X_basic_test, Y_test, verbose = 0)","metadata":{"execution":{"iopub.status.busy":"2021-07-22T17:21:23.795255Z","iopub.execute_input":"2021-07-22T17:21:23.797152Z","iopub.status.idle":"2021-07-22T17:21:23.957094Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# calculating the performance measures\npred.nn <- model %>% predict(model_X_basic_test)\nMSE.nn = summary(lm((Y_test-pred.nn)^2~1))$coef[1:2]\nR2.nn <- 1-MSE.nn[1]/var(Y_test)\n# printing R^2\ncat(\"R^2 of the neural network:\",R2.nn)","metadata":{"execution":{"iopub.status.busy":"2021-07-22T17:21:23.959228Z","iopub.execute_input":"2021-07-22T17:21:23.960916Z","iopub.status.idle":"2021-07-22T17:21:24.068429Z"},"trusted":true},"execution_count":null,"outputs":[]}]}
