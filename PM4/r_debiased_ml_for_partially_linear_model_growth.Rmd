---
title: An R Markdown document converted from "PM4/r_debiased_ml_for_partially_linear_model_growth.irnb"
output: html_document
---

# Double/Debiased Machine Learning for the Partially Linear Regression Model

This is a simple implementation of Debiased Machine Learning for the Partially Linear Regression Model, which provides an application of DML inference to determine the causal effect of countries' intitial wealth on the rate of economic growth.


Reference:

- https://arxiv.org/abs/1608.00060
- https://www.amazon.com/Business-Data-Science-Combining-Accelerate/dp/1260452778

The code is based on the book.

```{r}
install.packages("xtable")
install.packages("hdm")
install.packages("randomForest")
install.packages("glmnet")
install.packages("sandwich")
```

```{r}
library(xtable)
library(randomForest)
library(hdm)
library(glmnet)
library(sandwich)

set.seed(1)
```

```{r}
file <- "https://raw.githubusercontent.com/CausalAIBook/MetricsMLNotebooks/main/data/GrowthData.csv"
data <- read.csv(file)
data <- subset(data, select = -1) # get rid of index column
head(data)
dim(data)
```

```{r}
y <- as.matrix(data[, 1]) # outcome: growth rate
d <- as.matrix(data[, 3]) # treatment: initial wealth
x <- as.matrix(data[, -c(1, 2, 3)]) # controls: country characteristics

# some summary statistics
cat(sprintf("\nThe length of y is %g \n", length(y)))
cat(sprintf("\nThe number of features in x is %g \n", dim(x)[2]))

lres <- summary(lm(y ~ d + x))$coef[2, 1:2]
cat(sprintf("\nNaive OLS that uses all features w/o cross-fitting Y ~ D+X yields: \ncoef (se) = %g (%g)\n",
            lres[1], lres[2]))
```

# DML algorithm

Here we perform estimation and inference of predictive coefficient $\alpha$ in the partially linear statistical model,
$$
Y = D\alpha + g(X) + U, \quad E (U | D, X) = 0.
$$
For $\tilde Y = Y- E(Y|X)$ and $\tilde D= D- E(D|X)$, we can write
$$
\tilde Y = \alpha \tilde D + U, \quad E (U |\tilde D) =0.
$$
Parameter $\alpha$ is then estimated using cross-fitting approach to obtain the residuals $\tilde D$ and $\tilde Y$.
The algorithm comsumes $Y, D, X$, and machine learning methods for learning the residuals $\tilde Y$ and $\tilde D$, where
the residuals are obtained by cross-validation (cross-fitting).

The statistical parameter $\alpha$ has a causal interpretation of being the effect of $D$ on $Y$ in the causal DAG $$ D\to Y, \quad X\to (D,Y)$$ or the counterfactual outcome model with conditionally exogenous (conditionally random) assignment of treatment $D$ given $X$:
$$
Y(d) = d\alpha + g(X) + U(d),\quad  U(d) \text{ indep } D |X, \quad Y = Y(D), \quad U = U(D).
$$

```{r}
dml2_for_plm <- function(x, d, y, dreg, yreg, nfold = 2) {
  nobs <- nrow(x) # number of observations
  foldid <- rep.int(1:nfold, times = ceiling(nobs / nfold))[sample.int(nobs)] # define folds indices
  I <- split(1:nobs, foldid) # split observation indices into folds
  ytil <- dtil <- rep(NA, nobs)
  cat("fold: ")
  for (b in seq_along(I)) {
    dfit <- dreg(x[-I[[b]], ], d[-I[[b]]]) # take a fold out
    yfit <- yreg(x[-I[[b]], ], y[-I[[b]]]) # take a foldt out
    dhat <- predict(dfit, x[I[[b]], ], type = "response") # predict the left-out fold
    yhat <- predict(yfit, x[I[[b]], ], type = "response") # predict the left-out fold
    dtil[I[[b]]] <- (d[I[[b]]] - dhat) # record residual for the left-out fold
    ytil[I[[b]]] <- (y[I[[b]]] - yhat) # record residial for the left-out fold
    cat(b, " ")
  }
  rfit <- lm(ytil ~ dtil) # estimate the main parameter by regressing one residual on the other
  coef.est <- coef(rfit)[2] # extract coefficient
  se <- sqrt(vcovHC(rfit)[2, 2]) # record robust standard error
  cat(sprintf("\ncoef (se) = %g (%g)\n", coef.est, se)) # printing output
  return(list(coef.est = coef.est, se = se, dtil = dtil, ytil = ytil)) # save output and residuals
}
```

We now run through DML using as first stage models:
 1. OLS
 2. (Rigorous) Lasso
 3. Random Forests
 4. Mix of Random Forest and Lasso

```{r}
# DML with OLS
cat(sprintf("\nDML with OLS w/o feature selection \n"))
dreg <- function(x, d) {
  glmnet(x, d, lambda = 0)
} # ML method= OLS using glmnet; using lm gives bugs
yreg <- function(x, y) {
  glmnet(x, y, lambda = 0)
} # ML method = OLS
dml2_ols <- dml2_for_plm(x, d, y, dreg, yreg, nfold = 10)


# DML with Lasso:
cat(sprintf("\nDML with Lasso \n"))
dreg <- function(x, d) {
  rlasso(x, d, post = FALSE)
} # ML method= lasso from hdm
yreg <- function(x, y) {
  rlasso(x, y, post = FALSE)
} # ML method = lasso from hdm
dml2_lasso <- dml2_for_plm(x, d, y, dreg, yreg, nfold = 10)


# DML with Random Forest:
cat(sprintf("\nDML with Random Forest \n"))
dreg <- function(x, d) {
  randomForest(x, d)
} # ML method=Forest
yreg <- function(x, y) {
  randomForest(x, y)
} # ML method=Forest
dml2_rf <- dml2_for_plm(x, d, y, dreg, yreg, nfold = 10)

# DML MIX:
cat(sprintf("\nDML with Lasso for D and Random Forest for Y \n"))
dreg <- function(x, d) {
  rlasso(x, d, post = FALSE)
} # ML method=Forest
yreg <- function(x, y) {
  randomForest(x, y)
} # ML method=Forest
dml2_mix <- dml2_for_plm(x, d, y, dreg, yreg, nfold = 10)
```

Now we examine the RMSE of D and Y to see which method performs well in the first-stage. We print all results below in the following table:

```{r}
pr_res_d <- c(mean((dml2_ols$dtil)^2), mean((dml2_lasso$dtil)^2), mean((dml2_rf$dtil)^2), mean((dml2_mix$dtil)^2))
pr_res_y <- c(mean((dml2_ols$ytil)^2), mean((dml2_lasso$ytil)^2), mean((dml2_rf$ytil)^2), mean((dml2_mix$ytil)^2))
pr_res <- rbind(sqrt(pr_res_d), sqrt(pr_res_y))
rownames(pr_res) <- c("RMSE D", "RMSE Y")
colnames(pr_res) <- c("OLS", "Lasso", "RF", "Mix")
```

```{r}
table <- matrix(0, 4, 4)

# Point Estimate
table[1, 1] <- as.numeric(dml2_ols$coef.est)
table[2, 1] <- as.numeric(dml2_lasso$coef.est)
table[3, 1] <- as.numeric(dml2_rf$coef.est)
table[4, 1] <- as.numeric(dml2_mix$coef.est)

# SE
table[1, 2] <- as.numeric(dml2_ols$se)
table[2, 2] <- as.numeric(dml2_lasso$se)
table[3, 2] <- as.numeric(dml2_rf$se)
table[4, 2] <- as.numeric(dml2_mix$se)

# RMSE Y
table[1, 3] <- as.numeric(pr_res[2, 1])
table[2, 3] <- as.numeric(pr_res[2, 2])
table[3, 3] <- as.numeric(pr_res[2, 3])
table[4, 3] <- as.numeric(pr_res[2, 4])

# RMSE D
table[1, 4] <- as.numeric(pr_res[1, 1])
table[2, 4] <- as.numeric(pr_res[1, 2])
table[3, 4] <- as.numeric(pr_res[1, 3])
table[4, 4] <- as.numeric(pr_res[1, 4])



# print results
colnames(table) <- c("Estimate", "Standard Error", "RMSE Y", "RMSE D")
rownames(table) <- c("OLS", "Lasso", "RF", "RF/Lasso Mix")
table
```

```{r}
print(table, digit = 3)
```

```{r}
tab <- xtable(table, digits = 3)
print(tab, type = "latex")
```

